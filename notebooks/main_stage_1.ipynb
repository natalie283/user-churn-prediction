{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder \n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import classification_report, roc_auc_score, f1_score, accuracy_score,confusion_matrix\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "   \n",
    "\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# calculate class weights based on the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_parquet('/Users/natalie/Desktop/DS Thesis/Code/data/test.parquet')\n",
    "train = pd.read_parquet('/Users/natalie/Desktop/DS Thesis/Code/data/train.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop column province\n",
    "test = test.drop(columns=['province'])\n",
    "train = train.drop(columns=['province'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET ='churn_user'\n",
    "CATEGORICAL_FEATURES  = ['os_name', 'age_group','gender', 'country', 'region', 'province_type']\n",
    "DATETIME_FEATURES  = ['first_date', 'lastest_active_day']\n",
    "SEARCH_CC_FEATURES = [ 'clicks', 'search_volume', 'dating_search', 'videoclip_search', 'technical_search', 'housekeeping_family_search', 'marketing_search', 'other_search']\n",
    "SEARCH_GG_FEATURES = [ 'serp_click', 'search_volume_gg', 'search_clicks_gg', 'other_search_gg','housekeeping_family_search_gg','videoclip_search_gg', 'dating_search_gg', 'marketing_search_gg', 'technical_search_gg']\n",
    "ACTIVE_FEATURES = ['active_day', 'life_time',  'not_active_day', 'total_active_time']\n",
    "ADS_FEATURES =  ['ads_impression', 'ads_click', 'ads_revenue']\n",
    "OTHERS_FEATURES =[ 'newtab_count', 'download_count', 'pip_count', 'sidebar_count', 'incognito_count', 'signin_count', 'youtube_count',\n",
    "                    'work_count', 'social_count', 'news_count', 'entertainment_count', 'ecommerce_count']\n",
    "NUMERICAL_FEATURES = SEARCH_CC_FEATURES + SEARCH_GG_FEATURES + ACTIVE_FEATURES + ADS_FEATURES + OTHERS_FEATURES\n",
    "MODEL_NAMES = ['log_reg', 'randomforest','lightgbm', 'xgboost', 'mlp']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_threshold_f1_score(train_labels, oofs, average='macro'):\n",
    "    scores = []\n",
    "    thresholds = []\n",
    "    best_score = 0\n",
    "    best_threshold = 0\n",
    "    for threshold in np.arange(0.1, 0.9, 0.01):\n",
    "        print(f'{threshold:.02f}, ', end='')\n",
    "        preds = (oofs > threshold).astype('int')\n",
    "        m = f1_score(train_labels, preds, average=average)\n",
    "        scores.append(m)\n",
    "        thresholds.append(threshold)\n",
    "        if m > best_score:\n",
    "            best_score = m\n",
    "            best_threshold = threshold\n",
    "    return best_threshold, best_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fillna(df):\n",
    "    df['total_active_time'] = df['total_active_time'].fillna(0)\n",
    "    df['ads_impression'] = df['ads_impression'].fillna(0)\n",
    "    df['ads_click'] = df['ads_click'].fillna(0)\n",
    "    df['ads_revenue'] = df['ads_revenue'].fillna(0)\n",
    "    df['clicks'] = df['clicks'].fillna(0)\n",
    "    for c in OTHERS_FEATURES:\n",
    "        df[c] = df[c].fillna(0)\n",
    "    return df\n",
    "\n",
    "def process_data(df,oh_encoder=None, robust_scaler=None,agg_features=None):\n",
    "    if not oh_encoder:\n",
    "        print(\"fit train OneHotEncoder\")\n",
    "        oh_encoder = OneHotEncoder()\n",
    "        oh_encoder.fit(df[CATEGORICAL_FEATURES])\n",
    "    else:\n",
    "        print(\"loadd onehot encoder\")\n",
    "    if not robust_scaler:\n",
    "        print(\"fit train RobustScaler\")\n",
    "        robust_scaler = RobustScaler()\n",
    "        robust_scaler.fit(df[NUMERICAL_FEATURES])\n",
    "    else:\n",
    "        print(\"loadd robust scaler\")\n",
    "    df_cat = pd.DataFrame(oh_encoder.transform(df[CATEGORICAL_FEATURES]).toarray())\n",
    "    new_cat_cols = oh_encoder.get_feature_names_out(CATEGORICAL_FEATURES)\n",
    "    df_cat.columns = new_cat_cols\n",
    "    df_num = pd.DataFrame(robust_scaler.transform(df[NUMERICAL_FEATURES]))\n",
    "    df_num.columns = NUMERICAL_FEATURES\n",
    "    new_df = pd.concat([df_cat.reset_index(drop=True), df_num.reset_index(drop=True)], axis=1)\n",
    "    new_df = fillna(new_df)\n",
    "    return new_df, oh_encoder, robust_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_ESTIMATORS = 200\n",
    "SEED=42\n",
    "\n",
    "LGBM_Hyperparameters = {\n",
    "    \"n_estimators\": N_ESTIMATORS,\n",
    "    'learning_rate': 0.09249779656872704,\n",
    "    'max_depth': 8,\n",
    "    'colsample_bytree': 1.0,\n",
    "    'subsample': 0.1,\n",
    "    'reg_lambda': 0.9648085464373999,\n",
    "    'reg_alpha': 0.9903394475964249,\n",
    "    'random_state': SEED\n",
    "    #'device':'gpu',\n",
    "    #\"class_weight\": \"balanced\"\n",
    "}\n",
    "\n",
    "XGBoost_Hyperparameters = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': ['logloss', 'auc'],\n",
    "    'n_estimators': N_ESTIMATORS,\n",
    "    'learning_rate': 0.034630277480196384,\n",
    "    'max_depth': 9,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'subsample': 0.30000000000000004,\n",
    "    'reg_alpha': 0.0020136244579038245,\n",
    "    'reg_lambda': 1.3270228907353322e-06,\n",
    "    'seed': SEED,\n",
    "    # 'scale_pos_weight':3,\n",
    "    'enable_categorical':True,\n",
    "    'early_stopping_rounds': 50,\n",
    "    #'tree_method':'gpu_hist'\n",
    "}\n",
    "\n",
    "RF_Hyperparameters = {\n",
    "    'n_estimators': N_ESTIMATORS,\n",
    "    'max_depth': 9,\n",
    "    'max_features': 0.5, #rf_items['colsample_bytree']\n",
    "    'random_state':SEED,\n",
    "    'n_jobs': -1\n",
    "}\n",
    "\n",
    "MLP_Hyperparametesr = {\n",
    "    'hidden_layer_sizes':(5,125), #'hidden_layer_sizes': 5\n",
    "    'random_state':SEED, \n",
    "    'max_iter':min(N_ESTIMATORS,100)}\n",
    "\n",
    "\n",
    "Logreg_Hyperparameters = {'max_iter':N_ESTIMATORS,'random_state':SEED}\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "\n",
    "def cross_validate(train, USE_SMOTE=False, USE_CLASS_WEIGHT=False, USE_UNDER_SAMPLING=False):\n",
    "    oofs = np.zeros((train.shape[0], len(MODEL_NAMES)))\n",
    "    for i, (train_index, valid_index) in enumerate(kfold.split(train, train[TARGET])):\n",
    "        print(f\"===========fold {i}================\")\n",
    "        X_train, oh_encoder, robust_scaler = process_data(train.iloc[train_index])\n",
    "        X_valid, _, _  = process_data(train.iloc[valid_index], oh_encoder,robust_scaler)\n",
    "        print(X_train.isnull().sum())\n",
    "        y_train = train.iloc[train_index][TARGET].values\n",
    "        y_valid = train.iloc[valid_index][TARGET].values\n",
    "        logreg_hyperparameters = Logreg_Hyperparameters.copy()\n",
    "        lgb_hyperparameters = LGBM_Hyperparameters.copy()\n",
    "        xgboost_hyperparameters = XGBoost_Hyperparameters.copy()\n",
    "        rf_hyperparameters = RF_Hyperparameters.copy()\n",
    "        mlp_hyperparameters = MLP_Hyperparametesr.copy()\n",
    "        if USE_SMOTE:\n",
    "            print(\"SMOTEEEE\")\n",
    "            sm = SMOTE(random_state=42)\n",
    "            X_train, y_train = sm.fit_resample(X_train, y_train)\n",
    "        elif USE_CLASS_WEIGHT:\n",
    "            print(\"CLASS_WEIGHTTTT\")\n",
    "           \n",
    "            class_weights = compute_class_weight('balanced', classes=np.array([0, 1]), y=y_train)\n",
    "            class_weights =  {0: class_weights[0], 1: class_weights[1]}\n",
    "            lgb_hyperparameters['class_weight'] = class_weights\n",
    "            logreg_hyperparameters['class_weight'] = class_weights\n",
    "            xgboost_hyperparameters['scale_pos_weight'] = class_weights[1] /  class_weights[0]\n",
    "            rf_hyperparameters['class_weight'] = class_weights\n",
    "        elif USE_UNDER_SAMPLING:\n",
    "            print(\"UNDER SAMPLING\")\n",
    "            rus = RandomUnderSampler(random_state=42)\n",
    "            X_train, y_train = rus.fit_resample(X_train, y_train)\n",
    "        print(\"LOGREG--------------\")\n",
    "        logreg_model = LogisticRegression(**logreg_hyperparameters)\n",
    "        logreg_model.fit(X_train, y_train)\n",
    "        logreg_y_pred_proba = logreg_model.predict_proba(X_valid)[:,1]\n",
    "        best_threshold, best_score = find_best_threshold_f1_score(y_valid, logreg_y_pred_proba)\n",
    "        print(f\"\\n best_threshold {best_threshold} best_score {best_score}\")\n",
    "        y_pred = [1 if y_hat >=best_threshold else 0 for y_hat in logreg_y_pred_proba]\n",
    "\n",
    "        print(roc_auc_score(y_valid, logreg_y_pred_proba))\n",
    "\n",
    "        print(classification_report(y_valid, y_pred, digits=4))\n",
    "        oofs[valid_index,0] = logreg_y_pred_proba\n",
    "\n",
    "        print(\"Random Forest--------------\")\n",
    "        rf_model = RandomForestClassifier(**rf_hyperparameters)\n",
    "        rf_model.fit(X_train, y_train)\n",
    "        rf_y_pred_proba = rf_model.predict_proba(X_valid)[:,1]\n",
    "        best_threshold, best_score = find_best_threshold_f1_score(y_valid, rf_y_pred_proba)\n",
    "        print(f\"\\n best_threshold {best_threshold} best_score {best_score}\")\n",
    "        y_pred = [1 if y_hat >=best_threshold else 0 for y_hat in rf_y_pred_proba]\n",
    "\n",
    "        print(roc_auc_score(y_valid, rf_y_pred_proba))\n",
    "\n",
    "        print(classification_report(y_valid, y_pred, digits=4))\n",
    "        oofs[valid_index,1] = rf_y_pred_proba\n",
    "    #     models.append(model)\n",
    "        print(\"LGBModel--------------\")\n",
    "        lgb_model = LGBMClassifier(**lgb_hyperparameters)\n",
    "        callbacks = [lgb.early_stopping(200, verbose=50), lgb.log_evaluation(period=50)]\n",
    "        lgb_model.fit(X_train, y_train,\n",
    "                  eval_set=[(X_train, y_train), (X_valid, y_valid)],\n",
    "                  eval_metric=[\"logloss\", \"auc\"],\n",
    "                  callbacks=callbacks)\n",
    "\n",
    "        lgb_y_pred_proba = lgb_model.predict_proba(X_valid)[:,1]\n",
    "        best_threshold, best_score = find_best_threshold_f1_score(y_valid, lgb_y_pred_proba)\n",
    "        print(f\"\\n best_threshold {best_threshold} best_score {best_score}\")\n",
    "        y_pred = [1 if y_hat >=best_threshold else 0 for y_hat in lgb_y_pred_proba]\n",
    "        print(roc_auc_score(y_valid, lgb_y_pred_proba))\n",
    "        print(classification_report(y_valid, y_pred, digits=4))\n",
    "        oofs[valid_index,2] = lgb_y_pred_proba\n",
    "    #     models.append(model)\n",
    "        # display(pd.DataFrame({'score': lgb_model.feature_importances_, 'feature': lgb_model.feature_name_}).sort_values('score',ascending=False))\n",
    "\n",
    "        print(\"XGBoost--------------\")\n",
    "        xgb_model = XGBClassifier(**xgboost_hyperparameters)\n",
    "        xgb_model.fit(X_train, y_train,\n",
    "                  eval_set=[(X_train, y_train), (X_valid, y_valid)],\n",
    "                  verbose=50)\n",
    "        xgb_y_pred_proba = xgb_model.predict_proba(X_valid)[:,1]\n",
    "        best_threshold, best_score = find_best_threshold_f1_score(y_valid, xgb_y_pred_proba)\n",
    "        print(f\"\\n best_threshold {best_threshold} best_score {best_score}\")\n",
    "        y_pred = [1 if y_hat >=best_threshold else 0 for y_hat in xgb_y_pred_proba]\n",
    "        print(roc_auc_score(y_valid, xgb_y_pred_proba))\n",
    "        print(classification_report(y_valid, y_pred, digits=4))\n",
    "        oofs[valid_index, 3] = xgb_y_pred_proba\n",
    "        \n",
    "        print(\"MLP------------------\")\n",
    "        mlp_model = MLPClassifier(**mlp_hyperparameters)\n",
    "        mlp_model.fit(X_train, y_train)\n",
    "        mlp_y_pred_proba = mlp_model.predict_proba(X_valid)[:,1]\n",
    "        best_threshold, best_score = find_best_threshold_f1_score(y_valid, mlp_y_pred_proba)\n",
    "        print(f\"\\n best_threshold {best_threshold} best_score {best_score}\")\n",
    "        y_pred = [1 if y_hat >=best_threshold else 0 for y_hat in mlp_y_pred_proba]\n",
    "        print(roc_auc_score(y_valid, mlp_y_pred_proba))\n",
    "        print(classification_report(y_valid, y_pred, digits=4))\n",
    "        oofs[valid_index, 4] = mlp_y_pred_proba\n",
    "    return oofs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_scores = []\n",
    "f1_scores = []\n",
    "auc_scores = []\n",
    "specificity_scores = []\n",
    "sensitivity_scores = []\n",
    "def scoring(y_test,y_pred_proba, best_threshold):\n",
    "    y_pred = [1 if y_hat >= best_threshold else 0 for y_hat in y_pred_proba]\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    _f1_score = f1_score(y_test, y_pred, average='macro')\n",
    "    auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "    specificity = tn / (tn+fp)\n",
    "    sensitivity = tp / (tp+fn)\n",
    "    print(\"accuracy\", acc)\n",
    "    print(\"f1_score\", _f1_score)\n",
    "    print(\"auc\", auc_score)\n",
    "    print(\"sensitivity\", sensitivity, \"specificity\", specificity)\n",
    "    print(classification_report(y_test, y_pred, digits=4))\n",
    "    return acc, _f1_score, auc_score, specificity, sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(oofs,X_train, y_train, X_test, y_test, USE_SMOTE=False,USE_CLASS_WEIGHT=False, USE_UNDER_SAMPLING=False):\n",
    "    models = []\n",
    "    predictions = []\n",
    "    accuracy_scores = []\n",
    "    f1_scores = []\n",
    "    auc_scores = []\n",
    "    specificity_scores = []\n",
    "    sensitivity_scores = []\n",
    "    best_thresholds = []\n",
    "    for i in range(len(MODEL_NAMES)):\n",
    "        best_threshold, best_score = find_best_threshold_f1_score(train[TARGET].values, oofs[:,i])\n",
    "        best_thresholds.append(best_threshold)\n",
    "        print('\\n',best_threshold, best_score)\n",
    "    logreg_hyperparameters = Logreg_Hyperparameters.copy()\n",
    "    lgb_hyperparameters = LGBM_Hyperparameters.copy()\n",
    "    xgboost_hyperparameters = XGBoost_Hyperparameters.copy()\n",
    "    del xgboost_hyperparameters['early_stopping_rounds']\n",
    "    rf_hyperparameters = RF_Hyperparameters.copy()\n",
    "    mlp_hyperparameters = MLP_Hyperparametesr.copy()\n",
    "    if USE_SMOTE:\n",
    "        print(\"SMOTEEEE\")\n",
    "        sm = SMOTE(random_state=42)\n",
    "        X_train, y_train = sm.fit_resample(X_train, y_train)\n",
    "    elif USE_CLASS_WEIGHT:\n",
    "        print(\"CLASS_WEIGHTTTT\")\n",
    "        class_weights = compute_class_weight('balanced', classes=np.array([0, 1]), y=y_train)\n",
    "        class_weights =  {0: class_weights[0], 1: class_weights[1]}\n",
    "        lgb_hyperparameters['class_weight'] = class_weights\n",
    "        logreg_hyperparameters['class_weight'] = class_weights\n",
    "        xgboost_hyperparameters['scale_pos_weight'] = class_weights[1]/ class_weights[0]\n",
    "        rf_hyperparameters['class_weight'] = class_weights\n",
    "    elif USE_UNDER_SAMPLING:\n",
    "        print(\"UNDER SAMPLING\")\n",
    "        rus = RandomUnderSampler(random_state=42)\n",
    "        X_train, y_train = rus.fit_resample(X_train, y_train)\n",
    "\n",
    "    print(\"LOGREG--------------\")\n",
    "    logreg_model = LogisticRegression(**logreg_hyperparameters)\n",
    "    logreg_model.fit(X_train, y_train)\n",
    "    logreg_y_pred_proba = logreg_model.predict_proba(X_test)[:,1]\n",
    "    acc, _f1_score, auc_score, specificity, sensitivity = scoring(y_test,logreg_y_pred_proba,best_thresholds[0])\n",
    "    accuracy_scores.append(acc)\n",
    "    f1_scores.append(_f1_score)\n",
    "    auc_scores.append(auc_score)\n",
    "    specificity_scores.append(specificity)\n",
    "    sensitivity_scores.append(sensitivity)\n",
    "    predictions.append(logreg_y_pred_proba)\n",
    "    models.append(logreg_model)\n",
    "\n",
    "    print(\"Random Forest--------------\")\n",
    "    rf_model = RandomForestClassifier(**rf_hyperparameters)\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    rf_y_pred_proba = rf_model.predict_proba(X_test)[:,1]\n",
    "    acc, _f1_score, auc_score, specificity, sensitivity = scoring(y_test,rf_y_pred_proba,best_thresholds[1])\n",
    "    accuracy_scores.append(acc)\n",
    "    f1_scores.append(_f1_score)\n",
    "    auc_scores.append(auc_score)\n",
    "    specificity_scores.append(specificity)\n",
    "    sensitivity_scores.append(sensitivity)\n",
    "    predictions.append(rf_y_pred_proba)\n",
    "    models.append(rf_model)\n",
    "\n",
    "    print(\"LGBModel--------------\")\n",
    "    lgb_model = LGBMClassifier(**lgb_hyperparameters)\n",
    "    lgb_model.fit(X_train, y_train)\n",
    "    lgb_y_pred_proba = lgb_model.predict_proba(X_test)[:,1]\n",
    "    acc, _f1_score, auc_score, specificity, sensitivity = scoring(y_test,lgb_y_pred_proba,best_thresholds[2])\n",
    "    accuracy_scores.append(acc)\n",
    "    f1_scores.append(_f1_score)\n",
    "    auc_scores.append(auc_score)\n",
    "    specificity_scores.append(specificity)\n",
    "    sensitivity_scores.append(sensitivity)\n",
    "    predictions.append(lgb_y_pred_proba)\n",
    "    models.append(lgb_model)\n",
    "\n",
    "    print(\"XGBoost--------------\")\n",
    "    print(xgboost_hyperparameters)\n",
    "    xgb_model = XGBClassifier(**xgboost_hyperparameters)\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    xgb_y_pred_proba = xgb_model.predict_proba(X_test)[:,1]\n",
    "    acc, _f1_score, auc_score, specificity, sensitivity = scoring(y_test,xgb_y_pred_proba,best_thresholds[3])\n",
    "    accuracy_scores.append(acc)\n",
    "    f1_scores.append(_f1_score)\n",
    "    auc_scores.append(auc_score)\n",
    "    specificity_scores.append(specificity)\n",
    "    sensitivity_scores.append(sensitivity)\n",
    "    predictions.append(xgb_y_pred_proba)\n",
    "    models.append(xgb_model)\n",
    "    \n",
    "    print(\"MLP--------------\")\n",
    "    mlp_model = MLPClassifier(**mlp_hyperparameters)\n",
    "    mlp_model.fit(X_train, y_train)\n",
    "    mlp_y_pred_proba = mlp_model.predict_proba(X_test)[:,1]\n",
    "    acc, _f1_score, auc_score, specificity, sensitivity = scoring(y_test,mlp_y_pred_proba,best_thresholds[4])\n",
    "    accuracy_scores.append(acc)\n",
    "    f1_scores.append(_f1_score)\n",
    "    auc_scores.append(auc_score)\n",
    "    specificity_scores.append(specificity)\n",
    "    sensitivity_scores.append(sensitivity)\n",
    "    predictions.append(mlp_y_pred_proba)\n",
    "    models.append(mlp_model)\n",
    "\n",
    "    print(MODEL_NAMES)\n",
    "    print(accuracy_scores)\n",
    "    print(f1_scores)\n",
    "    print(auc_scores)\n",
    "    print(specificity_scores)\n",
    "    print(sensitivity_scores)\n",
    "    score_df = pd.DataFrame({'model_name': MODEL_NAMES,\n",
    "                         'accuracy_score':accuracy_scores, \n",
    "                         'f1_score': f1_scores, \n",
    "                         'auc_score': auc_scores, \n",
    "                         'specificity_score': specificity_scores, \n",
    "                         'sensitivity_score': sensitivity_scores})\n",
    "    return score_df,models, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n"
     ]
    }
   ],
   "source": [
    "X_train,oh_encoder,robust_scaler = process_data(train)\n",
    "X_test, _,_ = process_data(test,oh_encoder,robust_scaler)\n",
    "y_train = train[TARGET].values\n",
    "y_test = test[TARGET].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not handle Balanced\n",
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "os_name_macos                    0\n",
      "os_name_windows                  0\n",
      "age_group_15-17                  0\n",
      "age_group_18-24                  0\n",
      "age_group_25-34                  0\n",
      "age_group_35-44                  0\n",
      "age_group_45-54                  0\n",
      "age_group_55+                    0\n",
      "age_group_under 14               0\n",
      "age_group_unknown                0\n",
      "gender_female                    0\n",
      "gender_male                      0\n",
      "gender_unknown                   0\n",
      "country_IN                       0\n",
      "country_UNKNOWN                  0\n",
      "country_VN                       0\n",
      "region_Central Vietnam           0\n",
      "region_Northern Vietnam          0\n",
      "region_Southern Vietnam          0\n",
      "region_None                      0\n",
      "province_type_rural              0\n",
      "province_type_unknown            0\n",
      "province_type_urban              0\n",
      "clicks                           0\n",
      "search_volume                    0\n",
      "dating_search                    0\n",
      "videoclip_search                 0\n",
      "technical_search                 0\n",
      "housekeeping_family_search       0\n",
      "marketing_search                 0\n",
      "other_search                     0\n",
      "serp_click                       0\n",
      "search_volume_gg                 0\n",
      "search_clicks_gg                 0\n",
      "other_search_gg                  0\n",
      "housekeeping_family_search_gg    0\n",
      "videoclip_search_gg              0\n",
      "dating_search_gg                 0\n",
      "marketing_search_gg              0\n",
      "technical_search_gg              0\n",
      "active_day                       0\n",
      "life_time                        0\n",
      "not_active_day                   0\n",
      "total_active_time                0\n",
      "ads_impression                   0\n",
      "ads_click                        0\n",
      "ads_revenue                      0\n",
      "newtab_count                     0\n",
      "download_count                   0\n",
      "pip_count                        0\n",
      "sidebar_count                    0\n",
      "incognito_count                  0\n",
      "signin_count                     0\n",
      "youtube_count                    0\n",
      "work_count                       0\n",
      "social_count                     0\n",
      "news_count                       0\n",
      "entertainment_count              0\n",
      "ecommerce_count                  0\n",
      "dtype: int64\n",
      "LOGREG--------------\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " best_threshold 0.4099999999999998 best_score 0.7640305662850968\n",
      "0.8788626925326052\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8899    0.8727    0.8812    151135\n",
      "           1     0.6286    0.6662    0.6468     48865\n",
      "\n",
      "    accuracy                         0.8223    200000\n",
      "   macro avg     0.7592    0.7694    0.7640    200000\n",
      "weighted avg     0.8261    0.8223    0.8240    200000\n",
      "\n",
      "Random Forest--------------\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " best_threshold 0.3999999999999998 best_score 0.7822899387763489\n",
      "0.9002293100796017\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9050    0.8704    0.8873    151135\n",
      "           1     0.6415    0.7173    0.6772     48865\n",
      "\n",
      "    accuracy                         0.8330    200000\n",
      "   macro avg     0.7732    0.7938    0.7823    200000\n",
      "weighted avg     0.8406    0.8330    0.8360    200000\n",
      "\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 195463, number of negative: 604537\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.056734 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7358\n",
      "[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 59\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244329 -> initscore=-1.129092\n",
      "[LightGBM] [Info] Start training from score -1.129092\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[50]\ttraining's binary_logloss: 0.327901\ttraining's auc: 0.901201\tvalid_1's binary_logloss: 0.328603\tvalid_1's auc: 0.900615\n",
      "[100]\ttraining's binary_logloss: 0.324823\ttraining's auc: 0.902778\tvalid_1's binary_logloss: 0.326753\tvalid_1's auc: 0.901399\n",
      "[150]\ttraining's binary_logloss: 0.323137\ttraining's auc: 0.90381\tvalid_1's binary_logloss: 0.326343\tvalid_1's auc: 0.901598\n",
      "[200]\ttraining's binary_logloss: 0.321615\ttraining's auc: 0.904751\tvalid_1's binary_logloss: 0.326037\tvalid_1's auc: 0.901733\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[200]\ttraining's binary_logloss: 0.321615\ttraining's auc: 0.904751\tvalid_1's binary_logloss: 0.326037\tvalid_1's auc: 0.901733\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " best_threshold 0.3999999999999998 best_score 0.783851717125985\n",
      "0.9017327857196079\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9064    0.8700    0.8878    151135\n",
      "           1     0.6423    0.7221    0.6799     48865\n",
      "\n",
      "    accuracy                         0.8339    200000\n",
      "   macro avg     0.7744    0.7961    0.7839    200000\n",
      "weighted avg     0.8419    0.8339    0.8370    200000\n",
      "\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-logloss:0.54359\tvalidation_0-auc:0.89462\tvalidation_1-logloss:0.54369\tvalidation_1-auc:0.89219\n",
      "[50]\tvalidation_0-logloss:0.34357\tvalidation_0-auc:0.90531\tvalidation_1-logloss:0.34802\tvalidation_1-auc:0.90089\n",
      "[100]\tvalidation_0-logloss:0.32016\tvalidation_0-auc:0.90802\tvalidation_1-logloss:0.32858\tvalidation_1-auc:0.90157\n",
      "[150]\tvalidation_0-logloss:0.31422\tvalidation_0-auc:0.91011\tvalidation_1-logloss:0.32599\tvalidation_1-auc:0.90184\n",
      "[199]\tvalidation_0-logloss:0.31136\tvalidation_0-auc:0.91168\tvalidation_1-logloss:0.32565\tvalidation_1-auc:0.90191\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " best_threshold 0.3899999999999999 best_score 0.7838214138759865\n",
      "0.9019240522726919\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9093    0.8645    0.8863    151135\n",
      "           1     0.6363    0.7333    0.6813     48865\n",
      "\n",
      "    accuracy                         0.8324    200000\n",
      "   macro avg     0.7728    0.7989    0.7838    200000\n",
      "weighted avg     0.8426    0.8324    0.8362    200000\n",
      "\n",
      "MLP------------------\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " best_threshold 0.3999999999999998 best_score 0.781233882573521\n",
      "0.8984413974371099\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9093    0.8605    0.8842    151135\n",
      "           1     0.6300    0.7345    0.6782     48865\n",
      "\n",
      "    accuracy                         0.8297    200000\n",
      "   macro avg     0.7696    0.7975    0.7812    200000\n",
      "weighted avg     0.8411    0.8297    0.8339    200000\n",
      "\n",
      "===========fold 1================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "os_name_macos                    0\n",
      "os_name_windows                  0\n",
      "age_group_15-17                  0\n",
      "age_group_18-24                  0\n",
      "age_group_25-34                  0\n",
      "age_group_35-44                  0\n",
      "age_group_45-54                  0\n",
      "age_group_55+                    0\n",
      "age_group_under 14               0\n",
      "age_group_unknown                0\n",
      "gender_female                    0\n",
      "gender_male                      0\n",
      "gender_unknown                   0\n",
      "country_IN                       0\n",
      "country_UNKNOWN                  0\n",
      "country_VN                       0\n",
      "region_Central Vietnam           0\n",
      "region_Northern Vietnam          0\n",
      "region_Southern Vietnam          0\n",
      "region_None                      0\n",
      "province_type_rural              0\n",
      "province_type_unknown            0\n",
      "province_type_urban              0\n",
      "clicks                           0\n",
      "search_volume                    0\n",
      "dating_search                    0\n",
      "videoclip_search                 0\n",
      "technical_search                 0\n",
      "housekeeping_family_search       0\n",
      "marketing_search                 0\n",
      "other_search                     0\n",
      "serp_click                       0\n",
      "search_volume_gg                 0\n",
      "search_clicks_gg                 0\n",
      "other_search_gg                  0\n",
      "housekeeping_family_search_gg    0\n",
      "videoclip_search_gg              0\n",
      "dating_search_gg                 0\n",
      "marketing_search_gg              0\n",
      "technical_search_gg              0\n",
      "active_day                       0\n",
      "life_time                        0\n",
      "not_active_day                   0\n",
      "total_active_time                0\n",
      "ads_impression                   0\n",
      "ads_click                        0\n",
      "ads_revenue                      0\n",
      "newtab_count                     0\n",
      "download_count                   0\n",
      "pip_count                        0\n",
      "sidebar_count                    0\n",
      "incognito_count                  0\n",
      "signin_count                     0\n",
      "youtube_count                    0\n",
      "work_count                       0\n",
      "social_count                     0\n",
      "news_count                       0\n",
      "entertainment_count              0\n",
      "ecommerce_count                  0\n",
      "dtype: int64\n",
      "LOGREG--------------\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " best_threshold 0.4299999999999998 best_score 0.7628184153791946\n",
      "0.878279204187073\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8843    0.8837    0.8840    151135\n",
      "           1     0.6410    0.6423    0.6416     48865\n",
      "\n",
      "    accuracy                         0.8247    200000\n",
      "   macro avg     0.7626    0.7630    0.7628    200000\n",
      "weighted avg     0.8248    0.8247    0.8248    200000\n",
      "\n",
      "Random Forest--------------\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " best_threshold 0.3999999999999998 best_score 0.7822318529709\n",
      "0.8997557147235633\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9046    0.8710    0.8875    151135\n",
      "           1     0.6421    0.7158    0.6770     48865\n",
      "\n",
      "    accuracy                         0.8331    200000\n",
      "   macro avg     0.7734    0.7934    0.7822    200000\n",
      "weighted avg     0.8405    0.8331    0.8361    200000\n",
      "\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 195463, number of negative: 604537\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.075144 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7397\n",
      "[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 59\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244329 -> initscore=-1.129092\n",
      "[LightGBM] [Info] Start training from score -1.129092\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[50]\ttraining's binary_logloss: 0.327701\ttraining's auc: 0.901278\tvalid_1's binary_logloss: 0.329163\tvalid_1's auc: 0.90025\n",
      "[100]\ttraining's binary_logloss: 0.324587\ttraining's auc: 0.902898\tvalid_1's binary_logloss: 0.327248\tvalid_1's auc: 0.901113\n",
      "[150]\ttraining's binary_logloss: 0.322966\ttraining's auc: 0.903907\tvalid_1's binary_logloss: 0.326934\tvalid_1's auc: 0.901266\n",
      "[200]\ttraining's binary_logloss: 0.32155\ttraining's auc: 0.904822\tvalid_1's binary_logloss: 0.326726\tvalid_1's auc: 0.901377\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[196]\ttraining's binary_logloss: 0.321656\ttraining's auc: 0.904751\tvalid_1's binary_logloss: 0.326725\tvalid_1's auc: 0.901377\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " best_threshold 0.3999999999999998 best_score 0.7833324112559489\n",
      "0.90137705739657\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9062    0.8696    0.8875    151135\n",
      "           1     0.6415    0.7215    0.6791     48865\n",
      "\n",
      "    accuracy                         0.8334    200000\n",
      "   macro avg     0.7738    0.7956    0.7833    200000\n",
      "weighted avg     0.8415    0.8334    0.8366    200000\n",
      "\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-logloss:0.54363\tvalidation_0-auc:0.89495\tvalidation_1-logloss:0.54378\tvalidation_1-auc:0.89222\n",
      "[50]\tvalidation_0-logloss:0.34347\tvalidation_0-auc:0.90537\tvalidation_1-logloss:0.34870\tvalidation_1-auc:0.90035\n",
      "[100]\tvalidation_0-logloss:0.32017\tvalidation_0-auc:0.90804\tvalidation_1-logloss:0.32936\tvalidation_1-auc:0.90110\n",
      "[150]\tvalidation_0-logloss:0.31430\tvalidation_0-auc:0.91010\tvalidation_1-logloss:0.32688\tvalidation_1-auc:0.90134\n",
      "[199]\tvalidation_0-logloss:0.31140\tvalidation_0-auc:0.91169\tvalidation_1-logloss:0.32652\tvalidation_1-auc:0.90143\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " best_threshold 0.3899999999999999 best_score 0.7831229054739637\n",
      "0.9014466930435452\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9084    0.8651    0.8862    151135\n",
      "           1     0.6364    0.7301    0.6800     48865\n",
      "\n",
      "    accuracy                         0.8321    200000\n",
      "   macro avg     0.7724    0.7976    0.7831    200000\n",
      "weighted avg     0.8419    0.8321    0.8358    200000\n",
      "\n",
      "MLP------------------\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " best_threshold 0.3899999999999999 best_score 0.779841569653607\n",
      "0.8982577305306861\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9069    0.8629    0.8843    151135\n",
      "           1     0.6313    0.7260    0.6753     48865\n",
      "\n",
      "    accuracy                         0.8295    200000\n",
      "   macro avg     0.7691    0.7944    0.7798    200000\n",
      "weighted avg     0.8396    0.8295    0.8333    200000\n",
      "\n",
      "===========fold 2================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "os_name_macos                    0\n",
      "os_name_windows                  0\n",
      "age_group_15-17                  0\n",
      "age_group_18-24                  0\n",
      "age_group_25-34                  0\n",
      "age_group_35-44                  0\n",
      "age_group_45-54                  0\n",
      "age_group_55+                    0\n",
      "age_group_under 14               0\n",
      "age_group_unknown                0\n",
      "gender_female                    0\n",
      "gender_male                      0\n",
      "gender_unknown                   0\n",
      "country_IN                       0\n",
      "country_UNKNOWN                  0\n",
      "country_VN                       0\n",
      "region_Central Vietnam           0\n",
      "region_Northern Vietnam          0\n",
      "region_Southern Vietnam          0\n",
      "region_None                      0\n",
      "province_type_rural              0\n",
      "province_type_unknown            0\n",
      "province_type_urban              0\n",
      "clicks                           0\n",
      "search_volume                    0\n",
      "dating_search                    0\n",
      "videoclip_search                 0\n",
      "technical_search                 0\n",
      "housekeeping_family_search       0\n",
      "marketing_search                 0\n",
      "other_search                     0\n",
      "serp_click                       0\n",
      "search_volume_gg                 0\n",
      "search_clicks_gg                 0\n",
      "other_search_gg                  0\n",
      "housekeeping_family_search_gg    0\n",
      "videoclip_search_gg              0\n",
      "dating_search_gg                 0\n",
      "marketing_search_gg              0\n",
      "technical_search_gg              0\n",
      "active_day                       0\n",
      "life_time                        0\n",
      "not_active_day                   0\n",
      "total_active_time                0\n",
      "ads_impression                   0\n",
      "ads_click                        0\n",
      "ads_revenue                      0\n",
      "newtab_count                     0\n",
      "download_count                   0\n",
      "pip_count                        0\n",
      "sidebar_count                    0\n",
      "incognito_count                  0\n",
      "signin_count                     0\n",
      "youtube_count                    0\n",
      "work_count                       0\n",
      "social_count                     0\n",
      "news_count                       0\n",
      "entertainment_count              0\n",
      "ecommerce_count                  0\n",
      "dtype: int64\n",
      "LOGREG--------------\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " best_threshold 0.3899999999999999 best_score 0.7630961394390932\n",
      "0.8786601012277819\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8909    0.8690    0.8798    151134\n",
      "           1     0.6235    0.6710    0.6464     48866\n",
      "\n",
      "    accuracy                         0.8206    200000\n",
      "   macro avg     0.7572    0.7700    0.7631    200000\n",
      "weighted avg     0.8256    0.8206    0.8228    200000\n",
      "\n",
      "Random Forest--------------\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " best_threshold 0.3899999999999999 best_score 0.7820067376776756\n",
      "0.9000673241377466\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9072    0.8656    0.8859    151134\n",
      "           1     0.6360    0.7262    0.6781     48866\n",
      "\n",
      "    accuracy                         0.8315    200000\n",
      "   macro avg     0.7716    0.7959    0.7820    200000\n",
      "weighted avg     0.8409    0.8315    0.8351    200000\n",
      "\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 195462, number of negative: 604538\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061467 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7351\n",
      "[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 59\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244328 -> initscore=-1.129099\n",
      "[LightGBM] [Info] Start training from score -1.129099\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[50]\ttraining's binary_logloss: 0.327829\ttraining's auc: 0.901191\tvalid_1's binary_logloss: 0.328658\tvalid_1's auc: 0.900672\n",
      "[100]\ttraining's binary_logloss: 0.324747\ttraining's auc: 0.902791\tvalid_1's binary_logloss: 0.326889\tvalid_1's auc: 0.901379\n",
      "[150]\ttraining's binary_logloss: 0.323139\ttraining's auc: 0.903801\tvalid_1's binary_logloss: 0.326516\tvalid_1's auc: 0.901568\n",
      "[200]\ttraining's binary_logloss: 0.321643\ttraining's auc: 0.904734\tvalid_1's binary_logloss: 0.326251\tvalid_1's auc: 0.901686\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[200]\ttraining's binary_logloss: 0.321643\ttraining's auc: 0.904734\tvalid_1's binary_logloss: 0.326251\tvalid_1's auc: 0.901686\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " best_threshold 0.4199999999999998 best_score 0.7833066590630473\n",
      "0.9016861905026391\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9002    0.8814    0.8907    151134\n",
      "           1     0.6554    0.6978    0.6759     48866\n",
      "\n",
      "    accuracy                         0.8365    200000\n",
      "   macro avg     0.7778    0.7896    0.7833    200000\n",
      "weighted avg     0.8404    0.8365    0.8382    200000\n",
      "\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-logloss:0.54357\tvalidation_0-auc:0.89509\tvalidation_1-logloss:0.54367\tvalidation_1-auc:0.89268\n",
      "[50]\tvalidation_0-logloss:0.34347\tvalidation_0-auc:0.90541\tvalidation_1-logloss:0.34837\tvalidation_1-auc:0.90059\n",
      "[100]\tvalidation_0-logloss:0.32016\tvalidation_0-auc:0.90803\tvalidation_1-logloss:0.32890\tvalidation_1-auc:0.90141\n",
      "[150]\tvalidation_0-logloss:0.31425\tvalidation_0-auc:0.91013\tvalidation_1-logloss:0.32634\tvalidation_1-auc:0.90174\n",
      "[199]\tvalidation_0-logloss:0.31147\tvalidation_0-auc:0.91167\tvalidation_1-logloss:0.32595\tvalidation_1-auc:0.90183\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " best_threshold 0.3999999999999998 best_score 0.7833090380956453\n",
      "0.9018314741687916\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9058    0.8704    0.8877    151134\n",
      "           1     0.6423    0.7199    0.6789     48866\n",
      "\n",
      "    accuracy                         0.8336    200000\n",
      "   macro avg     0.7740    0.7951    0.7833    200000\n",
      "weighted avg     0.8414    0.8336    0.8367    200000\n",
      "\n",
      "MLP------------------\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " best_threshold 0.3999999999999998 best_score 0.7805147844582552\n",
      "0.8985014798918414\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9054    0.8667    0.8857    151134\n",
      "           1     0.6359    0.7200    0.6754     48866\n",
      "\n",
      "    accuracy                         0.8309    200000\n",
      "   macro avg     0.7707    0.7934    0.7805    200000\n",
      "weighted avg     0.8396    0.8309    0.8343    200000\n",
      "\n",
      "===========fold 3================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "os_name_macos                    0\n",
      "os_name_windows                  0\n",
      "age_group_15-17                  0\n",
      "age_group_18-24                  0\n",
      "age_group_25-34                  0\n",
      "age_group_35-44                  0\n",
      "age_group_45-54                  0\n",
      "age_group_55+                    0\n",
      "age_group_under 14               0\n",
      "age_group_unknown                0\n",
      "gender_female                    0\n",
      "gender_male                      0\n",
      "gender_unknown                   0\n",
      "country_IN                       0\n",
      "country_UNKNOWN                  0\n",
      "country_VN                       0\n",
      "region_Central Vietnam           0\n",
      "region_Northern Vietnam          0\n",
      "region_Southern Vietnam          0\n",
      "region_None                      0\n",
      "province_type_rural              0\n",
      "province_type_unknown            0\n",
      "province_type_urban              0\n",
      "clicks                           0\n",
      "search_volume                    0\n",
      "dating_search                    0\n",
      "videoclip_search                 0\n",
      "technical_search                 0\n",
      "housekeeping_family_search       0\n",
      "marketing_search                 0\n",
      "other_search                     0\n",
      "serp_click                       0\n",
      "search_volume_gg                 0\n",
      "search_clicks_gg                 0\n",
      "other_search_gg                  0\n",
      "housekeeping_family_search_gg    0\n",
      "videoclip_search_gg              0\n",
      "dating_search_gg                 0\n",
      "marketing_search_gg              0\n",
      "technical_search_gg              0\n",
      "active_day                       0\n",
      "life_time                        0\n",
      "not_active_day                   0\n",
      "total_active_time                0\n",
      "ads_impression                   0\n",
      "ads_click                        0\n",
      "ads_revenue                      0\n",
      "newtab_count                     0\n",
      "download_count                   0\n",
      "pip_count                        0\n",
      "sidebar_count                    0\n",
      "incognito_count                  0\n",
      "signin_count                     0\n",
      "youtube_count                    0\n",
      "work_count                       0\n",
      "social_count                     0\n",
      "news_count                       0\n",
      "entertainment_count              0\n",
      "ecommerce_count                  0\n",
      "dtype: int64\n",
      "LOGREG--------------\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " best_threshold 0.4299999999999998 best_score 0.7649666770014838\n",
      "0.8788777744357759\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8849    0.8856    0.8853    151134\n",
      "           1     0.6454    0.6439    0.6446     48866\n",
      "\n",
      "    accuracy                         0.8266    200000\n",
      "   macro avg     0.7652    0.7648    0.7650    200000\n",
      "weighted avg     0.8264    0.8266    0.8265    200000\n",
      "\n",
      "Random Forest--------------\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " best_threshold 0.3999999999999998 best_score 0.7829175548090719\n",
      "0.8999716829103334\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9048    0.8717    0.8879    151134\n",
      "           1     0.6435    0.7162    0.6779     48866\n",
      "\n",
      "    accuracy                         0.8337    200000\n",
      "   macro avg     0.7741    0.7939    0.7829    200000\n",
      "weighted avg     0.8409    0.8337    0.8366    200000\n",
      "\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 195462, number of negative: 604538\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.071067 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7395\n",
      "[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 59\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244328 -> initscore=-1.129099\n",
      "[LightGBM] [Info] Start training from score -1.129099\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[50]\ttraining's binary_logloss: 0.327795\ttraining's auc: 0.901256\tvalid_1's binary_logloss: 0.328742\tvalid_1's auc: 0.9005\n",
      "[100]\ttraining's binary_logloss: 0.32482\ttraining's auc: 0.902775\tvalid_1's binary_logloss: 0.32708\tvalid_1's auc: 0.901223\n",
      "[150]\ttraining's binary_logloss: 0.323021\ttraining's auc: 0.903859\tvalid_1's binary_logloss: 0.326459\tvalid_1's auc: 0.901561\n",
      "[200]\ttraining's binary_logloss: 0.321579\ttraining's auc: 0.904754\tvalid_1's binary_logloss: 0.326241\tvalid_1's auc: 0.90167\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[200]\ttraining's binary_logloss: 0.321579\ttraining's auc: 0.904754\tvalid_1's binary_logloss: 0.326241\tvalid_1's auc: 0.90167\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " best_threshold 0.3999999999999998 best_score 0.783630326159713\n",
      "0.9016704095894232\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9061    0.8703    0.8878    151134\n",
      "           1     0.6425    0.7209    0.6794     48866\n",
      "\n",
      "    accuracy                         0.8338    200000\n",
      "   macro avg     0.7743    0.7956    0.7836    200000\n",
      "weighted avg     0.8417    0.8338    0.8369    200000\n",
      "\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-logloss:0.54367\tvalidation_0-auc:0.89465\tvalidation_1-logloss:0.54380\tvalidation_1-auc:0.89161\n",
      "[50]\tvalidation_0-logloss:0.34347\tvalidation_0-auc:0.90542\tvalidation_1-logloss:0.34820\tvalidation_1-auc:0.90059\n",
      "[100]\tvalidation_0-logloss:0.32009\tvalidation_0-auc:0.90808\tvalidation_1-logloss:0.32880\tvalidation_1-auc:0.90137\n",
      "[150]\tvalidation_0-logloss:0.31412\tvalidation_0-auc:0.91016\tvalidation_1-logloss:0.32631\tvalidation_1-auc:0.90168\n",
      "[199]\tvalidation_0-logloss:0.31156\tvalidation_0-auc:0.91157\tvalidation_1-logloss:0.32608\tvalidation_1-auc:0.90172\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " best_threshold 0.3999999999999998 best_score 0.7840931869527109\n",
      "0.9017295427552439\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9057    0.8717    0.8884    151134\n",
      "           1     0.6445    0.7192    0.6798     48866\n",
      "\n",
      "    accuracy                         0.8345    200000\n",
      "   macro avg     0.7751    0.7955    0.7841    200000\n",
      "weighted avg     0.8419    0.8345    0.8374    200000\n",
      "\n",
      "MLP------------------\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " best_threshold 0.3899999999999999 best_score 0.7806834822123279\n",
      "0.8986327928589302\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9035    0.8708    0.8868    151134\n",
      "           1     0.6406    0.7122    0.6745     48866\n",
      "\n",
      "    accuracy                         0.8321    200000\n",
      "   macro avg     0.7720    0.7915    0.7807    200000\n",
      "weighted avg     0.8392    0.8321    0.8350    200000\n",
      "\n",
      "===========fold 4================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "os_name_macos                    0\n",
      "os_name_windows                  0\n",
      "age_group_15-17                  0\n",
      "age_group_18-24                  0\n",
      "age_group_25-34                  0\n",
      "age_group_35-44                  0\n",
      "age_group_45-54                  0\n",
      "age_group_55+                    0\n",
      "age_group_under 14               0\n",
      "age_group_unknown                0\n",
      "gender_female                    0\n",
      "gender_male                      0\n",
      "gender_unknown                   0\n",
      "country_IN                       0\n",
      "country_UNKNOWN                  0\n",
      "country_VN                       0\n",
      "region_Central Vietnam           0\n",
      "region_Northern Vietnam          0\n",
      "region_Southern Vietnam          0\n",
      "region_None                      0\n",
      "province_type_rural              0\n",
      "province_type_unknown            0\n",
      "province_type_urban              0\n",
      "clicks                           0\n",
      "search_volume                    0\n",
      "dating_search                    0\n",
      "videoclip_search                 0\n",
      "technical_search                 0\n",
      "housekeeping_family_search       0\n",
      "marketing_search                 0\n",
      "other_search                     0\n",
      "serp_click                       0\n",
      "search_volume_gg                 0\n",
      "search_clicks_gg                 0\n",
      "other_search_gg                  0\n",
      "housekeeping_family_search_gg    0\n",
      "videoclip_search_gg              0\n",
      "dating_search_gg                 0\n",
      "marketing_search_gg              0\n",
      "technical_search_gg              0\n",
      "active_day                       0\n",
      "life_time                        0\n",
      "not_active_day                   0\n",
      "total_active_time                0\n",
      "ads_impression                   0\n",
      "ads_click                        0\n",
      "ads_revenue                      0\n",
      "newtab_count                     0\n",
      "download_count                   0\n",
      "pip_count                        0\n",
      "sidebar_count                    0\n",
      "incognito_count                  0\n",
      "signin_count                     0\n",
      "youtube_count                    0\n",
      "work_count                       0\n",
      "social_count                     0\n",
      "news_count                       0\n",
      "entertainment_count              0\n",
      "ecommerce_count                  0\n",
      "dtype: int64\n",
      "LOGREG--------------\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " best_threshold 0.4199999999999998 best_score 0.7633275617276017\n",
      "0.8782332369832531\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8863    0.8797    0.8830    151134\n",
      "           1     0.6364    0.6511    0.6436     48866\n",
      "\n",
      "    accuracy                         0.8238    200000\n",
      "   macro avg     0.7614    0.7654    0.7633    200000\n",
      "weighted avg     0.8253    0.8238    0.8245    200000\n",
      "\n",
      "Random Forest--------------\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " best_threshold 0.4099999999999998 best_score 0.7820695510925804\n",
      "0.8999866410420176\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9021    0.8757    0.8887    151134\n",
      "           1     0.6474    0.7061    0.6755     48866\n",
      "\n",
      "    accuracy                         0.8342    200000\n",
      "   macro avg     0.7747    0.7909    0.7821    200000\n",
      "weighted avg     0.8399    0.8342    0.8366    200000\n",
      "\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 195462, number of negative: 604538\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.059949 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7383\n",
      "[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 59\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244328 -> initscore=-1.129099\n",
      "[LightGBM] [Info] Start training from score -1.129099\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[50]\ttraining's binary_logloss: 0.327788\ttraining's auc: 0.901208\tvalid_1's binary_logloss: 0.328752\tvalid_1's auc: 0.900585\n",
      "[100]\ttraining's binary_logloss: 0.324721\ttraining's auc: 0.902783\tvalid_1's binary_logloss: 0.32675\tvalid_1's auc: 0.901461\n",
      "[150]\ttraining's binary_logloss: 0.323028\ttraining's auc: 0.903845\tvalid_1's binary_logloss: 0.326302\tvalid_1's auc: 0.901695\n",
      "[200]\ttraining's binary_logloss: 0.321631\ttraining's auc: 0.90473\tvalid_1's binary_logloss: 0.326007\tvalid_1's auc: 0.901856\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[200]\ttraining's binary_logloss: 0.321631\ttraining's auc: 0.90473\tvalid_1's binary_logloss: 0.326007\tvalid_1's auc: 0.901856\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " best_threshold 0.3899999999999999 best_score 0.7830097343163127\n",
      "0.9018557061782816\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9088    0.8642    0.8859    151134\n",
      "           1     0.6353    0.7316    0.6801     48866\n",
      "\n",
      "    accuracy                         0.8318    200000\n",
      "   macro avg     0.7720    0.7979    0.7830    200000\n",
      "weighted avg     0.8420    0.8318    0.8356    200000\n",
      "\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-logloss:0.54357\tvalidation_0-auc:0.89484\tvalidation_1-logloss:0.54367\tvalidation_1-auc:0.89253\n",
      "[50]\tvalidation_0-logloss:0.34356\tvalidation_0-auc:0.90530\tvalidation_1-logloss:0.34810\tvalidation_1-auc:0.90075\n",
      "[100]\tvalidation_0-logloss:0.32021\tvalidation_0-auc:0.90799\tvalidation_1-logloss:0.32861\tvalidation_1-auc:0.90162\n",
      "[150]\tvalidation_0-logloss:0.31429\tvalidation_0-auc:0.91007\tvalidation_1-logloss:0.32602\tvalidation_1-auc:0.90192\n",
      "[199]\tvalidation_0-logloss:0.31137\tvalidation_0-auc:0.91166\tvalidation_1-logloss:0.32567\tvalidation_1-auc:0.90199\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " best_threshold 0.3899999999999999 best_score 0.7838649203787258\n",
      "0.9019920537315473\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9093    0.8645    0.8863    151134\n",
      "           1     0.6364    0.7333    0.6814     48866\n",
      "\n",
      "    accuracy                         0.8325    200000\n",
      "   macro avg     0.7728    0.7989    0.7839    200000\n",
      "weighted avg     0.8426    0.8325    0.8363    200000\n",
      "\n",
      "MLP------------------\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " best_threshold 0.4099999999999998 best_score 0.7805928051125676\n",
      "0.8990819525669991\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9052    0.8674    0.8859    151134\n",
      "           1     0.6367    0.7189    0.6753     48866\n",
      "\n",
      "    accuracy                         0.8311    200000\n",
      "   macro avg     0.7709    0.7931    0.7806    200000\n",
      "weighted avg     0.8396    0.8311    0.8344    200000\n",
      "\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " 0.4199999999999998 0.7634821943688991\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " 0.3999999999999998 0.7821917360393063\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " 0.3999999999999998 0.7833771131563059\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " 0.3899999999999999 0.7835773919393311\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " 0.3999999999999998 0.7803996462509664\n",
      "LOGREG--------------\n",
      "accuracy 0.823535\n",
      "f1_score 0.7642047675901841\n",
      "auc 0.8787298171488469\n",
      "sensitivity 0.6579732657482729 specificity 0.8771529184373221\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8879    0.8772    0.8825    151074\n",
      "           1     0.6343    0.6580    0.6459     48926\n",
      "\n",
      "    accuracy                         0.8235    200000\n",
      "   macro avg     0.7611    0.7676    0.7642    200000\n",
      "weighted avg     0.8258    0.8235    0.8246    200000\n",
      "\n",
      "Random Forest--------------\n",
      "accuracy 0.832955\n",
      "f1_score 0.7822717567208922\n",
      "auc 0.8996805761101926\n",
      "sensitivity 0.7163471364918448 specificity 0.8707189853978845\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9046    0.8707    0.8873    151074\n",
      "           1     0.6422    0.7163    0.6772     48926\n",
      "\n",
      "    accuracy                         0.8330    200000\n",
      "   macro avg     0.7734    0.7935    0.7823    200000\n",
      "weighted avg     0.8404    0.8330    0.8359    200000\n",
      "\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 244328, number of negative: 755672\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.072066 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7373\n",
      "[LightGBM] [Info] Number of data points in the train set: 1000000, number of used features: 59\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244328 -> initscore=-1.129096\n",
      "[LightGBM] [Info] Start training from score -1.129096\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "accuracy 0.83337\n",
      "f1_score 0.7833795014778788\n",
      "auc 0.901481926218966\n",
      "sensitivity 0.7214568940849446 specificity 0.8696135668612733\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9060    0.8696    0.8874    151074\n",
      "           1     0.6418    0.7215    0.6793     48926\n",
      "\n",
      "    accuracy                         0.8334    200000\n",
      "   macro avg     0.7739    0.7955    0.7834    200000\n",
      "weighted avg     0.8414    0.8334    0.8365    200000\n",
      "\n",
      "XGBoost--------------\n",
      "{'objective': 'binary:logistic', 'eval_metric': ['logloss', 'auc'], 'n_estimators': 200, 'learning_rate': 0.034630277480196384, 'max_depth': 9, 'colsample_bytree': 0.8, 'subsample': 0.30000000000000004, 'reg_alpha': 0.0020136244579038245, 'reg_lambda': 1.3270228907353322e-06, 'seed': 42, 'enable_categorical': True}\n",
      "accuracy 0.8324\n",
      "f1_score 0.7837608995413738\n",
      "auc 0.901586912502433\n",
      "sensitivity 0.7319829947267301 specificity 0.8649205025351814\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9088    0.8649    0.8863    151074\n",
      "           1     0.6370    0.7320    0.6812     48926\n",
      "\n",
      "    accuracy                         0.8324    200000\n",
      "   macro avg     0.7729    0.7985    0.7838    200000\n",
      "weighted avg     0.8423    0.8324    0.8361    200000\n",
      "\n",
      "MLP--------------\n",
      "accuracy 0.83311\n",
      "f1_score 0.7805091459833216\n",
      "auc 0.8985109978724376\n",
      "sensitivity 0.702223766504517 specificity 0.8754981002687424\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9008    0.8755    0.8880    151074\n",
      "           1     0.6462    0.7022    0.6731     48926\n",
      "\n",
      "    accuracy                         0.8331    200000\n",
      "   macro avg     0.7735    0.7889    0.7805    200000\n",
      "weighted avg     0.8385    0.8331    0.8354    200000\n",
      "\n",
      "['log_reg', 'randomforest', 'lightgbm', 'xgboost', 'mlp']\n",
      "[0.823535, 0.832955, 0.83337, 0.8324, 0.83311]\n",
      "[0.7642047675901841, 0.7822717567208922, 0.7833795014778788, 0.7837608995413738, 0.7805091459833216]\n",
      "[0.8787298171488469, 0.8996805761101926, 0.901481926218966, 0.901586912502433, 0.8985109978724376]\n",
      "[0.8771529184373221, 0.8707189853978845, 0.8696135668612733, 0.8649205025351814, 0.8754981002687424]\n",
      "[0.6579732657482729, 0.7163471364918448, 0.7214568940849446, 0.7319829947267301, 0.702223766504517]\n"
     ]
    }
   ],
   "source": [
    "print(\"Not handle Balanced\")\n",
    "nothing_oofs = cross_validate(train)\n",
    "nothing_score_df, nothing_models, nothing_predictions = train_model(nothing_oofs,X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/natalie/Desktop/DS Thesis/user-churn-prediction/checkpoints/nothing_model_after_tune.pkl', 'wb') as f:\n",
    "    pickle.dump({\n",
    "        \"score_df\": nothing_score_df,\n",
    "        \"oofs\": nothing_oofs,\n",
    "        \"models\": nothing_models,\n",
    "        \"model_names\": MODEL_NAMES,\n",
    "        \"predictions\":nothing_predictions},f,protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use SMOTE\n",
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "os_name_macos                    0\n",
      "os_name_windows                  0\n",
      "age_group_15-17                  0\n",
      "age_group_18-24                  0\n",
      "age_group_25-34                  0\n",
      "age_group_35-44                  0\n",
      "age_group_45-54                  0\n",
      "age_group_55+                    0\n",
      "age_group_under 14               0\n",
      "age_group_unknown                0\n",
      "gender_female                    0\n",
      "gender_male                      0\n",
      "gender_unknown                   0\n",
      "country_IN                       0\n",
      "country_UNKNOWN                  0\n",
      "country_VN                       0\n",
      "region_Central Vietnam           0\n",
      "region_Northern Vietnam          0\n",
      "region_Southern Vietnam          0\n",
      "region_None                      0\n",
      "province_type_rural              0\n",
      "province_type_unknown            0\n",
      "province_type_urban              0\n",
      "clicks                           0\n",
      "search_volume                    0\n",
      "dating_search                    0\n",
      "videoclip_search                 0\n",
      "technical_search                 0\n",
      "housekeeping_family_search       0\n",
      "marketing_search                 0\n",
      "other_search                     0\n",
      "serp_click                       0\n",
      "search_volume_gg                 0\n",
      "search_clicks_gg                 0\n",
      "other_search_gg                  0\n",
      "housekeeping_family_search_gg    0\n",
      "videoclip_search_gg              0\n",
      "dating_search_gg                 0\n",
      "marketing_search_gg              0\n",
      "technical_search_gg              0\n",
      "active_day                       0\n",
      "life_time                        0\n",
      "not_active_day                   0\n",
      "total_active_time                0\n",
      "ads_impression                   0\n",
      "ads_click                        0\n",
      "ads_revenue                      0\n",
      "newtab_count                     0\n",
      "download_count                   0\n",
      "pip_count                        0\n",
      "sidebar_count                    0\n",
      "incognito_count                  0\n",
      "signin_count                     0\n",
      "youtube_count                    0\n",
      "work_count                       0\n",
      "social_count                     0\n",
      "news_count                       0\n",
      "entertainment_count              0\n",
      "ecommerce_count                  0\n",
      "dtype: int64\n",
      "SMOTEEEE\n",
      "LOGREG--------------\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " best_threshold 0.6899999999999997 best_score 0.7658578694358995\n",
      "0.8785115794503275\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8870    0.8822    0.8846    251891\n",
      "           1     0.6417    0.6526    0.6471     81443\n",
      "\n",
      "    accuracy                         0.8261    333334\n",
      "   macro avg     0.7644    0.7674    0.7659    333334\n",
      "weighted avg     0.8271    0.8261    0.8266    333334\n",
      "\n",
      "Random Forest--------------\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " best_threshold 0.5199999999999998 best_score 0.7786094473648886\n",
      "0.896683784384123\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9079    0.8592    0.8828    251891\n",
      "           1     0.6264    0.7303    0.6744     81443\n",
      "\n",
      "    accuracy                         0.8277    333334\n",
      "   macro avg     0.7671    0.7947    0.7786    333334\n",
      "weighted avg     0.8391    0.8277    0.8319    333334\n",
      "\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 503781, number of negative: 503781\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.135834 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14801\n",
      "[LightGBM] [Info] Number of data points in the train set: 1007562, number of used features: 59\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[50]\ttraining's binary_logloss: 0.252902\ttraining's auc: 0.963329\tvalid_1's binary_logloss: 0.340485\tvalid_1's auc: 0.897233\n",
      "[100]\ttraining's binary_logloss: 0.230095\ttraining's auc: 0.966739\tvalid_1's binary_logloss: 0.331792\tvalid_1's auc: 0.899185\n",
      "[150]\ttraining's binary_logloss: 0.223808\ttraining's auc: 0.967715\tvalid_1's binary_logloss: 0.32957\tvalid_1's auc: 0.900065\n",
      "[200]\ttraining's binary_logloss: 0.220726\ttraining's auc: 0.968266\tvalid_1's binary_logloss: 0.328564\tvalid_1's auc: 0.900506\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[200]\ttraining's binary_logloss: 0.220726\ttraining's auc: 0.968266\tvalid_1's binary_logloss: 0.328564\tvalid_1's auc: 0.900506\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " best_threshold 0.3999999999999998 best_score 0.7821568009604196\n",
      "0.9005057980425294\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9083    0.8638    0.8855    251891\n",
      "           1     0.6342    0.7303    0.6788     81443\n",
      "\n",
      "    accuracy                         0.8312    333334\n",
      "   macro avg     0.7712    0.7970    0.7822    333334\n",
      "weighted avg     0.8413    0.8312    0.8350    333334\n",
      "\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-logloss:0.67199\tvalidation_0-auc:0.94426\tvalidation_1-logloss:0.67457\tvalidation_1-auc:0.89100\n",
      "[50]\tvalidation_0-logloss:0.31366\tvalidation_0-auc:0.95988\tvalidation_1-logloss:0.37786\tvalidation_1-auc:0.89759\n",
      "[100]\tvalidation_0-logloss:0.25797\tvalidation_0-auc:0.96373\tvalidation_1-logloss:0.34171\tvalidation_1-auc:0.89889\n",
      "[150]\tvalidation_0-logloss:0.23827\tvalidation_0-auc:0.96649\tvalidation_1-logloss:0.33362\tvalidation_1-auc:0.89967\n",
      "[199]\tvalidation_0-logloss:0.22847\tvalidation_0-auc:0.96806\tvalidation_1-logloss:0.33102\tvalidation_1-auc:0.90002\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " best_threshold 0.4299999999999998 best_score 0.7816458563330806\n",
      "0.9000210339934306\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9072    0.8650    0.8856    251891\n",
      "           1     0.6351    0.7264    0.6777     81443\n",
      "\n",
      "    accuracy                         0.8312    333334\n",
      "   macro avg     0.7711    0.7957    0.7816    333334\n",
      "weighted avg     0.8407    0.8312    0.8348    333334\n",
      "\n",
      "MLP------------------\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " best_threshold 0.5699999999999997 best_score 0.7748602987702584\n",
      "0.8941382455244868\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9104    0.8486    0.8784    251891\n",
      "           1     0.6130    0.7419    0.6713     81443\n",
      "\n",
      "    accuracy                         0.8225    333334\n",
      "   macro avg     0.7617    0.7952    0.7749    333334\n",
      "weighted avg     0.8378    0.8225    0.8278    333334\n",
      "\n",
      "===========fold 1================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "os_name_macos                    0\n",
      "os_name_windows                  0\n",
      "age_group_15-17                  0\n",
      "age_group_18-24                  0\n",
      "age_group_25-34                  0\n",
      "age_group_35-44                  0\n",
      "age_group_45-54                  0\n",
      "age_group_55+                    0\n",
      "age_group_under 14               0\n",
      "age_group_unknown                0\n",
      "gender_female                    0\n",
      "gender_male                      0\n",
      "gender_unknown                   0\n",
      "country_IN                       0\n",
      "country_UNKNOWN                  0\n",
      "country_VN                       0\n",
      "region_Central Vietnam           0\n",
      "region_Northern Vietnam          0\n",
      "region_Southern Vietnam          0\n",
      "region_None                      0\n",
      "province_type_rural              0\n",
      "province_type_unknown            0\n",
      "province_type_urban              0\n",
      "clicks                           0\n",
      "search_volume                    0\n",
      "dating_search                    0\n",
      "videoclip_search                 0\n",
      "technical_search                 0\n",
      "housekeeping_family_search       0\n",
      "marketing_search                 0\n",
      "other_search                     0\n",
      "serp_click                       0\n",
      "search_volume_gg                 0\n",
      "search_clicks_gg                 0\n",
      "other_search_gg                  0\n",
      "housekeeping_family_search_gg    0\n",
      "videoclip_search_gg              0\n",
      "dating_search_gg                 0\n",
      "marketing_search_gg              0\n",
      "technical_search_gg              0\n",
      "active_day                       0\n",
      "life_time                        0\n",
      "not_active_day                   0\n",
      "total_active_time                0\n",
      "ads_impression                   0\n",
      "ads_click                        0\n",
      "ads_revenue                      0\n",
      "newtab_count                     0\n",
      "download_count                   0\n",
      "pip_count                        0\n",
      "sidebar_count                    0\n",
      "incognito_count                  0\n",
      "signin_count                     0\n",
      "youtube_count                    0\n",
      "work_count                       0\n",
      "social_count                     0\n",
      "news_count                       0\n",
      "entertainment_count              0\n",
      "ecommerce_count                  0\n",
      "dtype: int64\n",
      "SMOTEEEE\n",
      "LOGREG--------------\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " best_threshold 0.6899999999999997 best_score 0.7665611498989624\n",
      "0.8787734265035797\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8876    0.8820    0.8848    251891\n",
      "           1     0.6421    0.6546    0.6483     81442\n",
      "\n",
      "    accuracy                         0.8265    333333\n",
      "   macro avg     0.7649    0.7683    0.7666    333333\n",
      "weighted avg     0.8276    0.8265    0.8270    333333\n",
      "\n",
      "Random Forest--------------\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " best_threshold 0.5199999999999998 best_score 0.7789910788141705\n",
      "0.8970415230389593\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9081    0.8593    0.8830    251891\n",
      "           1     0.6268    0.7311    0.6750     81442\n",
      "\n",
      "    accuracy                         0.8280    333333\n",
      "   macro avg     0.7675    0.7952    0.7790    333333\n",
      "weighted avg     0.8394    0.8280    0.8322    333333\n",
      "\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 503781, number of negative: 503781\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.129390 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14803\n",
      "[LightGBM] [Info] Number of data points in the train set: 1007562, number of used features: 59\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[50]\ttraining's binary_logloss: 0.254291\ttraining's auc: 0.963045\tvalid_1's binary_logloss: 0.340201\tvalid_1's auc: 0.897656\n",
      "[100]\ttraining's binary_logloss: 0.230401\ttraining's auc: 0.966636\tvalid_1's binary_logloss: 0.331111\tvalid_1's auc: 0.899803\n",
      "[150]\ttraining's binary_logloss: 0.223602\ttraining's auc: 0.967649\tvalid_1's binary_logloss: 0.32853\tvalid_1's auc: 0.900786\n",
      "[200]\ttraining's binary_logloss: 0.220619\ttraining's auc: 0.968199\tvalid_1's binary_logloss: 0.327491\tvalid_1's auc: 0.901265\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[200]\ttraining's binary_logloss: 0.220619\ttraining's auc: 0.968199\tvalid_1's binary_logloss: 0.327491\tvalid_1's auc: 0.901265\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " best_threshold 0.4099999999999998 best_score 0.7833896413216394\n",
      "0.901264749156542\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9053    0.8714    0.8880    251891\n",
      "           1     0.6435    0.7181    0.6788     81442\n",
      "\n",
      "    accuracy                         0.8339    333333\n",
      "   macro avg     0.7744    0.7947    0.7834    333333\n",
      "weighted avg     0.8413    0.8339    0.8369    333333\n",
      "\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-logloss:0.67206\tvalidation_0-auc:0.94349\tvalidation_1-logloss:0.67455\tvalidation_1-auc:0.89205\n",
      "[50]\tvalidation_0-logloss:0.31624\tvalidation_0-auc:0.95888\tvalidation_1-logloss:0.37810\tvalidation_1-auc:0.89792\n",
      "[100]\tvalidation_0-logloss:0.26023\tvalidation_0-auc:0.96315\tvalidation_1-logloss:0.34181\tvalidation_1-auc:0.89920\n",
      "[150]\tvalidation_0-logloss:0.24001\tvalidation_0-auc:0.96611\tvalidation_1-logloss:0.33358\tvalidation_1-auc:0.89996\n",
      "[199]\tvalidation_0-logloss:0.22960\tvalidation_0-auc:0.96778\tvalidation_1-logloss:0.33068\tvalidation_1-auc:0.90038\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " best_threshold 0.4299999999999998 best_score 0.782482554404039\n",
      "0.900387717080845\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9081    0.8647    0.8859    251891\n",
      "           1     0.6354    0.7292    0.6791     81442\n",
      "\n",
      "    accuracy                         0.8316    333333\n",
      "   macro avg     0.7717    0.7970    0.7825    333333\n",
      "weighted avg     0.8414    0.8316    0.8353    333333\n",
      "\n",
      "MLP------------------\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " best_threshold 0.6799999999999997 best_score 0.7773519684196188\n",
      "0.8960398198940431\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9080    0.8570    0.8818    251891\n",
      "           1     0.6232    0.7314    0.6729     81442\n",
      "\n",
      "    accuracy                         0.8263    333333\n",
      "   macro avg     0.7656    0.7942    0.7774    333333\n",
      "weighted avg     0.8384    0.8263    0.8307    333333\n",
      "\n",
      "===========fold 2================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "os_name_macos                    0\n",
      "os_name_windows                  0\n",
      "age_group_15-17                  0\n",
      "age_group_18-24                  0\n",
      "age_group_25-34                  0\n",
      "age_group_35-44                  0\n",
      "age_group_45-54                  0\n",
      "age_group_55+                    0\n",
      "age_group_under 14               0\n",
      "age_group_unknown                0\n",
      "gender_female                    0\n",
      "gender_male                      0\n",
      "gender_unknown                   0\n",
      "country_IN                       0\n",
      "country_UNKNOWN                  0\n",
      "country_VN                       0\n",
      "region_Central Vietnam           0\n",
      "region_Northern Vietnam          0\n",
      "region_Southern Vietnam          0\n",
      "region_None                      0\n",
      "province_type_rural              0\n",
      "province_type_unknown            0\n",
      "province_type_urban              0\n",
      "clicks                           0\n",
      "search_volume                    0\n",
      "dating_search                    0\n",
      "videoclip_search                 0\n",
      "technical_search                 0\n",
      "housekeeping_family_search       0\n",
      "marketing_search                 0\n",
      "other_search                     0\n",
      "serp_click                       0\n",
      "search_volume_gg                 0\n",
      "search_clicks_gg                 0\n",
      "other_search_gg                  0\n",
      "housekeeping_family_search_gg    0\n",
      "videoclip_search_gg              0\n",
      "dating_search_gg                 0\n",
      "marketing_search_gg              0\n",
      "technical_search_gg              0\n",
      "active_day                       0\n",
      "life_time                        0\n",
      "not_active_day                   0\n",
      "total_active_time                0\n",
      "ads_impression                   0\n",
      "ads_click                        0\n",
      "ads_revenue                      0\n",
      "newtab_count                     0\n",
      "download_count                   0\n",
      "pip_count                        0\n",
      "sidebar_count                    0\n",
      "incognito_count                  0\n",
      "signin_count                     0\n",
      "youtube_count                    0\n",
      "work_count                       0\n",
      "social_count                     0\n",
      "news_count                       0\n",
      "entertainment_count              0\n",
      "ecommerce_count                  0\n",
      "dtype: int64\n",
      "SMOTEEEE\n",
      "LOGREG--------------\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " best_threshold 0.6599999999999997 best_score 0.7668907723004809\n",
      "0.8787236800874129\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8954    0.8656    0.8802    251890\n",
      "           1     0.6231    0.6872    0.6536     81443\n",
      "\n",
      "    accuracy                         0.8220    333333\n",
      "   macro avg     0.7592    0.7764    0.7669    333333\n",
      "weighted avg     0.8288    0.8220    0.8248    333333\n",
      "\n",
      "Random Forest--------------\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " best_threshold 0.5199999999999998 best_score 0.7781519291185377\n",
      "0.8970571974783984\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9083    0.8576    0.8822    251890\n",
      "           1     0.6245    0.7322    0.6741     81443\n",
      "\n",
      "    accuracy                         0.8270    333333\n",
      "   macro avg     0.7664    0.7949    0.7782    333333\n",
      "weighted avg     0.8390    0.8270    0.8314    333333\n",
      "\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 503782, number of negative: 503782\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.128935 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14809\n",
      "[LightGBM] [Info] Number of data points in the train set: 1007564, number of used features: 59\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[50]\ttraining's binary_logloss: 0.253195\ttraining's auc: 0.963337\tvalid_1's binary_logloss: 0.340171\tvalid_1's auc: 0.897435\n",
      "[100]\ttraining's binary_logloss: 0.230252\ttraining's auc: 0.966678\tvalid_1's binary_logloss: 0.331485\tvalid_1's auc: 0.89937\n",
      "[150]\ttraining's binary_logloss: 0.223905\ttraining's auc: 0.967663\tvalid_1's binary_logloss: 0.328998\tvalid_1's auc: 0.900392\n",
      "[200]\ttraining's binary_logloss: 0.220789\ttraining's auc: 0.968221\tvalid_1's binary_logloss: 0.328018\tvalid_1's auc: 0.90085\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[200]\ttraining's binary_logloss: 0.220789\ttraining's auc: 0.968221\tvalid_1's binary_logloss: 0.328018\tvalid_1's auc: 0.90085\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " best_threshold 0.3999999999999998 best_score 0.7824719998042808\n",
      "0.9008497325973288\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9085    0.8638    0.8856    251890\n",
      "           1     0.6345    0.7310    0.6793     81443\n",
      "\n",
      "    accuracy                         0.8314    333333\n",
      "   macro avg     0.7715    0.7974    0.7825    333333\n",
      "weighted avg     0.8416    0.8314    0.8352    333333\n",
      "\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-logloss:0.67222\tvalidation_0-auc:0.94213\tvalidation_1-logloss:0.67473\tvalidation_1-auc:0.89080\n",
      "[50]\tvalidation_0-logloss:0.31569\tvalidation_0-auc:0.95927\tvalidation_1-logloss:0.37820\tvalidation_1-auc:0.89774\n",
      "[100]\tvalidation_0-logloss:0.25908\tvalidation_0-auc:0.96349\tvalidation_1-logloss:0.34161\tvalidation_1-auc:0.89901\n",
      "[150]\tvalidation_0-logloss:0.23911\tvalidation_0-auc:0.96633\tvalidation_1-logloss:0.33352\tvalidation_1-auc:0.89976\n",
      "[199]\tvalidation_0-logloss:0.22918\tvalidation_0-auc:0.96791\tvalidation_1-logloss:0.33075\tvalidation_1-auc:0.90020\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " best_threshold 0.4299999999999998 best_score 0.7814938203475374\n",
      "0.9002034660328828\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9077    0.8638    0.8852    251890\n",
      "           1     0.6337    0.7284    0.6777     81443\n",
      "\n",
      "    accuracy                         0.8308    333333\n",
      "   macro avg     0.7707    0.7961    0.7815    333333\n",
      "weighted avg     0.8408    0.8308    0.8345    333333\n",
      "\n",
      "MLP------------------\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " best_threshold 0.6199999999999998 best_score 0.7759590158456778\n",
      "0.8940526246943001\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9087    0.8536    0.8803    251890\n",
      "           1     0.6186    0.7346    0.6717     81443\n",
      "\n",
      "    accuracy                         0.8245    333333\n",
      "   macro avg     0.7636    0.7941    0.7760    333333\n",
      "weighted avg     0.8378    0.8245    0.8293    333333\n",
      "\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " 0.6899999999999997 0.766330781629651\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " 0.5199999999999998 0.7785840425734107\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " 0.3999999999999998 0.7826575994815903\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " 0.4299999999999998 0.7818741801195496\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " 0.6199999999999998 0.7743499908294352\n",
      "SMOTEEEE\n",
      "LOGREG--------------\n",
      "accuracy 0.82443\n",
      "f1_score 0.7666608483920679\n",
      "auc 0.8787814015036497\n",
      "sensitivity 0.6680701467522381 specificity 0.8750678475449117\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8906    0.8751    0.8828    151074\n",
      "           1     0.6339    0.6681    0.6506     48926\n",
      "\n",
      "    accuracy                         0.8244    200000\n",
      "   macro avg     0.7623    0.7716    0.7667    200000\n",
      "weighted avg     0.8278    0.8244    0.8260    200000\n",
      "\n",
      "Random Forest--------------\n",
      "accuracy 0.827115\n",
      "f1_score 0.7781888165392379\n",
      "auc 0.8967170974556591\n",
      "sensitivity 0.7306135796917794 specificity 0.8583674225876061\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9077    0.8584    0.8824    151074\n",
      "           1     0.6256    0.7306    0.6740     48926\n",
      "\n",
      "    accuracy                         0.8271    200000\n",
      "   macro avg     0.7666    0.7945    0.7782    200000\n",
      "weighted avg     0.8387    0.8271    0.8314    200000\n",
      "\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 755672, number of negative: 755672\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.201435 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14803\n",
      "[LightGBM] [Info] Number of data points in the train set: 1511344, number of used features: 59\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "accuracy 0.831805\n",
      "f1_score 0.7829387563748219\n",
      "auc 0.9005652674867407\n",
      "sensitivity 0.7303478722969382 specificity 0.8646623509008831\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9083    0.8647    0.8859    151074\n",
      "           1     0.6361    0.7303    0.6799     48926\n",
      "\n",
      "    accuracy                         0.8318    200000\n",
      "   macro avg     0.7722    0.7975    0.7829    200000\n",
      "weighted avg     0.8417    0.8318    0.8355    200000\n",
      "\n",
      "XGBoost--------------\n",
      "{'objective': 'binary:logistic', 'eval_metric': ['logloss', 'auc'], 'n_estimators': 200, 'learning_rate': 0.034630277480196384, 'max_depth': 9, 'colsample_bytree': 0.8, 'subsample': 0.30000000000000004, 'reg_alpha': 0.0020136244579038245, 'reg_lambda': 1.3270228907353322e-06, 'seed': 42, 'enable_categorical': True}\n",
      "accuracy 0.83125\n",
      "f1_score 0.7820716298605319\n",
      "auc 0.8998877467222003\n",
      "sensitivity 0.7280587008952295 specificity 0.8646689701735574\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9076    0.8647    0.8856    151074\n",
      "           1     0.6353    0.7281    0.6785     48926\n",
      "\n",
      "    accuracy                         0.8313    200000\n",
      "   macro avg     0.7715    0.7964    0.7821    200000\n",
      "weighted avg     0.8410    0.8313    0.8349    200000\n",
      "\n",
      "MLP--------------\n",
      "accuracy 0.825455\n",
      "f1_score 0.7724847297899295\n",
      "auc 0.8920791069907766\n",
      "sensitivity 0.7009361075910558 specificity 0.8657810079828429\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8994    0.8658    0.8823    151074\n",
      "           1     0.6284    0.7009    0.6627     48926\n",
      "\n",
      "    accuracy                         0.8255    200000\n",
      "   macro avg     0.7639    0.7834    0.7725    200000\n",
      "weighted avg     0.8331    0.8255    0.8286    200000\n",
      "\n",
      "['log_reg', 'randomforest', 'lightgbm', 'xgboost', 'mlp']\n",
      "[0.82443, 0.827115, 0.831805, 0.83125, 0.825455]\n",
      "[0.7666608483920679, 0.7781888165392379, 0.7829387563748219, 0.7820716298605319, 0.7724847297899295]\n",
      "[0.8787814015036497, 0.8967170974556591, 0.9005652674867407, 0.8998877467222003, 0.8920791069907766]\n",
      "[0.8750678475449117, 0.8583674225876061, 0.8646623509008831, 0.8646689701735574, 0.8657810079828429]\n",
      "[0.6680701467522381, 0.7306135796917794, 0.7303478722969382, 0.7280587008952295, 0.7009361075910558]\n"
     ]
    }
   ],
   "source": [
    "print(\"Use SMOTE\")\n",
    "smote_oofs = cross_validate(train, USE_SMOTE=True)\n",
    "smote_score_df, smote_models, smote_predictions = train_model(smote_oofs,X_train, y_train, X_test, y_test, USE_SMOTE=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/natalie/Desktop/DS Thesis/user-churn-prediction/checkpoints/smote_model_after_tune.pkl', 'wb') as f:\n",
    "     pickle.dump({\n",
    "             \"score_df\":smote_score_df,\n",
    "             \"oofs\": smote_oofs,\n",
    "             \"models\": smote_models,\n",
    "             \"model_names\": MODEL_NAMES,\n",
    "             \"predictions\":smote_predictions},f,protocol=pickle.HIGHEST_PROTOCOL)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use Class weight\n",
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "os_name_macos                    0\n",
      "os_name_windows                  0\n",
      "age_group_15-17                  0\n",
      "age_group_18-24                  0\n",
      "age_group_25-34                  0\n",
      "age_group_35-44                  0\n",
      "age_group_45-54                  0\n",
      "age_group_55+                    0\n",
      "age_group_under 14               0\n",
      "age_group_unknown                0\n",
      "gender_female                    0\n",
      "gender_male                      0\n",
      "gender_unknown                   0\n",
      "country_IN                       0\n",
      "country_UNKNOWN                  0\n",
      "country_VN                       0\n",
      "region_Central Vietnam           0\n",
      "region_Northern Vietnam          0\n",
      "region_Southern Vietnam          0\n",
      "region_None                      0\n",
      "province_type_rural              0\n",
      "province_type_unknown            0\n",
      "province_type_urban              0\n",
      "clicks                           0\n",
      "search_volume                    0\n",
      "dating_search                    0\n",
      "videoclip_search                 0\n",
      "technical_search                 0\n",
      "housekeeping_family_search       0\n",
      "marketing_search                 0\n",
      "other_search                     0\n",
      "serp_click                       0\n",
      "search_volume_gg                 0\n",
      "search_clicks_gg                 0\n",
      "other_search_gg                  0\n",
      "housekeeping_family_search_gg    0\n",
      "videoclip_search_gg              0\n",
      "dating_search_gg                 0\n",
      "marketing_search_gg              0\n",
      "technical_search_gg              0\n",
      "active_day                       0\n",
      "life_time                        0\n",
      "not_active_day                   0\n",
      "total_active_time                0\n",
      "ads_impression                   0\n",
      "ads_click                        0\n",
      "ads_revenue                      0\n",
      "newtab_count                     0\n",
      "download_count                   0\n",
      "pip_count                        0\n",
      "sidebar_count                    0\n",
      "incognito_count                  0\n",
      "signin_count                     0\n",
      "youtube_count                    0\n",
      "work_count                       0\n",
      "social_count                     0\n",
      "news_count                       0\n",
      "entertainment_count              0\n",
      "ecommerce_count                  0\n",
      "dtype: int64\n",
      "CLASS_WEIGHTTTT\n",
      "LOGREG--------------\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " best_threshold 0.6999999999999996 best_score 0.7658413465358792\n",
      "0.8789427542510527\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8880    0.8800    0.8840    251891\n",
      "           1     0.6389    0.6567    0.6477     81443\n",
      "\n",
      "    accuracy                         0.8255    333334\n",
      "   macro avg     0.7635    0.7684    0.7658    333334\n",
      "weighted avg     0.8271    0.8255    0.8263    333334\n",
      "\n",
      "Random Forest--------------\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " best_threshold 0.6699999999999997 best_score 0.7817177217355398\n",
      "0.8996357637540593\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9042    0.8709    0.8873    251891\n",
      "           1     0.6416    0.7147    0.6762     81443\n",
      "\n",
      "    accuracy                         0.8327    333334\n",
      "   macro avg     0.7729    0.7928    0.7817    333334\n",
      "weighted avg     0.8401    0.8327    0.8357    333334\n",
      "\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162885, number of negative: 503781\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.044970 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7373\n",
      "[LightGBM] [Info] Number of data points in the train set: 666666, number of used features: 59\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[50]\ttraining's binary_logloss: 0.389241\ttraining's auc: 0.901211\tvalid_1's binary_logloss: 0.401578\tvalid_1's auc: 0.900021\n",
      "[100]\ttraining's binary_logloss: 0.384631\ttraining's auc: 0.902966\tvalid_1's binary_logloss: 0.399081\tvalid_1's auc: 0.900753\n",
      "[150]\ttraining's binary_logloss: 0.381957\ttraining's auc: 0.904092\tvalid_1's binary_logloss: 0.398298\tvalid_1's auc: 0.900941\n",
      "[200]\ttraining's binary_logloss: 0.379516\ttraining's auc: 0.905122\tvalid_1's binary_logloss: 0.397637\tvalid_1's auc: 0.901068\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[200]\ttraining's binary_logloss: 0.379516\ttraining's auc: 0.905122\tvalid_1's binary_logloss: 0.397637\tvalid_1's auc: 0.901068\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " best_threshold 0.6699999999999997 best_score 0.7830860872253358\n",
      "0.9010682045110339\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9077    0.8663    0.8865    251891\n",
      "           1     0.6377    0.7275    0.6796     81443\n",
      "\n",
      "    accuracy                         0.8324    333334\n",
      "   macro avg     0.7727    0.7969    0.7831    333334\n",
      "weighted avg     0.8417    0.8324    0.8360    333334\n",
      "\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-logloss:0.67633\tvalidation_0-auc:0.89321\tvalidation_1-logloss:0.67647\tvalidation_1-auc:0.88917\n",
      "[50]\tvalidation_0-logloss:0.41618\tvalidation_0-auc:0.90532\tvalidation_1-logloss:0.42196\tvalidation_1-auc:0.89960\n",
      "[100]\tvalidation_0-logloss:0.38710\tvalidation_0-auc:0.90852\tvalidation_1-logloss:0.39774\tvalidation_1-auc:0.90058\n",
      "[150]\tvalidation_0-logloss:0.37956\tvalidation_0-auc:0.91096\tvalidation_1-logloss:0.39430\tvalidation_1-auc:0.90090\n",
      "[199]\tvalidation_0-logloss:0.37535\tvalidation_0-auc:0.91290\tvalidation_1-logloss:0.39332\tvalidation_1-auc:0.90095\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " best_threshold 0.6599999999999997 best_score 0.7829179481121453\n",
      "0.900976707285731\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9083    0.8649    0.8861    251891\n",
      "           1     0.6359    0.7301    0.6798     81443\n",
      "\n",
      "    accuracy                         0.8319    333334\n",
      "   macro avg     0.7721    0.7975    0.7829    333334\n",
      "weighted avg     0.8418    0.8319    0.8357    333334\n",
      "\n",
      "MLP------------------\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " best_threshold 0.3999999999999998 best_score 0.780454224128587\n",
      "0.8982146802108479\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9027    0.8720    0.8871    251891\n",
      "           1     0.6418    0.7093    0.6738     81443\n",
      "\n",
      "    accuracy                         0.8322    333334\n",
      "   macro avg     0.7722    0.7906    0.7805    333334\n",
      "weighted avg     0.8389    0.8322    0.8350    333334\n",
      "\n",
      "===========fold 1================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "os_name_macos                    0\n",
      "os_name_windows                  0\n",
      "age_group_15-17                  0\n",
      "age_group_18-24                  0\n",
      "age_group_25-34                  0\n",
      "age_group_35-44                  0\n",
      "age_group_45-54                  0\n",
      "age_group_55+                    0\n",
      "age_group_under 14               0\n",
      "age_group_unknown                0\n",
      "gender_female                    0\n",
      "gender_male                      0\n",
      "gender_unknown                   0\n",
      "country_IN                       0\n",
      "country_UNKNOWN                  0\n",
      "country_VN                       0\n",
      "region_Central Vietnam           0\n",
      "region_Northern Vietnam          0\n",
      "region_Southern Vietnam          0\n",
      "region_None                      0\n",
      "province_type_rural              0\n",
      "province_type_unknown            0\n",
      "province_type_urban              0\n",
      "clicks                           0\n",
      "search_volume                    0\n",
      "dating_search                    0\n",
      "videoclip_search                 0\n",
      "technical_search                 0\n",
      "housekeeping_family_search       0\n",
      "marketing_search                 0\n",
      "other_search                     0\n",
      "serp_click                       0\n",
      "search_volume_gg                 0\n",
      "search_clicks_gg                 0\n",
      "other_search_gg                  0\n",
      "housekeeping_family_search_gg    0\n",
      "videoclip_search_gg              0\n",
      "dating_search_gg                 0\n",
      "marketing_search_gg              0\n",
      "technical_search_gg              0\n",
      "active_day                       0\n",
      "life_time                        0\n",
      "not_active_day                   0\n",
      "total_active_time                0\n",
      "ads_impression                   0\n",
      "ads_click                        0\n",
      "ads_revenue                      0\n",
      "newtab_count                     0\n",
      "download_count                   0\n",
      "pip_count                        0\n",
      "sidebar_count                    0\n",
      "incognito_count                  0\n",
      "signin_count                     0\n",
      "youtube_count                    0\n",
      "work_count                       0\n",
      "social_count                     0\n",
      "news_count                       0\n",
      "entertainment_count              0\n",
      "ecommerce_count                  0\n",
      "dtype: int64\n",
      "CLASS_WEIGHTTTT\n",
      "LOGREG--------------\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " best_threshold 0.6899999999999997 best_score 0.7672659658953729\n",
      "0.878980945798922\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8891    0.8798    0.8844    251891\n",
      "           1     0.6399    0.6606    0.6501     81442\n",
      "\n",
      "    accuracy                         0.8263    333333\n",
      "   macro avg     0.7645    0.7702    0.7673    333333\n",
      "weighted avg     0.8282    0.8263    0.8272    333333\n",
      "\n",
      "Random Forest--------------\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " best_threshold 0.6499999999999997 best_score 0.7819089007998975\n",
      "0.9000606734400587\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9098    0.8606    0.8845    251891\n",
      "           1     0.6307    0.7360    0.6793     81442\n",
      "\n",
      "    accuracy                         0.8302    333333\n",
      "   macro avg     0.7702    0.7983    0.7819    333333\n",
      "weighted avg     0.8416    0.8302    0.8344    333333\n",
      "\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162886, number of negative: 503781\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.048948 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7377\n",
      "[LightGBM] [Info] Number of data points in the train set: 666667, number of used features: 59\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[50]\ttraining's binary_logloss: 0.389574\ttraining's auc: 0.90102\tvalid_1's binary_logloss: 0.400216\tvalid_1's auc: 0.900544\n",
      "[100]\ttraining's binary_logloss: 0.384934\ttraining's auc: 0.902809\tvalid_1's binary_logloss: 0.397631\tvalid_1's auc: 0.901338\n",
      "[150]\ttraining's binary_logloss: 0.382168\ttraining's auc: 0.903966\tvalid_1's binary_logloss: 0.396797\tvalid_1's auc: 0.901545\n",
      "[200]\ttraining's binary_logloss: 0.379891\ttraining's auc: 0.904964\tvalid_1's binary_logloss: 0.396121\tvalid_1's auc: 0.90169\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[200]\ttraining's binary_logloss: 0.379891\ttraining's auc: 0.904964\tvalid_1's binary_logloss: 0.396121\tvalid_1's auc: 0.90169\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " best_threshold 0.6799999999999997 best_score 0.7836784082729478\n",
      "0.9016900448546401\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9044    0.8736    0.8887    251891\n",
      "           1     0.6463    0.7144    0.6786     81442\n",
      "\n",
      "    accuracy                         0.8347    333333\n",
      "   macro avg     0.7753    0.7940    0.7837    333333\n",
      "weighted avg     0.8413    0.8347    0.8374    333333\n",
      "\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-logloss:0.67633\tvalidation_0-auc:0.89338\tvalidation_1-logloss:0.67643\tvalidation_1-auc:0.89016\n",
      "[50]\tvalidation_0-logloss:0.41635\tvalidation_0-auc:0.90517\tvalidation_1-logloss:0.42089\tvalidation_1-auc:0.89988\n",
      "[100]\tvalidation_0-logloss:0.38742\tvalidation_0-auc:0.90836\tvalidation_1-logloss:0.39657\tvalidation_1-auc:0.90086\n",
      "[150]\tvalidation_0-logloss:0.38025\tvalidation_0-auc:0.91067\tvalidation_1-logloss:0.39329\tvalidation_1-auc:0.90116\n",
      "[199]\tvalidation_0-logloss:0.37595\tvalidation_0-auc:0.91254\tvalidation_1-logloss:0.39203\tvalidation_1-auc:0.90123\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " best_threshold 0.6699999999999997 best_score 0.7827314191388421\n",
      "0.9012297328626465\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9044    0.8722    0.8880    251891\n",
      "           1     0.6439    0.7147    0.6775     81442\n",
      "\n",
      "    accuracy                         0.8337    333333\n",
      "   macro avg     0.7741    0.7935    0.7827    333333\n",
      "weighted avg     0.8407    0.8337    0.8366    333333\n",
      "\n",
      "MLP------------------\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " best_threshold 0.3899999999999999 best_score 0.7805763771150327\n",
      "0.8989276133230555\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9063    0.8651    0.8852    251891\n",
      "           1     0.6342    0.7234    0.6759     81442\n",
      "\n",
      "    accuracy                         0.8305    333333\n",
      "   macro avg     0.7703    0.7943    0.7806    333333\n",
      "weighted avg     0.8398    0.8305    0.8341    333333\n",
      "\n",
      "===========fold 2================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "os_name_macos                    0\n",
      "os_name_windows                  0\n",
      "age_group_15-17                  0\n",
      "age_group_18-24                  0\n",
      "age_group_25-34                  0\n",
      "age_group_35-44                  0\n",
      "age_group_45-54                  0\n",
      "age_group_55+                    0\n",
      "age_group_under 14               0\n",
      "age_group_unknown                0\n",
      "gender_female                    0\n",
      "gender_male                      0\n",
      "gender_unknown                   0\n",
      "country_IN                       0\n",
      "country_UNKNOWN                  0\n",
      "country_VN                       0\n",
      "region_Central Vietnam           0\n",
      "region_Northern Vietnam          0\n",
      "region_Southern Vietnam          0\n",
      "region_None                      0\n",
      "province_type_rural              0\n",
      "province_type_unknown            0\n",
      "province_type_urban              0\n",
      "clicks                           0\n",
      "search_volume                    0\n",
      "dating_search                    0\n",
      "videoclip_search                 0\n",
      "technical_search                 0\n",
      "housekeeping_family_search       0\n",
      "marketing_search                 0\n",
      "other_search                     0\n",
      "serp_click                       0\n",
      "search_volume_gg                 0\n",
      "search_clicks_gg                 0\n",
      "other_search_gg                  0\n",
      "housekeeping_family_search_gg    0\n",
      "videoclip_search_gg              0\n",
      "dating_search_gg                 0\n",
      "marketing_search_gg              0\n",
      "technical_search_gg              0\n",
      "active_day                       0\n",
      "life_time                        0\n",
      "not_active_day                   0\n",
      "total_active_time                0\n",
      "ads_impression                   0\n",
      "ads_click                        0\n",
      "ads_revenue                      0\n",
      "newtab_count                     0\n",
      "download_count                   0\n",
      "pip_count                        0\n",
      "sidebar_count                    0\n",
      "incognito_count                  0\n",
      "signin_count                     0\n",
      "youtube_count                    0\n",
      "work_count                       0\n",
      "social_count                     0\n",
      "news_count                       0\n",
      "entertainment_count              0\n",
      "ecommerce_count                  0\n",
      "dtype: int64\n",
      "CLASS_WEIGHTTTT\n",
      "LOGREG--------------\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " best_threshold 0.6899999999999997 best_score 0.7664141806446689\n",
      "0.8791556874684385\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8893    0.8781    0.8836    251890\n",
      "           1     0.6370    0.6618    0.6492     81443\n",
      "\n",
      "    accuracy                         0.8252    333333\n",
      "   macro avg     0.7631    0.7700    0.7664    333333\n",
      "weighted avg     0.8276    0.8252    0.8264    333333\n",
      "\n",
      "Random Forest--------------\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " best_threshold 0.6699999999999997 best_score 0.7819112342842316\n",
      "0.899851723843375\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9044    0.8710    0.8873    251890\n",
      "           1     0.6418    0.7151    0.6765     81443\n",
      "\n",
      "    accuracy                         0.8329    333333\n",
      "   macro avg     0.7731    0.7930    0.7819    333333\n",
      "weighted avg     0.8402    0.8329    0.8358    333333\n",
      "\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162885, number of negative: 503782\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.049164 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7367\n",
      "[LightGBM] [Info] Number of data points in the train set: 666667, number of used features: 59\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[50]\ttraining's binary_logloss: 0.389434\ttraining's auc: 0.901077\tvalid_1's binary_logloss: 0.401057\tvalid_1's auc: 0.900254\n",
      "[100]\ttraining's binary_logloss: 0.384853\ttraining's auc: 0.902832\tvalid_1's binary_logloss: 0.398394\tvalid_1's auc: 0.901126\n",
      "[150]\ttraining's binary_logloss: 0.382132\ttraining's auc: 0.903969\tvalid_1's binary_logloss: 0.39761\tvalid_1's auc: 0.901325\n",
      "[200]\ttraining's binary_logloss: 0.379856\ttraining's auc: 0.904986\tvalid_1's binary_logloss: 0.396963\tvalid_1's auc: 0.901461\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[200]\ttraining's binary_logloss: 0.379856\ttraining's auc: 0.904986\tvalid_1's binary_logloss: 0.396963\tvalid_1's auc: 0.901461\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " best_threshold 0.6599999999999997 best_score 0.7829107050173636\n",
      "0.9014609805021807\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9108    0.8602    0.8848    251890\n",
      "           1     0.6311    0.7396    0.6810     81443\n",
      "\n",
      "    accuracy                         0.8307    333333\n",
      "   macro avg     0.7710    0.7999    0.7829    333333\n",
      "weighted avg     0.8425    0.8307    0.8350    333333\n",
      "\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-logloss:0.67635\tvalidation_0-auc:0.89314\tvalidation_1-logloss:0.67647\tvalidation_1-auc:0.88979\n",
      "[50]\tvalidation_0-logloss:0.41656\tvalidation_0-auc:0.90517\tvalidation_1-logloss:0.42169\tvalidation_1-auc:0.89984\n",
      "[100]\tvalidation_0-logloss:0.38719\tvalidation_0-auc:0.90837\tvalidation_1-logloss:0.39701\tvalidation_1-auc:0.90087\n",
      "[150]\tvalidation_0-logloss:0.38011\tvalidation_0-auc:0.91070\tvalidation_1-logloss:0.39378\tvalidation_1-auc:0.90122\n",
      "[199]\tvalidation_0-logloss:0.37616\tvalidation_0-auc:0.91253\tvalidation_1-logloss:0.39281\tvalidation_1-auc:0.90130\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " best_threshold 0.6699999999999997 best_score 0.7830997452112047\n",
      "0.9013063253517125\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9052    0.8712    0.8878    251890\n",
      "           1     0.6430    0.7178    0.6784     81443\n",
      "\n",
      "    accuracy                         0.8337    333333\n",
      "   macro avg     0.7741    0.7945    0.7831    333333\n",
      "weighted avg     0.8411    0.8337    0.8367    333333\n",
      "\n",
      "MLP------------------\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " best_threshold 0.3899999999999999 best_score 0.7807754883981486\n",
      "0.8981279435696431\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9057    0.8666    0.8857    251890\n",
      "           1     0.6360    0.7209    0.6758     81443\n",
      "\n",
      "    accuracy                         0.8310    333333\n",
      "   macro avg     0.7709    0.7938    0.7808    333333\n",
      "weighted avg     0.8398    0.8310    0.8344    333333\n",
      "\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " 0.6999999999999996 0.7664159968521791\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " 0.6599999999999997 0.7818230327454454\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " 0.6799999999999997 0.7830985968176438\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " 0.6699999999999997 0.7828623265199736\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " 0.3899999999999999 0.7805943667213415\n",
      "CLASS_WEIGHTTTT\n",
      "LOGREG--------------\n",
      "accuracy 0.828375\n",
      "f1_score 0.7659437494244055\n",
      "auc 0.8789506318425202\n",
      "sensitivity 0.6375137963455013 specificity 0.8901862663330553\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8835    0.8902    0.8868    151074\n",
      "           1     0.6528    0.6375    0.6451     48926\n",
      "\n",
      "    accuracy                         0.8284    200000\n",
      "   macro avg     0.7681    0.7639    0.7659    200000\n",
      "weighted avg     0.8271    0.8284    0.8277    200000\n",
      "\n",
      "Random Forest--------------\n",
      "accuracy 0.830845\n",
      "f1_score 0.7815874534218212\n",
      "auc 0.8997264734861524\n",
      "sensitivity 0.7275272861055472 specificity 0.8643049101764698\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9074    0.8643    0.8853    151074\n",
      "           1     0.6345    0.7275    0.6779     48926\n",
      "\n",
      "    accuracy                         0.8308    200000\n",
      "   macro avg     0.7710    0.7959    0.7816    200000\n",
      "weighted avg     0.8406    0.8308    0.8346    200000\n",
      "\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 244328, number of negative: 755672\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.078691 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7373\n",
      "[LightGBM] [Info] Number of data points in the train set: 1000000, number of used features: 59\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "accuracy 0.833755\n",
      "f1_score 0.7827146317486356\n",
      "auc 0.9012849072058038\n",
      "sensitivity 0.7135061112700813 specificity 0.8726981479275058\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9039    0.8727    0.8880    151074\n",
      "           1     0.6448    0.7135    0.6774     48926\n",
      "\n",
      "    accuracy                         0.8338    200000\n",
      "   macro avg     0.7743    0.7931    0.7827    200000\n",
      "weighted avg     0.8405    0.8338    0.8365    200000\n",
      "\n",
      "XGBoost--------------\n",
      "{'objective': 'binary:logistic', 'eval_metric': ['logloss', 'auc'], 'n_estimators': 200, 'learning_rate': 0.034630277480196384, 'max_depth': 9, 'colsample_bytree': 0.8, 'subsample': 0.30000000000000004, 'reg_alpha': 0.0020136244579038245, 'reg_lambda': 1.3270228907353322e-06, 'seed': 42, 'enable_categorical': True, 'scale_pos_weight': 3.092858780000655}\n",
      "accuracy 0.833635\n",
      "f1_score 0.7835738889788901\n",
      "auc 0.901421983283769\n",
      "sensitivity 0.7208641622041451 specificity 0.8701563472205674\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9059    0.8702    0.8877    151074\n",
      "           1     0.6426    0.7209    0.6795     48926\n",
      "\n",
      "    accuracy                         0.8336    200000\n",
      "   macro avg     0.7742    0.7955    0.7836    200000\n",
      "weighted avg     0.8415    0.8336    0.8367    200000\n",
      "\n",
      "MLP--------------\n",
      "accuracy 0.83132\n",
      "f1_score 0.7803325575363741\n",
      "auc 0.8985109978724376\n",
      "sensitivity 0.7144258676368394 specificity 0.8691766948647682\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9038    0.8692    0.8862    151074\n",
      "           1     0.6388    0.7144    0.6745     48926\n",
      "\n",
      "    accuracy                         0.8313    200000\n",
      "   macro avg     0.7713    0.7918    0.7803    200000\n",
      "weighted avg     0.8390    0.8313    0.8344    200000\n",
      "\n",
      "['log_reg', 'randomforest', 'lightgbm', 'xgboost', 'mlp']\n",
      "[0.828375, 0.830845, 0.833755, 0.833635, 0.83132]\n",
      "[0.7659437494244055, 0.7815874534218212, 0.7827146317486356, 0.7835738889788901, 0.7803325575363741]\n",
      "[0.8789506318425202, 0.8997264734861524, 0.9012849072058038, 0.901421983283769, 0.8985109978724376]\n",
      "[0.8901862663330553, 0.8643049101764698, 0.8726981479275058, 0.8701563472205674, 0.8691766948647682]\n",
      "[0.6375137963455013, 0.7275272861055472, 0.7135061112700813, 0.7208641622041451, 0.7144258676368394]\n"
     ]
    }
   ],
   "source": [
    "print(\"Use Class weight\")\n",
    "class_weight_oofs = cross_validate(train, USE_CLASS_WEIGHT=True)\n",
    "class_weight_score_df, class_weight_models, class_weight_predictions = train_model(class_weight_oofs,X_train, y_train, X_test, y_test, USE_CLASS_WEIGHT=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/natalie/Desktop/DS Thesis/user-churn-prediction/checkpoints/class_weight_model_after_tune.pkl', 'wb') as f:\n",
    "    pickle.dump({\n",
    "            \"score_df\":class_weight_score_df,\n",
    "            \"oofs\":   class_weight_oofs,\n",
    "            \"models\": class_weight_models,\n",
    "            \"model_names\": MODEL_NAMES,\n",
    "            \"predictions\":class_weight_predictions},f,protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use Under sampling\n",
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "os_name_macos                    0\n",
      "os_name_windows                  0\n",
      "age_group_15-17                  0\n",
      "age_group_18-24                  0\n",
      "age_group_25-34                  0\n",
      "age_group_35-44                  0\n",
      "age_group_45-54                  0\n",
      "age_group_55+                    0\n",
      "age_group_under 14               0\n",
      "age_group_unknown                0\n",
      "gender_female                    0\n",
      "gender_male                      0\n",
      "gender_unknown                   0\n",
      "country_IN                       0\n",
      "country_UNKNOWN                  0\n",
      "country_VN                       0\n",
      "region_Central Vietnam           0\n",
      "region_Northern Vietnam          0\n",
      "region_Southern Vietnam          0\n",
      "region_None                      0\n",
      "province_type_rural              0\n",
      "province_type_unknown            0\n",
      "province_type_urban              0\n",
      "clicks                           0\n",
      "search_volume                    0\n",
      "dating_search                    0\n",
      "videoclip_search                 0\n",
      "technical_search                 0\n",
      "housekeeping_family_search       0\n",
      "marketing_search                 0\n",
      "other_search                     0\n",
      "serp_click                       0\n",
      "search_volume_gg                 0\n",
      "search_clicks_gg                 0\n",
      "other_search_gg                  0\n",
      "housekeeping_family_search_gg    0\n",
      "videoclip_search_gg              0\n",
      "dating_search_gg                 0\n",
      "marketing_search_gg              0\n",
      "technical_search_gg              0\n",
      "active_day                       0\n",
      "life_time                        0\n",
      "not_active_day                   0\n",
      "total_active_time                0\n",
      "ads_impression                   0\n",
      "ads_click                        0\n",
      "ads_revenue                      0\n",
      "newtab_count                     0\n",
      "download_count                   0\n",
      "pip_count                        0\n",
      "sidebar_count                    0\n",
      "incognito_count                  0\n",
      "signin_count                     0\n",
      "youtube_count                    0\n",
      "work_count                       0\n",
      "social_count                     0\n",
      "news_count                       0\n",
      "entertainment_count              0\n",
      "ecommerce_count                  0\n",
      "dtype: int64\n",
      "UNDER SAMPLING\n",
      "LOGREG--------------\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " best_threshold 0.6799999999999997 best_score 0.7661315205410163\n",
      "0.8786012393398603\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8903    0.8753    0.8827    251891\n",
      "           1     0.6334    0.6665    0.6495     81443\n",
      "\n",
      "    accuracy                         0.8243    333334\n",
      "   macro avg     0.7619    0.7709    0.7661    333334\n",
      "weighted avg     0.8275    0.8243    0.8258    333334\n",
      "\n",
      "Random Forest--------------\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " best_threshold 0.6699999999999997 best_score 0.7818815756197031\n",
      "0.8996418911475987\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9055    0.8686    0.8867    251891\n",
      "           1     0.6391    0.7198    0.6771     81443\n",
      "\n",
      "    accuracy                         0.8322    333334\n",
      "   macro avg     0.7723    0.7942    0.7819    333334\n",
      "weighted avg     0.8405    0.8322    0.8355    333334\n",
      "\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162885, number of negative: 162885\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.036707 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7204\n",
      "[LightGBM] [Info] Number of data points in the train set: 325770, number of used features: 59\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[50]\ttraining's binary_logloss: 0.388748\ttraining's auc: 0.901621\tvalid_1's binary_logloss: 0.401856\tvalid_1's auc: 0.899978\n",
      "[100]\ttraining's binary_logloss: 0.383646\ttraining's auc: 0.903799\tvalid_1's binary_logloss: 0.399635\tvalid_1's auc: 0.900723\n",
      "[150]\ttraining's binary_logloss: 0.380393\ttraining's auc: 0.905367\tvalid_1's binary_logloss: 0.399265\tvalid_1's auc: 0.900859\n",
      "[200]\ttraining's binary_logloss: 0.377608\ttraining's auc: 0.906753\tvalid_1's binary_logloss: 0.39909\tvalid_1's auc: 0.900921\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[193]\ttraining's binary_logloss: 0.377994\ttraining's auc: 0.906556\tvalid_1's binary_logloss: 0.39908\tvalid_1's auc: 0.900922\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " best_threshold 0.6699999999999997 best_score 0.7827099750784596\n",
      "0.9009217873368414\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9077    0.8658    0.8862    251891\n",
      "           1     0.6367    0.7277    0.6792     81443\n",
      "\n",
      "    accuracy                         0.8320    333334\n",
      "   macro avg     0.7722    0.7967    0.7827    333334\n",
      "weighted avg     0.8415    0.8320    0.8356    333334\n",
      "\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-logloss:0.67631\tvalidation_0-auc:0.89293\tvalidation_1-logloss:0.67657\tvalidation_1-auc:0.88751\n",
      "[50]\tvalidation_0-logloss:0.40786\tvalidation_0-auc:0.90759\tvalidation_1-logloss:0.42517\tvalidation_1-auc:0.89963\n",
      "[100]\tvalidation_0-logloss:0.37443\tvalidation_0-auc:0.91130\tvalidation_1-logloss:0.40220\tvalidation_1-auc:0.90050\n",
      "[150]\tvalidation_0-logloss:0.36470\tvalidation_0-auc:0.91439\tvalidation_1-logloss:0.39973\tvalidation_1-auc:0.90067\n",
      "[199]\tvalidation_0-logloss:0.35950\tvalidation_0-auc:0.91663\tvalidation_1-logloss:0.39931\tvalidation_1-auc:0.90069\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " best_threshold 0.6599999999999997 best_score 0.7822930267797544\n",
      "0.9007017605715673\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9103    0.8602    0.8846    251891\n",
      "           1     0.6306    0.7379    0.6800     81443\n",
      "\n",
      "    accuracy                         0.8303    333334\n",
      "   macro avg     0.7704    0.7991    0.7823    333334\n",
      "weighted avg     0.8420    0.8303    0.8346    333334\n",
      "\n",
      "MLP------------------\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " best_threshold 0.6599999999999997 best_score 0.7797397913998503\n",
      "0.8974406713510733\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9056    0.8651    0.8849    251891\n",
      "           1     0.6336    0.7212    0.6746     81443\n",
      "\n",
      "    accuracy                         0.8300    333334\n",
      "   macro avg     0.7696    0.7932    0.7797    333334\n",
      "weighted avg     0.8392    0.8300    0.8335    333334\n",
      "\n",
      "===========fold 1================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "os_name_macos                    0\n",
      "os_name_windows                  0\n",
      "age_group_15-17                  0\n",
      "age_group_18-24                  0\n",
      "age_group_25-34                  0\n",
      "age_group_35-44                  0\n",
      "age_group_45-54                  0\n",
      "age_group_55+                    0\n",
      "age_group_under 14               0\n",
      "age_group_unknown                0\n",
      "gender_female                    0\n",
      "gender_male                      0\n",
      "gender_unknown                   0\n",
      "country_IN                       0\n",
      "country_UNKNOWN                  0\n",
      "country_VN                       0\n",
      "region_Central Vietnam           0\n",
      "region_Northern Vietnam          0\n",
      "region_Southern Vietnam          0\n",
      "region_None                      0\n",
      "province_type_rural              0\n",
      "province_type_unknown            0\n",
      "province_type_urban              0\n",
      "clicks                           0\n",
      "search_volume                    0\n",
      "dating_search                    0\n",
      "videoclip_search                 0\n",
      "technical_search                 0\n",
      "housekeeping_family_search       0\n",
      "marketing_search                 0\n",
      "other_search                     0\n",
      "serp_click                       0\n",
      "search_volume_gg                 0\n",
      "search_clicks_gg                 0\n",
      "other_search_gg                  0\n",
      "housekeeping_family_search_gg    0\n",
      "videoclip_search_gg              0\n",
      "dating_search_gg                 0\n",
      "marketing_search_gg              0\n",
      "technical_search_gg              0\n",
      "active_day                       0\n",
      "life_time                        0\n",
      "not_active_day                   0\n",
      "total_active_time                0\n",
      "ads_impression                   0\n",
      "ads_click                        0\n",
      "ads_revenue                      0\n",
      "newtab_count                     0\n",
      "download_count                   0\n",
      "pip_count                        0\n",
      "sidebar_count                    0\n",
      "incognito_count                  0\n",
      "signin_count                     0\n",
      "youtube_count                    0\n",
      "work_count                       0\n",
      "social_count                     0\n",
      "news_count                       0\n",
      "entertainment_count              0\n",
      "ecommerce_count                  0\n",
      "dtype: int64\n",
      "UNDER SAMPLING\n",
      "LOGREG--------------\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " best_threshold 0.6899999999999997 best_score 0.7670993551608513\n",
      "0.8791131415676788\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8903    0.8768    0.8835    251891\n",
      "           1     0.6361    0.6660    0.6507     81442\n",
      "\n",
      "    accuracy                         0.8253    333333\n",
      "   macro avg     0.7632    0.7714    0.7671    333333\n",
      "weighted avg     0.8282    0.8253    0.8266    333333\n",
      "\n",
      "Random Forest--------------\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " best_threshold 0.6699999999999997 best_score 0.7817944768974155\n",
      "0.8999974411132348\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9060    0.8676    0.8864    251891\n",
      "           1     0.6380    0.7216    0.6772     81442\n",
      "\n",
      "    accuracy                         0.8319    333333\n",
      "   macro avg     0.7720    0.7946    0.7818    333333\n",
      "weighted avg     0.8405    0.8319    0.8353    333333\n",
      "\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162886, number of negative: 162886\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031483 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7233\n",
      "[LightGBM] [Info] Number of data points in the train set: 325772, number of used features: 59\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[50]\ttraining's binary_logloss: 0.388665\ttraining's auc: 0.90163\tvalid_1's binary_logloss: 0.400611\tvalid_1's auc: 0.900496\n",
      "[100]\ttraining's binary_logloss: 0.383433\ttraining's auc: 0.903901\tvalid_1's binary_logloss: 0.398414\tvalid_1's auc: 0.901253\n",
      "[150]\ttraining's binary_logloss: 0.380221\ttraining's auc: 0.905485\tvalid_1's binary_logloss: 0.398138\tvalid_1's auc: 0.901382\n",
      "[200]\ttraining's binary_logloss: 0.377314\ttraining's auc: 0.90689\tvalid_1's binary_logloss: 0.397964\tvalid_1's auc: 0.901421\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[194]\ttraining's binary_logloss: 0.377647\ttraining's auc: 0.906716\tvalid_1's binary_logloss: 0.397949\tvalid_1's auc: 0.901423\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " best_threshold 0.6799999999999997 best_score 0.7833283555051755\n",
      "0.9014231042916758\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9050    0.8719    0.8881    251891\n",
      "           1     0.6440    0.7169    0.6785     81442\n",
      "\n",
      "    accuracy                         0.8340    333333\n",
      "   macro avg     0.7745    0.7944    0.7833    333333\n",
      "weighted avg     0.8412    0.8340    0.8369    333333\n",
      "\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-logloss:0.67637\tvalidation_0-auc:0.89338\tvalidation_1-logloss:0.67655\tvalidation_1-auc:0.88820\n",
      "[50]\tvalidation_0-logloss:0.40785\tvalidation_0-auc:0.90757\tvalidation_1-logloss:0.42417\tvalidation_1-auc:0.89988\n",
      "[100]\tvalidation_0-logloss:0.37443\tvalidation_0-auc:0.91138\tvalidation_1-logloss:0.40091\tvalidation_1-auc:0.90089\n",
      "[150]\tvalidation_0-logloss:0.36482\tvalidation_0-auc:0.91431\tvalidation_1-logloss:0.39818\tvalidation_1-auc:0.90124\n",
      "[199]\tvalidation_0-logloss:0.35954\tvalidation_0-auc:0.91659\tvalidation_1-logloss:0.39778\tvalidation_1-auc:0.90125\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " best_threshold 0.6699999999999997 best_score 0.782933031970544\n",
      "0.901271008044514\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9067    0.8681    0.8870    251891\n",
      "           1     0.6394    0.7236    0.6789     81442\n",
      "\n",
      "    accuracy                         0.8328    333333\n",
      "   macro avg     0.7731    0.7958    0.7829    333333\n",
      "weighted avg     0.8414    0.8328    0.8361    333333\n",
      "\n",
      "MLP------------------\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " best_threshold 0.6799999999999997 best_score 0.7795349638537215\n",
      "0.8975440875456011\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9027    0.8706    0.8863    251891\n",
      "           1     0.6394    0.7098    0.6727     81442\n",
      "\n",
      "    accuracy                         0.8313    333333\n",
      "   macro avg     0.7710    0.7902    0.7795    333333\n",
      "weighted avg     0.8384    0.8313    0.8341    333333\n",
      "\n",
      "===========fold 2================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "os_name_macos                    0\n",
      "os_name_windows                  0\n",
      "age_group_15-17                  0\n",
      "age_group_18-24                  0\n",
      "age_group_25-34                  0\n",
      "age_group_35-44                  0\n",
      "age_group_45-54                  0\n",
      "age_group_55+                    0\n",
      "age_group_under 14               0\n",
      "age_group_unknown                0\n",
      "gender_female                    0\n",
      "gender_male                      0\n",
      "gender_unknown                   0\n",
      "country_IN                       0\n",
      "country_UNKNOWN                  0\n",
      "country_VN                       0\n",
      "region_Central Vietnam           0\n",
      "region_Northern Vietnam          0\n",
      "region_Southern Vietnam          0\n",
      "region_None                      0\n",
      "province_type_rural              0\n",
      "province_type_unknown            0\n",
      "province_type_urban              0\n",
      "clicks                           0\n",
      "search_volume                    0\n",
      "dating_search                    0\n",
      "videoclip_search                 0\n",
      "technical_search                 0\n",
      "housekeeping_family_search       0\n",
      "marketing_search                 0\n",
      "other_search                     0\n",
      "serp_click                       0\n",
      "search_volume_gg                 0\n",
      "search_clicks_gg                 0\n",
      "other_search_gg                  0\n",
      "housekeeping_family_search_gg    0\n",
      "videoclip_search_gg              0\n",
      "dating_search_gg                 0\n",
      "marketing_search_gg              0\n",
      "technical_search_gg              0\n",
      "active_day                       0\n",
      "life_time                        0\n",
      "not_active_day                   0\n",
      "total_active_time                0\n",
      "ads_impression                   0\n",
      "ads_click                        0\n",
      "ads_revenue                      0\n",
      "newtab_count                     0\n",
      "download_count                   0\n",
      "pip_count                        0\n",
      "sidebar_count                    0\n",
      "incognito_count                  0\n",
      "signin_count                     0\n",
      "youtube_count                    0\n",
      "work_count                       0\n",
      "social_count                     0\n",
      "news_count                       0\n",
      "entertainment_count              0\n",
      "ecommerce_count                  0\n",
      "dtype: int64\n",
      "UNDER SAMPLING\n",
      "LOGREG--------------\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " best_threshold 0.6999999999999996 best_score 0.7660351365649716\n",
      "0.8794654037713752\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8862    0.8844    0.8853    251890\n",
      "           1     0.6447    0.6489    0.6468     81443\n",
      "\n",
      "    accuracy                         0.8268    333333\n",
      "   macro avg     0.7655    0.7666    0.7660    333333\n",
      "weighted avg     0.8272    0.8268    0.8270    333333\n",
      "\n",
      "Random Forest--------------\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " best_threshold 0.6699999999999997 best_score 0.7818181303616055\n",
      "0.8998219502090173\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9064    0.8669    0.8862    251890\n",
      "           1     0.6372    0.7232    0.6774     81443\n",
      "\n",
      "    accuracy                         0.8317    333333\n",
      "   macro avg     0.7718    0.7950    0.7818    333333\n",
      "weighted avg     0.8406    0.8317    0.8352    333333\n",
      "\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162885, number of negative: 162885\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033825 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7250\n",
      "[LightGBM] [Info] Number of data points in the train set: 325770, number of used features: 59\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[50]\ttraining's binary_logloss: 0.389265\ttraining's auc: 0.901154\tvalid_1's binary_logloss: 0.401452\tvalid_1's auc: 0.900169\n",
      "[100]\ttraining's binary_logloss: 0.384246\ttraining's auc: 0.903269\tvalid_1's binary_logloss: 0.399261\tvalid_1's auc: 0.90092\n",
      "[150]\ttraining's binary_logloss: 0.381163\ttraining's auc: 0.904783\tvalid_1's binary_logloss: 0.398792\tvalid_1's auc: 0.901112\n",
      "[200]\ttraining's binary_logloss: 0.378274\ttraining's auc: 0.906242\tvalid_1's binary_logloss: 0.398585\tvalid_1's auc: 0.901173\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[200]\ttraining's binary_logloss: 0.378274\ttraining's auc: 0.906242\tvalid_1's binary_logloss: 0.398585\tvalid_1's auc: 0.901173\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " best_threshold 0.6699999999999997 best_score 0.7828395140169353\n",
      "0.9011732704435569\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9080    0.8655    0.8862    251890\n",
      "           1     0.6365    0.7286    0.6795     81443\n",
      "\n",
      "    accuracy                         0.8320    333333\n",
      "   macro avg     0.7722    0.7971    0.7828    333333\n",
      "weighted avg     0.8416    0.8320    0.8357    333333\n",
      "\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-logloss:0.67636\tvalidation_0-auc:0.89259\tvalidation_1-logloss:0.67658\tvalidation_1-auc:0.88782\n",
      "[50]\tvalidation_0-logloss:0.40855\tvalidation_0-auc:0.90713\tvalidation_1-logloss:0.42462\tvalidation_1-auc:0.89990\n",
      "[100]\tvalidation_0-logloss:0.37480\tvalidation_0-auc:0.91107\tvalidation_1-logloss:0.40138\tvalidation_1-auc:0.90088\n",
      "[150]\tvalidation_0-logloss:0.36535\tvalidation_0-auc:0.91402\tvalidation_1-logloss:0.39882\tvalidation_1-auc:0.90111\n",
      "[199]\tvalidation_0-logloss:0.36013\tvalidation_0-auc:0.91626\tvalidation_1-logloss:0.39837\tvalidation_1-auc:0.90118\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " best_threshold 0.6799999999999997 best_score 0.7826241133293822\n",
      "0.9011869958605495\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9038    0.8732    0.8882    251890\n",
      "           1     0.6449    0.7125    0.6770     81443\n",
      "\n",
      "    accuracy                         0.8339    333333\n",
      "   macro avg     0.7744    0.7928    0.7826    333333\n",
      "weighted avg     0.8405    0.8339    0.8366    333333\n",
      "\n",
      "MLP------------------\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " best_threshold 0.6599999999999997 best_score 0.7792995847633912\n",
      "0.8974707679863979\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9075    0.8609    0.8836    251890\n",
      "           1     0.6288    0.7285    0.6750     81443\n",
      "\n",
      "    accuracy                         0.8286    333333\n",
      "   macro avg     0.7681    0.7947    0.7793    333333\n",
      "weighted avg     0.8394    0.8286    0.8326    333333\n",
      "\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " 0.6899999999999997 0.7663501673458367\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " 0.6699999999999997 0.7818315832476861\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " 0.6699999999999997 0.7828618855759584\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " 0.6699999999999997 0.7825592620958217\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " 0.6699999999999997 0.7793930334035171\n",
      "UNDER SAMPLING\n",
      "LOGREG--------------\n",
      "accuracy 0.824245\n",
      "f1_score 0.7665892951482309\n",
      "auc 0.8789566532619886\n",
      "sensitivity 0.6688468299063892 specificity 0.8745714020943379\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8908    0.8746    0.8826    151074\n",
      "           1     0.6333    0.6688    0.6506     48926\n",
      "\n",
      "    accuracy                         0.8242    200000\n",
      "   macro avg     0.7620    0.7717    0.7666    200000\n",
      "weighted avg     0.8278    0.8242    0.8258    200000\n",
      "\n",
      "Random Forest--------------\n",
      "accuracy 0.83228\n",
      "f1_score 0.7817852109322752\n",
      "auc 0.8996973845927536\n",
      "sensitivity 0.7179005028001472 specificity 0.8693223188636032\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9049    0.8693    0.8868    151074\n",
      "           1     0.6402    0.7179    0.6768     48926\n",
      "\n",
      "    accuracy                         0.8323    200000\n",
      "   macro avg     0.7725    0.7936    0.7818    200000\n",
      "weighted avg     0.8401    0.8323    0.8354    200000\n",
      "\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 244328, number of negative: 244328\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.047165 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7224\n",
      "[LightGBM] [Info] Number of data points in the train set: 488656, number of used features: 59\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "accuracy 0.831805\n",
      "f1_score 0.7825145600370549\n",
      "auc 0.9011127243325504\n",
      "sensitivity 0.7270980664677268 specificity 0.8657148152560996\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9074    0.8657    0.8861    151074\n",
      "           1     0.6368    0.7271    0.6790     48926\n",
      "\n",
      "    accuracy                         0.8318    200000\n",
      "   macro avg     0.7721    0.7964    0.7825    200000\n",
      "weighted avg     0.8412    0.8318    0.8354    200000\n",
      "\n",
      "XGBoost--------------\n",
      "{'objective': 'binary:logistic', 'eval_metric': ['logloss', 'auc'], 'n_estimators': 200, 'learning_rate': 0.034630277480196384, 'max_depth': 9, 'colsample_bytree': 0.8, 'subsample': 0.30000000000000004, 'reg_alpha': 0.0020136244579038245, 'reg_lambda': 1.3270228907353322e-06, 'seed': 42, 'enable_categorical': True}\n",
      "accuracy 0.83285\n",
      "f1_score 0.7833915652831547\n",
      "auc 0.9012758525505636\n",
      "sensitivity 0.7256060172505416 specificity 0.8675814501502574\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9071    0.8676    0.8869    151074\n",
      "           1     0.6396    0.7256    0.6799     48926\n",
      "\n",
      "    accuracy                         0.8328    200000\n",
      "   macro avg     0.7733    0.7966    0.7834    200000\n",
      "weighted avg     0.8417    0.8328    0.8363    200000\n",
      "\n",
      "MLP--------------\n",
      "accuracy 0.832575\n",
      "f1_score 0.7795312986112016\n",
      "auc 0.8982711128926514\n",
      "sensitivity 0.6991579119486572 specificity 0.8757827289937382\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8999    0.8758    0.8877    151074\n",
      "           1     0.6457    0.6992    0.6714     48926\n",
      "\n",
      "    accuracy                         0.8326    200000\n",
      "   macro avg     0.7728    0.7875    0.7795    200000\n",
      "weighted avg     0.8377    0.8326    0.8348    200000\n",
      "\n",
      "['log_reg', 'randomforest', 'lightgbm', 'xgboost', 'mlp']\n",
      "[0.824245, 0.83228, 0.831805, 0.83285, 0.832575]\n",
      "[0.7665892951482309, 0.7817852109322752, 0.7825145600370549, 0.7833915652831547, 0.7795312986112016]\n",
      "[0.8789566532619886, 0.8996973845927536, 0.9011127243325504, 0.9012758525505636, 0.8982711128926514]\n",
      "[0.8745714020943379, 0.8693223188636032, 0.8657148152560996, 0.8675814501502574, 0.8757827289937382]\n",
      "[0.6688468299063892, 0.7179005028001472, 0.7270980664677268, 0.7256060172505416, 0.6991579119486572]\n"
     ]
    }
   ],
   "source": [
    "print(\"Use Under sampling\")\n",
    "under_sampling_oofs = cross_validate(train, USE_UNDER_SAMPLING=True)\n",
    "under_sampling_score_df, under_sampling_models, under_sampling_predictions = train_model(under_sampling_oofs,X_train, y_train, X_test, y_test, USE_UNDER_SAMPLING=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/natalie/Desktop/DS Thesis/user-churn-prediction/checkpoints/under_sampling_model_after_tune.pkl', 'wb') as f:\n",
    "     pickle.dump({\n",
    "             \"score_df\":under_sampling_score_df,\n",
    "             \"oofs\": under_sampling_oofs,\n",
    "             \"models\": under_sampling_models,\n",
    "             \"model_names\": MODEL_NAMES,\n",
    "             \"predictions\":under_sampling_predictions},f,protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "churn_prediction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
