{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, confusion_matrix, accuracy_score, roc_auc_score ,classification_report\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_threshold_f1_score(train_labels, oofs, average='macro'):\n",
    "    scores = []\n",
    "    thresholds = []\n",
    "    best_score = 0\n",
    "    best_threshold = 0\n",
    "    for threshold in np.arange(0.1, 0.9, 0.01):\n",
    "        print(f'{threshold:.02f}, ', end='')\n",
    "        preds = (oofs > threshold).astype('int')\n",
    "        m = f1_score(train_labels, preds, average=average)\n",
    "        scores.append(m)\n",
    "        thresholds.append(threshold)\n",
    "        if m > best_score:\n",
    "            best_score = m\n",
    "            best_threshold = threshold\n",
    "    return best_threshold, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"/Users/natalie/Desktop/DS Thesis/user-churn-prediction/checkpoints/nothing_model_after_tune.pkl\", \"rb\") as f:\n",
    "    nothing_loadded = pickle.load(f)\n",
    "with open(f\"/Users/natalie/Desktop/DS Thesis/user-churn-prediction/checkpoints/smote_model_after_tune.pkl\", \"rb\") as f:\n",
    "    smote_loadded = pickle.load(f)\n",
    "with open(f\"/Users/natalie/Desktop/DS Thesis/user-churn-prediction/checkpoints/class_weight_model_after_tune.pkl\", \"rb\") as f:\n",
    "    class_weight_loadded = pickle.load(f)\n",
    "with open(f\"/Users/natalie/Desktop/DS Thesis/user-churn-prediction/checkpoints/under_sampling_model_after_tune.pkl\", \"rb\") as f:\n",
    "    under_sampling_loadded = pickle.load(f)\n",
    "test = pd.read_parquet(\"/Users/natalie/Desktop/DS Thesis/Code/data/test.parquet\")\n",
    "train = pd.read_parquet(\"/Users/natalie/Desktop/DS Thesis/Code/data/train.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop column province\n",
    "test = test.drop(columns=['province'])\n",
    "train = train.drop(columns=['province'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAMES = nothing_loadded['model_names']\n",
    "nothing_score_df = nothing_loadded['score_df']\n",
    "nothing_oofs = nothing_loadded['oofs']\n",
    "nothing_models = nothing_loadded['models']\n",
    "nothing_predictions = nothing_loadded['predictions']\n",
    "\n",
    "smote_score_df = smote_loadded['score_df']\n",
    "smote_oofs = smote_loadded['oofs']\n",
    "smote_models = smote_loadded['models']\n",
    "smote_predictions = smote_loadded['predictions']\n",
    "\n",
    "class_weight_score_df = class_weight_loadded['score_df']\n",
    "class_weight_oofs = class_weight_loadded['oofs']\n",
    "class_weight_df_models = class_weight_loadded['models']\n",
    "class_weight_predictions = class_weight_loadded['predictions']\n",
    "\n",
    "under_sampling_score_df = under_sampling_loadded['score_df']\n",
    "under_sampling_oofs = under_sampling_loadded['oofs']\n",
    "under_sampling_models = under_sampling_loadded['models']\n",
    "under_sampling_predictions = under_sampling_loadded['predictions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoring(y_test,y_pred_proba, best_threshold):\n",
    "    y_pred = [1 if y_hat >= best_threshold else 0 for y_hat in y_pred_proba]\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    _f1_score = f1_score(y_test, y_pred, average='macro')\n",
    "    auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "    specificity = tn / (tn+fp)\n",
    "    sensitivity = tp / (tp+fn)\n",
    "    print(\"accuracy\", acc)\n",
    "    print(\"f1_score\", _f1_score)\n",
    "    print(\"auc\", auc_score)\n",
    "    print(\"sensitivity\", sensitivity, \"specificity\", specificity)\n",
    "    print(classification_report(y_test, y_pred, digits=4))\n",
    "    return acc, _f1_score, auc_score, specificity, sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_ESTIMATORS=200\n",
    "SEED=42\n",
    "\n",
    "XGBoost_Hyperparameters = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': ['logloss', 'auc'],\n",
    "    'n_estimators': N_ESTIMATORS,\n",
    "    'learning_rate': 0.034630277480196384,\n",
    "    'max_depth': 9,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'subsample': 0.30000000000000004,\n",
    "    'reg_alpha': 0.0020136244579038245,\n",
    "    'reg_lambda': 1.3270228907353322e-06,\n",
    "    'seed': SEED,\n",
    "    # 'scale_pos_weight':3,\n",
    "    'enable_categorical':True,\n",
    "    'early_stopping_rounds': 50,\n",
    "    #'tree_method':'gpu_hist'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "def stacking(step_1_oofs,step_1_predictions):\n",
    "    oofs = np.zeros(step_1_oofs.shape[0])\n",
    "    for i, (train_index, valid_index) in enumerate(kfold.split(class_weight_oofs, train['churn_user'])):\n",
    "        X_train = step_1_oofs[train_index]\n",
    "        X_valid = step_1_oofs[valid_index]\n",
    "        y_train = train.iloc[train_index]['churn_user']\n",
    "        y_valid = train.iloc[valid_index]['churn_user']\n",
    "        xgb_model = XGBClassifier(**XGBoost_Hyperparameters)\n",
    "        xgb_model.fit(X_train, y_train,\n",
    "                    eval_set=[(X_train, y_train), (X_valid, y_valid)],\n",
    "                    verbose=50)\n",
    "        oofs[valid_index] = xgb_model.predict_proba(X_valid)[:,1]\n",
    "        best_threshold, best_score = find_best_threshold_f1_score(y_valid, oofs[valid_index])\n",
    "        acc, _f1_score, auc_score, specificity, sensitivity = scoring(y_valid,oofs[valid_index],best_threshold)\n",
    "    best_threshold, best_score = find_best_threshold_f1_score(train['churn_user'], oofs)\n",
    "    xgboost_hyperparameters = XGBoost_Hyperparameters.copy()\n",
    "    del xgboost_hyperparameters['early_stopping_rounds']\n",
    "    xgb_model = XGBClassifier(**xgboost_hyperparameters)\n",
    "    xgb_model.fit(step_1_oofs, train['churn_user'])\n",
    "    X_test = np.vstack(step_1_predictions).transpose()\n",
    "    y_test = test['churn_user']\n",
    "    y_pred_proba = xgb_model.predict_proba(X_test)[:,1]\n",
    "    acc, _f1_score, auc_score, specificity, sensitivity = scoring(y_test,y_pred_proba,best_threshold)\n",
    "    return acc, _f1_score, auc_score, specificity, sensitivity, y_pred_proba\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blend(stage_1_oofs, stage_1_predictions):\n",
    "    y_pred_proba = stage_1_oofs.mean(axis=1)\n",
    "    best_threshold, best_score = find_best_threshold_f1_score(train['churn_user'], y_pred_proba)\n",
    "    X_test = np.vstack(stage_1_predictions).transpose()\n",
    "    y_test = test['churn_user']\n",
    "    y_pred_proba= X_test.mean(axis=1)\n",
    "    acc, _f1_score, auc_score, specificity, sensitivity = scoring(y_test,y_pred_proba,best_threshold)\n",
    "    print(\"accuracy\", acc)\n",
    "    print(\"f1_score\", _f1_score)\n",
    "    print(\"auc\", auc_score)\n",
    "    print(\"specificity\", specificity)\n",
    "    print(\"sensitivity\", sensitivity)\n",
    "    return acc, _f1_score, auc_score, specificity, sensitivity, y_pred_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble(stage_1_oofs, stage_1_predictions):\n",
    "    accuracy_scores = []\n",
    "    f1_scores = []\n",
    "    auc_scores = []\n",
    "    specificity_scores = []\n",
    "    sensitivity_scores = []\n",
    "    predictions = []\n",
    "   \n",
    "\n",
    "    acc, _f1_score, auc_score, specificity, sensitivity, y_pred_proba = blend(stage_1_oofs, stage_1_predictions)\n",
    "    print(\"accuracy\", acc)\n",
    "    print(\"f1_score\", _f1_score)\n",
    "    print(\"auc\", auc_score)\n",
    "    print(\"specificity\", specificity)\n",
    "    print(\"sensitivity\", sensitivity)\n",
    "    accuracy_scores.append(acc)\n",
    "    f1_scores.append(_f1_score)\n",
    "    auc_scores.append(auc_score)\n",
    "    specificity_scores.append(specificity)\n",
    "    sensitivity_scores.append(sensitivity)\n",
    "    predictions.append(y_pred_proba)\n",
    "\n",
    "\n",
    "    acc, _f1_score, auc_score, specificity, sensitivity, y_pred_proba = stacking(stage_1_oofs, stage_1_predictions)\n",
    "    print(\"accuracy\", acc)\n",
    "    print(\"f1_score\", _f1_score)\n",
    "    print(\"auc\", auc_score)\n",
    "    print(\"specificity\", specificity)\n",
    "    print(\"sensitivity\", sensitivity)\n",
    "    accuracy_scores.append(acc)\n",
    "    f1_scores.append(_f1_score)\n",
    "    auc_scores.append(auc_score)\n",
    "    specificity_scores.append(specificity)\n",
    "    sensitivity_scores.append(sensitivity)\n",
    "    predictions.append(y_pred_proba)\n",
    "\n",
    "    score_df = pd.DataFrame({'model_name': ['blend', 'stack'],\n",
    "                         'accuracy_score':accuracy_scores, \n",
    "                         'f1_score': f1_scores, \n",
    "                         'auc_score': auc_scores, \n",
    "                         'specificity_score': specificity_scores, \n",
    "                         'sensitivity_score': sensitivity_scores})\n",
    "    return score_df, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, accuracy 0.82144\n",
      "f1_score 0.7728461745986004\n",
      "auc 0.88954296234857\n",
      "sensitivity 0.7335976781261497 specificity 0.849888134291804\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9078    0.8499    0.8779    151074\n",
      "           1     0.6128    0.7336    0.6678     48926\n",
      "\n",
      "    accuracy                         0.8214    200000\n",
      "   macro avg     0.7603    0.7917    0.7728    200000\n",
      "weighted avg     0.8357    0.8214    0.8265    200000\n",
      "\n",
      "accuracy 0.82144\n",
      "f1_score 0.7728461745986004\n",
      "auc 0.88954296234857\n",
      "specificity 0.849888134291804\n",
      "sensitivity 0.7335976781261497\n",
      "accuracy 0.82144\n",
      "f1_score 0.7728461745986004\n",
      "auc 0.88954296234857\n",
      "specificity 0.849888134291804\n",
      "sensitivity 0.7335976781261497\n",
      "[0]\tvalidation_0-logloss:0.54618\tvalidation_0-auc:0.88498\tvalidation_1-logloss:0.54599\tvalidation_1-auc:0.88391\n",
      "[50]\tvalidation_0-logloss:0.35582\tvalidation_0-auc:0.89975\tvalidation_1-logloss:0.35785\tvalidation_1-auc:0.89938\n",
      "[100]\tvalidation_0-logloss:0.33265\tvalidation_0-auc:0.90046\tvalidation_1-logloss:0.33465\tvalidation_1-auc:0.89973\n",
      "[150]\tvalidation_0-logloss:0.32871\tvalidation_0-auc:0.90091\tvalidation_1-logloss:0.33097\tvalidation_1-auc:0.89984\n",
      "[199]\tvalidation_0-logloss:0.32763\tvalidation_0-auc:0.90122\tvalidation_1-logloss:0.33026\tvalidation_1-auc:0.89988\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, accuracy 0.831484337031326\n",
      "f1_score 0.7822097995852276\n",
      "auc 0.8998830616175623\n",
      "sensitivity 0.7281779895141387 specificity 0.8648860022787634\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9078    0.8649    0.8858    251891\n",
      "           1     0.6354    0.7282    0.6786     81443\n",
      "\n",
      "    accuracy                         0.8315    333334\n",
      "   macro avg     0.7716    0.7965    0.7822    333334\n",
      "weighted avg     0.8412    0.8315    0.8352    333334\n",
      "\n",
      "[0]\tvalidation_0-logloss:0.54614\tvalidation_0-auc:0.88671\tvalidation_1-logloss:0.54676\tvalidation_1-auc:0.88456\n",
      "[50]\tvalidation_0-logloss:0.35558\tvalidation_0-auc:0.89996\tvalidation_1-logloss:0.35692\tvalidation_1-auc:0.89884\n",
      "[100]\tvalidation_0-logloss:0.33240\tvalidation_0-auc:0.90066\tvalidation_1-logloss:0.33468\tvalidation_1-auc:0.89917\n",
      "[150]\tvalidation_0-logloss:0.32842\tvalidation_0-auc:0.90107\tvalidation_1-logloss:0.33113\tvalidation_1-auc:0.89933\n",
      "[199]\tvalidation_0-logloss:0.32735\tvalidation_0-auc:0.90136\tvalidation_1-logloss:0.33038\tvalidation_1-auc:0.89943\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, accuracy 0.8328908328908329\n",
      "f1_score 0.7817566115797481\n",
      "auc 0.8994305735009203\n",
      "sensitivity 0.7138945507232141 specificity 0.871364995176485\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9040    0.8714    0.8874    251891\n",
      "           1     0.6421    0.7139    0.6761     81442\n",
      "\n",
      "    accuracy                         0.8329    333333\n",
      "   macro avg     0.7731    0.7926    0.7818    333333\n",
      "weighted avg     0.8400    0.8329    0.8358    333333\n",
      "\n",
      "[0]\tvalidation_0-logloss:0.54611\tvalidation_0-auc:0.88702\tvalidation_1-logloss:0.54621\tvalidation_1-auc:0.88439\n",
      "[50]\tvalidation_0-logloss:0.35531\tvalidation_0-auc:0.90029\tvalidation_1-logloss:0.35700\tvalidation_1-auc:0.89848\n",
      "[100]\tvalidation_0-logloss:0.33191\tvalidation_0-auc:0.90102\tvalidation_1-logloss:0.33455\tvalidation_1-auc:0.89882\n",
      "[150]\tvalidation_0-logloss:0.32783\tvalidation_0-auc:0.90145\tvalidation_1-logloss:0.33156\tvalidation_1-auc:0.89896\n",
      "[199]\tvalidation_0-logloss:0.32675\tvalidation_0-auc:0.90173\tvalidation_1-logloss:0.33131\tvalidation_1-auc:0.89901\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, accuracy 0.8318798318798318\n",
      "f1_score 0.7821900287585716\n",
      "auc 0.899007827604007\n",
      "sensitivity 0.7249364586275063 specificity 0.8664575806899837\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9069    0.8665    0.8862    251890\n",
      "           1     0.6370    0.7249    0.6782     81443\n",
      "\n",
      "    accuracy                         0.8319    333333\n",
      "   macro avg     0.7720    0.7957    0.7822    333333\n",
      "weighted avg     0.8410    0.8319    0.8354    333333\n",
      "\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, accuracy 0.829935\n",
      "f1_score 0.7820980146853604\n",
      "auc 0.8999744691922769\n",
      "sensitivity 0.7386461186281322 specificity 0.8594993182149145\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9104    0.8595    0.8842    151074\n",
      "           1     0.6300    0.7386    0.6800     48926\n",
      "\n",
      "    accuracy                         0.8299    200000\n",
      "   macro avg     0.7702    0.7991    0.7821    200000\n",
      "weighted avg     0.8418    0.8299    0.8342    200000\n",
      "\n",
      "accuracy 0.829935\n",
      "f1_score 0.7820980146853604\n",
      "auc 0.8999744691922769\n",
      "specificity 0.8594993182149145\n",
      "sensitivity 0.7386461186281322\n"
     ]
    }
   ],
   "source": [
    "nothing_ensemble_df, nothing_ensemble_predictions = ensemble(nothing_oofs, nothing_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, accuracy 0.817555\n",
      "f1_score 0.7693196381548502\n",
      "auc 0.8920577932331126\n",
      "sensitivity 0.7363773862567959 specificity 0.8438447383401512\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9081    0.8438    0.8748    151074\n",
      "           1     0.6043    0.7364    0.6638     48926\n",
      "\n",
      "    accuracy                         0.8176    200000\n",
      "   macro avg     0.7562    0.7901    0.7693    200000\n",
      "weighted avg     0.8338    0.8176    0.8232    200000\n",
      "\n",
      "accuracy 0.817555\n",
      "f1_score 0.7693196381548502\n",
      "auc 0.8920577932331126\n",
      "specificity 0.8438447383401512\n",
      "sensitivity 0.7363773862567959\n",
      "accuracy 0.817555\n",
      "f1_score 0.7693196381548502\n",
      "auc 0.8920577932331126\n",
      "specificity 0.8438447383401512\n",
      "sensitivity 0.7363773862567959\n",
      "[0]\tvalidation_0-logloss:0.54656\tvalidation_0-auc:0.87980\tvalidation_1-logloss:0.54695\tvalidation_1-auc:0.87783\n",
      "[50]\tvalidation_0-logloss:0.35746\tvalidation_0-auc:0.89876\tvalidation_1-logloss:0.35986\tvalidation_1-auc:0.89759\n",
      "[100]\tvalidation_0-logloss:0.33436\tvalidation_0-auc:0.89943\tvalidation_1-logloss:0.33728\tvalidation_1-auc:0.89779\n",
      "[150]\tvalidation_0-logloss:0.33033\tvalidation_0-auc:0.89988\tvalidation_1-logloss:0.33382\tvalidation_1-auc:0.89791\n",
      "[199]\tvalidation_0-logloss:0.32922\tvalidation_0-auc:0.90017\tvalidation_1-logloss:0.33321\tvalidation_1-auc:0.89796\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, accuracy 0.8309353381293237\n",
      "f1_score 0.7807973753908093\n",
      "auc 0.8979597661963494\n",
      "sensitivity 0.7217317633191311 specificity 0.8662437324080654\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9059    0.8662    0.8856    251891\n",
      "           1     0.6357    0.7217    0.6760     81443\n",
      "\n",
      "    accuracy                         0.8309    333334\n",
      "   macro avg     0.7708    0.7940    0.7808    333334\n",
      "weighted avg     0.8399    0.8309    0.8344    333334\n",
      "\n",
      "[0]\tvalidation_0-logloss:0.54662\tvalidation_0-auc:0.87914\tvalidation_1-logloss:0.54673\tvalidation_1-auc:0.87566\n",
      "[50]\tvalidation_0-logloss:0.35753\tvalidation_0-auc:0.89877\tvalidation_1-logloss:0.35877\tvalidation_1-auc:0.89739\n",
      "[100]\tvalidation_0-logloss:0.33450\tvalidation_0-auc:0.89946\tvalidation_1-logloss:0.33655\tvalidation_1-auc:0.89772\n",
      "[150]\tvalidation_0-logloss:0.33050\tvalidation_0-auc:0.89989\tvalidation_1-logloss:0.33315\tvalidation_1-auc:0.89789\n",
      "[199]\tvalidation_0-logloss:0.32938\tvalidation_0-auc:0.90018\tvalidation_1-logloss:0.33249\tvalidation_1-auc:0.89796\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, accuracy 0.826035826035826\n",
      "f1_score 0.7782788607886966\n",
      "auc 0.8979637817441848\n",
      "sensitivity 0.7406743449325901 specificity 0.8536351040727934\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9106    0.8536    0.8812    251891\n",
      "           1     0.6207    0.7407    0.6754     81442\n",
      "\n",
      "    accuracy                         0.8260    333333\n",
      "   macro avg     0.7656    0.7972    0.7783    333333\n",
      "weighted avg     0.8397    0.8260    0.8309    333333\n",
      "\n",
      "[0]\tvalidation_0-logloss:0.54660\tvalidation_0-auc:0.87947\tvalidation_1-logloss:0.54653\tvalidation_1-auc:0.87615\n",
      "[50]\tvalidation_0-logloss:0.35756\tvalidation_0-auc:0.89876\tvalidation_1-logloss:0.36060\tvalidation_1-auc:0.89682\n",
      "[100]\tvalidation_0-logloss:0.33453\tvalidation_0-auc:0.89938\tvalidation_1-logloss:0.33785\tvalidation_1-auc:0.89708\n",
      "[150]\tvalidation_0-logloss:0.33055\tvalidation_0-auc:0.89977\tvalidation_1-logloss:0.33418\tvalidation_1-auc:0.89725\n",
      "[199]\tvalidation_0-logloss:0.32947\tvalidation_0-auc:0.90005\tvalidation_1-logloss:0.33354\tvalidation_1-auc:0.89730\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, accuracy 0.8237198237198238\n",
      "f1_score 0.7776835386183677\n",
      "auc 0.8973006244616395\n",
      "sensitivity 0.7544417568115124 specificity 0.8461193378061852\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9142    0.8461    0.8788    251890\n",
      "           1     0.6132    0.7544    0.6765     81443\n",
      "\n",
      "    accuracy                         0.8237    333333\n",
      "   macro avg     0.7637    0.8003    0.7777    333333\n",
      "weighted avg     0.8407    0.8237    0.8294    333333\n",
      "\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, accuracy 0.82881\n",
      "f1_score 0.778749410402365\n",
      "auc 0.8984966564306567\n",
      "sensitivity 0.721783918570903 specificity 0.8634708818195057\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9055    0.8635    0.8840    151074\n",
      "           1     0.6313    0.7218    0.6735     48926\n",
      "\n",
      "    accuracy                         0.8288    200000\n",
      "   macro avg     0.7684    0.7926    0.7787    200000\n",
      "weighted avg     0.8384    0.8288    0.8325    200000\n",
      "\n",
      "accuracy 0.82881\n",
      "f1_score 0.778749410402365\n",
      "auc 0.8984966564306567\n",
      "specificity 0.8634708818195057\n",
      "sensitivity 0.721783918570903\n"
     ]
    }
   ],
   "source": [
    "smote_ensemble_df, smote_ensemble_predictions = ensemble(smote_oofs, smote_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, accuracy 0.82601\n",
      "f1_score 0.7717170619527024\n",
      "auc 0.8909411367067884\n",
      "sensitivity 0.6915137145893799 specificity 0.869567231952553\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8969    0.8696    0.8830    151074\n",
      "           1     0.6319    0.6915    0.6604     48926\n",
      "\n",
      "    accuracy                         0.8260    200000\n",
      "   macro avg     0.7644    0.7805    0.7717    200000\n",
      "weighted avg     0.8321    0.8260    0.8286    200000\n",
      "\n",
      "accuracy 0.82601\n",
      "f1_score 0.7717170619527024\n",
      "auc 0.8909411367067884\n",
      "specificity 0.869567231952553\n",
      "sensitivity 0.6915137145893799\n",
      "accuracy 0.82601\n",
      "f1_score 0.7717170619527024\n",
      "auc 0.8909411367067884\n",
      "specificity 0.869567231952553\n",
      "sensitivity 0.6915137145893799\n",
      "[0]\tvalidation_0-logloss:0.54650\tvalidation_0-auc:0.88163\tvalidation_1-logloss:0.54635\tvalidation_1-auc:0.87979\n",
      "[50]\tvalidation_0-logloss:0.35589\tvalidation_0-auc:0.90023\tvalidation_1-logloss:0.36046\tvalidation_1-auc:0.89821\n",
      "[100]\tvalidation_0-logloss:0.33223\tvalidation_0-auc:0.90104\tvalidation_1-logloss:0.33764\tvalidation_1-auc:0.89845\n",
      "[150]\tvalidation_0-logloss:0.32806\tvalidation_0-auc:0.90151\tvalidation_1-logloss:0.33414\tvalidation_1-auc:0.89859\n",
      "[199]\tvalidation_0-logloss:0.32693\tvalidation_0-auc:0.90182\tvalidation_1-logloss:0.33333\tvalidation_1-auc:0.89865\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, accuracy 0.8311333377333245\n",
      "f1_score 0.7808680760413935\n",
      "auc 0.8986494285120478\n",
      "sensitivity 0.7207372027061871 specificity 0.8668273181653969\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9057    0.8668    0.8858    251891\n",
      "           1     0.6363    0.7207    0.6759     81443\n",
      "\n",
      "    accuracy                         0.8311    333334\n",
      "   macro avg     0.7710    0.7938    0.7809    333334\n",
      "weighted avg     0.8399    0.8311    0.8345    333334\n",
      "\n",
      "[0]\tvalidation_0-logloss:0.54664\tvalidation_0-auc:0.87943\tvalidation_1-logloss:0.54580\tvalidation_1-auc:0.88353\n",
      "[50]\tvalidation_0-logloss:0.35605\tvalidation_0-auc:0.89999\tvalidation_1-logloss:0.35669\tvalidation_1-auc:0.89848\n",
      "[100]\tvalidation_0-logloss:0.33249\tvalidation_0-auc:0.90072\tvalidation_1-logloss:0.33461\tvalidation_1-auc:0.89904\n",
      "[150]\tvalidation_0-logloss:0.32840\tvalidation_0-auc:0.90116\tvalidation_1-logloss:0.33120\tvalidation_1-auc:0.89933\n",
      "[199]\tvalidation_0-logloss:0.32729\tvalidation_0-auc:0.90148\tvalidation_1-logloss:0.33053\tvalidation_1-auc:0.89952\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, accuracy 0.8316878316878317\n",
      "f1_score 0.7818570208838433\n",
      "auc 0.8995153903339592\n",
      "sensitivity 0.7239139510326367 specificity 0.866533540301162\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9066    0.8665    0.8861    251891\n",
      "           1     0.6368    0.7239    0.6776     81442\n",
      "\n",
      "    accuracy                         0.8317    333333\n",
      "   macro avg     0.7717    0.7952    0.7819    333333\n",
      "weighted avg     0.8407    0.8317    0.8352    333333\n",
      "\n",
      "[0]\tvalidation_0-logloss:0.54623\tvalidation_0-auc:0.88414\tvalidation_1-logloss:0.54827\tvalidation_1-auc:0.87086\n",
      "[50]\tvalidation_0-logloss:0.35604\tvalidation_0-auc:0.90000\tvalidation_1-logloss:0.36103\tvalidation_1-auc:0.89830\n",
      "[100]\tvalidation_0-logloss:0.33258\tvalidation_0-auc:0.90076\tvalidation_1-logloss:0.33711\tvalidation_1-auc:0.89840\n",
      "[142]\tvalidation_0-logloss:0.32879\tvalidation_0-auc:0.90115\tvalidation_1-logloss:0.33342\tvalidation_1-auc:0.89832\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, accuracy 0.8287388287388288\n",
      "f1_score 0.7784451884655266\n",
      "auc 0.8984126195322789\n",
      "sensitivity 0.7209336591235588 specificity 0.8635952201357735\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9054    0.8636    0.8840    251890\n",
      "           1     0.6308    0.7209    0.6729     81443\n",
      "\n",
      "    accuracy                         0.8287    333333\n",
      "   macro avg     0.7681    0.7923    0.7784    333333\n",
      "weighted avg     0.8383    0.8287    0.8324    333333\n",
      "\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, accuracy 0.83393\n",
      "f1_score 0.7798417864463674\n",
      "auc 0.8990268029300295\n",
      "sensitivity 0.6913910804071455 specificity 0.8800918755047196\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8980    0.8801    0.8890    151074\n",
      "           1     0.6512    0.6914    0.6707     48926\n",
      "\n",
      "    accuracy                         0.8339    200000\n",
      "   macro avg     0.7746    0.7857    0.7798    200000\n",
      "weighted avg     0.8377    0.8339    0.8356    200000\n",
      "\n",
      "accuracy 0.83393\n",
      "f1_score 0.7798417864463674\n",
      "auc 0.8990268029300295\n",
      "specificity 0.8800918755047196\n",
      "sensitivity 0.6913910804071455\n"
     ]
    }
   ],
   "source": [
    "under_sampling_ensemble_df, under_sampling_ensemble_predictions = ensemble(under_sampling_oofs, under_sampling_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"../checkpoints/nothing_ensemble.pkl\", \"wb\") as f:\n",
    "    pickle.dump({\n",
    "            \"score_df\":nothing_ensemble_df,\n",
    "            \"predictions\":nothing_ensemble_predictions},f,protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open(f\"../checkpoints/smote_ensemble.pkl\", \"wb\") as f:\n",
    "    pickle.dump({\n",
    "            \"score_df\":smote_ensemble_df,\n",
    "            \"predictions\":smote_ensemble_df},f,protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open(f\"../checkpoints/under_sampling_ensemble.pkl\", \"wb\") as f:\n",
    "    pickle.dump({\n",
    "            \"score_df\":under_sampling_ensemble_df,\n",
    "            \"predictions\":under_sampling_ensemble_predictions},f,protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>accuracy_score</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>auc_score</th>\n",
       "      <th>specificity_score</th>\n",
       "      <th>sensitivity_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blend</td>\n",
       "      <td>0.82601</td>\n",
       "      <td>0.771717</td>\n",
       "      <td>0.890941</td>\n",
       "      <td>0.869567</td>\n",
       "      <td>0.691514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>stack</td>\n",
       "      <td>0.83393</td>\n",
       "      <td>0.779842</td>\n",
       "      <td>0.899027</td>\n",
       "      <td>0.880092</td>\n",
       "      <td>0.691391</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  model_name  accuracy_score  f1_score  auc_score  specificity_score  \\\n",
       "0      blend         0.82601  0.771717   0.890941           0.869567   \n",
       "1      stack         0.83393  0.779842   0.899027           0.880092   \n",
       "\n",
       "   sensitivity_score  \n",
       "0           0.691514  \n",
       "1           0.691391  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "under_sampling_ensemble_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "churn_prediction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
