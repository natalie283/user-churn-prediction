{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/natalie/anaconda3/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, confusion_matrix, accuracy_score, roc_auc_score ,classification_report\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_threshold_f1_score(train_labels, oofs, average='macro'):\n",
    "    scores = []\n",
    "    thresholds = []\n",
    "    best_score = 0\n",
    "    best_threshold = 0\n",
    "    for threshold in np.arange(0.1, 0.9, 0.01):\n",
    "        print(f'{threshold:.02f}, ', end='')\n",
    "        preds = (oofs > threshold).astype('int')\n",
    "        m = f1_score(train_labels, preds, average=average)\n",
    "        scores.append(m)\n",
    "        thresholds.append(threshold)\n",
    "        if m > best_score:\n",
    "            best_score = m\n",
    "            best_threshold = threshold\n",
    "    return best_threshold, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"/Users/natalie/Desktop/DS Thesis/user-churn-prediction/checkpoints/nothing_model_after_tune.pkl\", \"rb\") as f:\n",
    "    nothing_loadded = pickle.load(f)\n",
    "with open(f\"/Users/natalie/Desktop/DS Thesis/user-churn-prediction/checkpoints/smote_model_after_tune.pkl\", \"rb\") as f:\n",
    "    smote_loadded = pickle.load(f)\n",
    "with open(f\"/Users/natalie/Desktop/DS Thesis/user-churn-prediction/checkpoints/class_weight_model_after_tune.pkl\", \"rb\") as f:\n",
    "    class_weight_loadded = pickle.load(f)\n",
    "with open(f\"/Users/natalie/Desktop/DS Thesis/user-churn-prediction/checkpoints/under_sampling_model_after_tune.pkl\", \"rb\") as f:\n",
    "    under_sampling_loadded = pickle.load(f)\n",
    "test = pd.read_parquet(\"/Users/natalie/Desktop/DS Thesis/Code/data/test.parquet\")\n",
    "train = pd.read_parquet(\"/Users/natalie/Desktop/DS Thesis/Code/data/train.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop column province\n",
    "test = test.drop(columns=['province'])\n",
    "train = train.drop(columns=['province'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAMES = nothing_loadded['model_names']\n",
    "nothing_score_df = nothing_loadded['score_df']\n",
    "nothing_oofs = nothing_loadded['oofs']\n",
    "nothing_models = nothing_loadded['models']\n",
    "nothing_predictions = nothing_loadded['predictions']\n",
    "\n",
    "smote_score_df = smote_loadded['score_df']\n",
    "smote_oofs = smote_loadded['oofs']\n",
    "smote_models = smote_loadded['models']\n",
    "smote_predictions = smote_loadded['predictions']\n",
    "\n",
    "class_weight_score_df = class_weight_loadded['score_df']\n",
    "class_weight_oofs = class_weight_loadded['oofs']\n",
    "class_weight_df_models = class_weight_loadded['models']\n",
    "class_weight_predictions = class_weight_loadded['predictions']\n",
    "\n",
    "under_sampling_score_df = under_sampling_loadded['score_df']\n",
    "under_sampling_oofs = under_sampling_loadded['oofs']\n",
    "under_sampling_models = under_sampling_loadded['models']\n",
    "under_sampling_predictions = under_sampling_loadded['predictions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoring(y_test,y_pred_proba, best_threshold):\n",
    "    y_pred = [1 if y_hat >= best_threshold else 0 for y_hat in y_pred_proba]\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    _f1_score = f1_score(y_test, y_pred, average='macro')\n",
    "    auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "    specificity = tn / (tn+fp)\n",
    "    sensitivity = tp / (tp+fn)\n",
    "    print(\"accuracy\", acc)\n",
    "    print(\"f1_score\", _f1_score)\n",
    "    print(\"auc\", auc_score)\n",
    "    print(\"sensitivity\", sensitivity, \"specificity\", specificity)\n",
    "    print(classification_report(y_test, y_pred, digits=4))\n",
    "    return acc, _f1_score, auc_score, specificity, sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_ESTIMATORS=200\n",
    "SEED=42\n",
    "\n",
    "XGBoost_Hyperparameters = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': ['logloss', 'auc'],\n",
    "    'n_estimators': N_ESTIMATORS,\n",
    "    'learning_rate': 0.034630277480196384,\n",
    "    'max_depth': 9,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'subsample': 0.30000000000000004,\n",
    "    'reg_alpha': 0.0020136244579038245,\n",
    "    'reg_lambda': 1.3270228907353322e-06,\n",
    "    'seed': SEED,\n",
    "    # 'scale_pos_weight':3,\n",
    "    'enable_categorical':True,\n",
    "    'early_stopping_rounds': 50,\n",
    "    #'tree_method':'gpu_hist'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "def stacking(step_1_oofs,step_1_predictions):\n",
    "    oofs = np.zeros(step_1_oofs.shape[0])\n",
    "    for i, (train_index, valid_index) in enumerate(kfold.split(class_weight_oofs, train['churn_user'])):\n",
    "        X_train = step_1_oofs[train_index]\n",
    "        X_valid = step_1_oofs[valid_index]\n",
    "        y_train = train.iloc[train_index]['churn_user']\n",
    "        y_valid = train.iloc[valid_index]['churn_user']\n",
    "        xgb_model = XGBClassifier(**XGBoost_Hyperparameters)\n",
    "        xgb_model.fit(X_train, y_train,\n",
    "                    eval_set=[(X_train, y_train), (X_valid, y_valid)],\n",
    "                    verbose=50)\n",
    "        oofs[valid_index] = xgb_model.predict_proba(X_valid)[:,1]\n",
    "        best_threshold, best_score = find_best_threshold_f1_score(y_valid, oofs[valid_index])\n",
    "        acc, _f1_score, auc_score, specificity, sensitivity = scoring(y_valid,oofs[valid_index],best_threshold)\n",
    "    best_threshold, best_score = find_best_threshold_f1_score(train['churn_user'], oofs)\n",
    "    xgboost_hyperparameters = XGBoost_Hyperparameters.copy()\n",
    "    del xgboost_hyperparameters['early_stopping_rounds']\n",
    "    xgb_model = XGBClassifier(**xgboost_hyperparameters)\n",
    "    xgb_model.fit(step_1_oofs, train['churn_user'])\n",
    "    X_test = np.vstack(step_1_predictions).transpose()\n",
    "    y_test = test['churn_user']\n",
    "    y_pred_proba = xgb_model.predict_proba(X_test)[:,1]\n",
    "    acc, _f1_score, auc_score, specificity, sensitivity = scoring(y_test,y_pred_proba,best_threshold)\n",
    "    return acc, _f1_score, auc_score, specificity, sensitivity, y_pred_proba\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blend(stage_1_oofs, stage_1_predictions):\n",
    "    y_pred_proba = stage_1_oofs.mean(axis=1)\n",
    "    best_threshold, best_score = find_best_threshold_f1_score(train['churn_user'], y_pred_proba)\n",
    "    X_test = np.vstack(stage_1_predictions).transpose()\n",
    "    y_test = test['churn_user']\n",
    "    y_pred_proba= X_test.mean(axis=1)\n",
    "    acc, _f1_score, auc_score, specificity, sensitivity = scoring(y_test,y_pred_proba,best_threshold)\n",
    "    print(\"accuracy\", acc)\n",
    "    print(\"f1_score\", _f1_score)\n",
    "    print(\"auc\", auc_score)\n",
    "    print(\"specificity\", specificity)\n",
    "    print(\"sensitivity\", sensitivity)\n",
    "    return acc, _f1_score, auc_score, specificity, sensitivity, y_pred_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble(stage_1_oofs, stage_1_predictions):\n",
    "    accuracy_scores = []\n",
    "    f1_scores = []\n",
    "    auc_scores = []\n",
    "    specificity_scores = []\n",
    "    sensitivity_scores = []\n",
    "    predictions = []\n",
    "   \n",
    "    # Blending\n",
    "    acc, _f1_score, auc_score, specificity, sensitivity, y_pred_proba = blend(stage_1_oofs, stage_1_predictions)\n",
    "    print(\"accuracy\", acc)\n",
    "    print(\"f1_score\", _f1_score)\n",
    "    print(\"auc\", auc_score)\n",
    "    print(\"specificity\", specificity)\n",
    "    print(\"sensitivity\", sensitivity)\n",
    "    accuracy_scores.append(acc)\n",
    "    f1_scores.append(_f1_score)\n",
    "    auc_scores.append(auc_score)\n",
    "    specificity_scores.append(specificity)\n",
    "    sensitivity_scores.append(sensitivity)\n",
    "    predictions.append(y_pred_proba)\n",
    "\n",
    "    # Stacking\n",
    "    acc, _f1_score, auc_score, specificity, sensitivity, y_pred_proba = stacking(stage_1_oofs, stage_1_predictions)\n",
    "    print(\"accuracy\", acc)\n",
    "    print(\"f1_score\", _f1_score)\n",
    "    print(\"auc\", auc_score)\n",
    "    print(\"specificity\", specificity)\n",
    "    print(\"sensitivity\", sensitivity)\n",
    "    accuracy_scores.append(acc)\n",
    "    f1_scores.append(_f1_score)\n",
    "    auc_scores.append(auc_score)\n",
    "    specificity_scores.append(specificity)\n",
    "    sensitivity_scores.append(sensitivity)\n",
    "    predictions.append(y_pred_proba)\n",
    "\n",
    "    score_df = pd.DataFrame({'model_name': ['blend', 'stack'],\n",
    "                         'accuracy_score':accuracy_scores, \n",
    "                         'f1_score': f1_scores, \n",
    "                         'auc_score': auc_scores, \n",
    "                         'specificity_score': specificity_scores, \n",
    "                         'sensitivity_score': sensitivity_scores})\n",
    "    return score_df, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, accuracy 0.832705\n",
      "f1_score 0.7822604639308044\n",
      "auc 0.900169332605727\n",
      "sensitivity 0.7181866492253608 specificity 0.8697922872234799\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9050    0.8698    0.8871    151074\n",
      "           1     0.6411    0.7182    0.6775     48926\n",
      "\n",
      "    accuracy                         0.8327    200000\n",
      "   macro avg     0.7731    0.7940    0.7823    200000\n",
      "weighted avg     0.8405    0.8327    0.8358    200000\n",
      "\n",
      "accuracy 0.832705\n",
      "f1_score 0.7822604639308044\n",
      "auc 0.900169332605727\n",
      "specificity 0.8697922872234799\n",
      "sensitivity 0.7181866492253608\n",
      "accuracy 0.832705\n",
      "f1_score 0.7822604639308044\n",
      "auc 0.900169332605727\n",
      "specificity 0.8697922872234799\n",
      "sensitivity 0.7181866492253608\n",
      "[0]\tvalidation_0-logloss:0.54315\tvalidation_0-auc:0.90016\tvalidation_1-logloss:0.54325\tvalidation_1-auc:0.89786\n",
      "[50]\tvalidation_0-logloss:0.34078\tvalidation_0-auc:0.90447\tvalidation_1-logloss:0.34386\tvalidation_1-auc:0.90155\n",
      "[100]\tvalidation_0-logloss:0.32264\tvalidation_0-auc:0.90497\tvalidation_1-logloss:0.32752\tvalidation_1-auc:0.90151\n",
      "[108]\tvalidation_0-logloss:0.32193\tvalidation_0-auc:0.90502\tvalidation_1-logloss:0.32701\tvalidation_1-auc:0.90151\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, accuracy 0.8309263381473238\n",
      "f1_score 0.7834804419268782\n",
      "auc 0.9015613480883742\n",
      "sensitivity 0.7424701938779269 specificity 0.8595265412420452\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9117    0.8595    0.8848    251891\n",
      "           1     0.6309    0.7425    0.6821     81443\n",
      "\n",
      "    accuracy                         0.8309    333334\n",
      "   macro avg     0.7713    0.8010    0.7835    333334\n",
      "weighted avg     0.8431    0.8309    0.8353    333334\n",
      "\n",
      "[0]\tvalidation_0-logloss:0.54310\tvalidation_0-auc:0.89979\tvalidation_1-logloss:0.54318\tvalidation_1-auc:0.89787\n",
      "[50]\tvalidation_0-logloss:0.34102\tvalidation_0-auc:0.90424\tvalidation_1-logloss:0.34341\tvalidation_1-auc:0.90203\n",
      "[100]\tvalidation_0-logloss:0.32291\tvalidation_0-auc:0.90474\tvalidation_1-logloss:0.32692\tvalidation_1-auc:0.90202\n",
      "[119]\tvalidation_0-logloss:0.32144\tvalidation_0-auc:0.90488\tvalidation_1-logloss:0.32599\tvalidation_1-auc:0.90199\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, accuracy 0.831981831981832\n",
      "f1_score 0.7835236968507788\n",
      "auc 0.9020354979801523\n",
      "sensitivity 0.7343753837086515 specificity 0.8635401820628764\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9095    0.8635    0.8859    251891\n",
      "           1     0.6350    0.7344    0.6811     81442\n",
      "\n",
      "    accuracy                         0.8320    333333\n",
      "   macro avg     0.7723    0.7990    0.7835    333333\n",
      "weighted avg     0.8425    0.8320    0.8359    333333\n",
      "\n",
      "[0]\tvalidation_0-logloss:0.54309\tvalidation_0-auc:0.90010\tvalidation_1-logloss:0.54315\tvalidation_1-auc:0.89858\n",
      "[50]\tvalidation_0-logloss:0.34098\tvalidation_0-auc:0.90433\tvalidation_1-logloss:0.34337\tvalidation_1-auc:0.90194\n",
      "[100]\tvalidation_0-logloss:0.32289\tvalidation_0-auc:0.90476\tvalidation_1-logloss:0.32692\tvalidation_1-auc:0.90194\n",
      "[122]\tvalidation_0-logloss:0.32128\tvalidation_0-auc:0.90493\tvalidation_1-logloss:0.32591\tvalidation_1-auc:0.90194\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, accuracy 0.8337938337938338\n",
      "f1_score 0.7835151824312359\n",
      "auc 0.901961333754874\n",
      "sensitivity 0.7200741622975578 specificity 0.8705625471435944\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9058    0.8706    0.8878    251890\n",
      "           1     0.6427    0.7201    0.6792     81443\n",
      "\n",
      "    accuracy                         0.8338    333333\n",
      "   macro avg     0.7743    0.7953    0.7835    333333\n",
      "weighted avg     0.8415    0.8338    0.8369    333333\n",
      "\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, accuracy 0.83026\n",
      "f1_score 0.7834430232822465\n",
      "auc 0.901558282179625\n",
      "sensitivity 0.7466377795037403 specificity 0.8573414353230867\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9127    0.8573    0.8841    151074\n",
      "           1     0.6289    0.7466    0.6828     48926\n",
      "\n",
      "    accuracy                         0.8303    200000\n",
      "   macro avg     0.7708    0.8020    0.7834    200000\n",
      "weighted avg     0.8432    0.8303    0.8349    200000\n",
      "\n",
      "accuracy 0.83026\n",
      "f1_score 0.7834430232822465\n",
      "auc 0.901558282179625\n",
      "specificity 0.8573414353230867\n",
      "sensitivity 0.7466377795037403\n"
     ]
    }
   ],
   "source": [
    "nothing_ensemble_df, nothing_ensemble_predictions = ensemble(nothing_oofs, nothing_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, accuracy 0.830165\n",
      "f1_score 0.7805046971801104\n",
      "auc 0.8987230958014302\n",
      "sensitivity 0.7245840657319217 specificity 0.8643578643578643\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9065    0.8644    0.8849    151074\n",
      "           1     0.6337    0.7246    0.6761     48926\n",
      "\n",
      "    accuracy                         0.8302    200000\n",
      "   macro avg     0.7701    0.7945    0.7805    200000\n",
      "weighted avg     0.8397    0.8302    0.8338    200000\n",
      "\n",
      "accuracy 0.830165\n",
      "f1_score 0.7805046971801104\n",
      "auc 0.8987230958014302\n",
      "specificity 0.8643578643578643\n",
      "sensitivity 0.7245840657319217\n",
      "accuracy 0.830165\n",
      "f1_score 0.7805046971801104\n",
      "auc 0.8987230958014302\n",
      "specificity 0.8643578643578643\n",
      "sensitivity 0.7245840657319217\n",
      "[0]\tvalidation_0-logloss:0.54327\tvalidation_0-auc:0.89901\tvalidation_1-logloss:0.54335\tvalidation_1-auc:0.89621\n",
      "[50]\tvalidation_0-logloss:0.34181\tvalidation_0-auc:0.90375\tvalidation_1-logloss:0.34521\tvalidation_1-auc:0.90060\n",
      "[90]\tvalidation_0-logloss:0.32508\tvalidation_0-auc:0.90414\tvalidation_1-logloss:0.33005\tvalidation_1-auc:0.90057\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, accuracy 0.8317993364013272\n",
      "f1_score 0.7821815214349945\n",
      "auc 0.9006204592253836\n",
      "sensitivity 0.7255012708274499 specificity 0.8661683029564375\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9071    0.8662    0.8861    251891\n",
      "           1     0.6367    0.7255    0.6782     81443\n",
      "\n",
      "    accuracy                         0.8318    333334\n",
      "   macro avg     0.7719    0.7958    0.7822    333334\n",
      "weighted avg     0.8410    0.8318    0.8353    333334\n",
      "\n",
      "[0]\tvalidation_0-logloss:0.54325\tvalidation_0-auc:0.89886\tvalidation_1-logloss:0.54324\tvalidation_1-auc:0.89747\n",
      "[50]\tvalidation_0-logloss:0.34219\tvalidation_0-auc:0.90339\tvalidation_1-logloss:0.34453\tvalidation_1-auc:0.90125\n",
      "[93]\tvalidation_0-logloss:0.32504\tvalidation_0-auc:0.90384\tvalidation_1-logloss:0.32895\tvalidation_1-auc:0.90123\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, accuracy 0.8331818331818331\n",
      "f1_score 0.7830737755664516\n",
      "auc 0.901257078828351\n",
      "sensitivity 0.7215073303700793 specificity 0.869288700271149\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9061    0.8693    0.8873    251891\n",
      "           1     0.6409    0.7215    0.6788     81442\n",
      "\n",
      "    accuracy                         0.8332    333333\n",
      "   macro avg     0.7735    0.7954    0.7831    333333\n",
      "weighted avg     0.8413    0.8332    0.8364    333333\n",
      "\n",
      "[0]\tvalidation_0-logloss:0.54322\tvalidation_0-auc:0.89901\tvalidation_1-logloss:0.54327\tvalidation_1-auc:0.89698\n",
      "[50]\tvalidation_0-logloss:0.34204\tvalidation_0-auc:0.90361\tvalidation_1-logloss:0.34489\tvalidation_1-auc:0.90078\n",
      "[100]\tvalidation_0-logloss:0.32400\tvalidation_0-auc:0.90408\tvalidation_1-logloss:0.32863\tvalidation_1-auc:0.90079\n",
      "[127]\tvalidation_0-logloss:0.32209\tvalidation_0-auc:0.90431\tvalidation_1-logloss:0.32756\tvalidation_1-auc:0.90078\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, accuracy 0.8338178338178338\n",
      "f1_score 0.7822342749175903\n",
      "auc 0.9008209488396207\n",
      "sensitivity 0.7103495696376607 specificity 0.873738536662829\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9032    0.8737    0.8882    251890\n",
      "           1     0.6453    0.7103    0.6762     81443\n",
      "\n",
      "    accuracy                         0.8338    333333\n",
      "   macro avg     0.7742    0.7920    0.7822    333333\n",
      "weighted avg     0.8402    0.8338    0.8364    333333\n",
      "\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, accuracy 0.82786\n",
      "f1_score 0.7815723624870002\n",
      "auc 0.9006339266454525\n",
      "sensitivity 0.7511752442464129 specificity 0.8526947059057151\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9137    0.8527    0.8821    151074\n",
      "           1     0.6229    0.7512    0.6810     48926\n",
      "\n",
      "    accuracy                         0.8279    200000\n",
      "   macro avg     0.7683    0.8019    0.7816    200000\n",
      "weighted avg     0.8425    0.8279    0.8329    200000\n",
      "\n",
      "accuracy 0.82786\n",
      "f1_score 0.7815723624870002\n",
      "auc 0.9006339266454525\n",
      "specificity 0.8526947059057151\n",
      "sensitivity 0.7511752442464129\n"
     ]
    }
   ],
   "source": [
    "smote_ensemble_df, smote_ensemble_predictions = ensemble(smote_oofs, smote_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, accuracy 0.832055\n",
      "f1_score 0.7815594440069826\n",
      "auc 0.8998048536108166\n",
      "sensitivity 0.717941380860892 specificity 0.8690112130479103\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9049    0.8690    0.8866    151074\n",
      "           1     0.6396    0.7179    0.6765     48926\n",
      "\n",
      "    accuracy                         0.8321    200000\n",
      "   macro avg     0.7723    0.7935    0.7816    200000\n",
      "weighted avg     0.8400    0.8321    0.8352    200000\n",
      "\n",
      "accuracy 0.832055\n",
      "f1_score 0.7815594440069826\n",
      "auc 0.8998048536108166\n",
      "specificity 0.8690112130479103\n",
      "sensitivity 0.717941380860892\n",
      "accuracy 0.832055\n",
      "f1_score 0.7815594440069826\n",
      "auc 0.8998048536108166\n",
      "specificity 0.8690112130479103\n",
      "sensitivity 0.717941380860892\n",
      "[0]\tvalidation_0-logloss:0.54322\tvalidation_0-auc:0.89993\tvalidation_1-logloss:0.54331\tvalidation_1-auc:0.89773\n",
      "[50]\tvalidation_0-logloss:0.34146\tvalidation_0-auc:0.90406\tvalidation_1-logloss:0.34469\tvalidation_1-auc:0.90102\n",
      "[96]\tvalidation_0-logloss:0.32386\tvalidation_0-auc:0.90450\tvalidation_1-logloss:0.32880\tvalidation_1-auc:0.90098\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, accuracy 0.8323783352433295\n",
      "f1_score 0.7831089521005916\n",
      "auc 0.9010269390488443\n",
      "sensitivity 0.7280429257271958 specificity 0.8661127233605012\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9078    0.8661    0.8865    251891\n",
      "           1     0.6374    0.7280    0.6797     81443\n",
      "\n",
      "    accuracy                         0.8324    333334\n",
      "   macro avg     0.7726    0.7971    0.7831    333334\n",
      "weighted avg     0.8418    0.8324    0.8360    333334\n",
      "\n",
      "[0]\tvalidation_0-logloss:0.54315\tvalidation_0-auc:0.89953\tvalidation_1-logloss:0.54318\tvalidation_1-auc:0.89777\n",
      "[50]\tvalidation_0-logloss:0.34170\tvalidation_0-auc:0.90382\tvalidation_1-logloss:0.34416\tvalidation_1-auc:0.90145\n",
      "[100]\tvalidation_0-logloss:0.32371\tvalidation_0-auc:0.90429\tvalidation_1-logloss:0.32793\tvalidation_1-auc:0.90142\n",
      "[122]\tvalidation_0-logloss:0.32209\tvalidation_0-auc:0.90446\tvalidation_1-logloss:0.32694\tvalidation_1-auc:0.90140\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, accuracy 0.832956832956833\n",
      "f1_score 0.7832525587727575\n",
      "auc 0.9014588895536062\n",
      "sensitivity 0.7246138356130744 specificity 0.8679865497377834\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9070    0.8680    0.8870    251891\n",
      "           1     0.6396    0.7246    0.6795     81442\n",
      "\n",
      "    accuracy                         0.8330    333333\n",
      "   macro avg     0.7733    0.7963    0.7833    333333\n",
      "weighted avg     0.8416    0.8330    0.8363    333333\n",
      "\n",
      "[0]\tvalidation_0-logloss:0.54315\tvalidation_0-auc:0.89953\tvalidation_1-logloss:0.54320\tvalidation_1-auc:0.89824\n",
      "[50]\tvalidation_0-logloss:0.34174\tvalidation_0-auc:0.90382\tvalidation_1-logloss:0.34441\tvalidation_1-auc:0.90132\n",
      "[100]\tvalidation_0-logloss:0.32373\tvalidation_0-auc:0.90426\tvalidation_1-logloss:0.32798\tvalidation_1-auc:0.90134\n",
      "[126]\tvalidation_0-logloss:0.32190\tvalidation_0-auc:0.90448\tvalidation_1-logloss:0.32691\tvalidation_1-auc:0.90133\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, accuracy 0.8343488343488343\n",
      "f1_score 0.782574364264881\n",
      "auc 0.901360510800763\n",
      "sensitivity 0.7088147538769446 specificity 0.87493747270634\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9028    0.8749    0.8887    251890\n",
      "           1     0.6470    0.7088    0.6765     81443\n",
      "\n",
      "    accuracy                         0.8343    333333\n",
      "   macro avg     0.7749    0.7919    0.7826    333333\n",
      "weighted avg     0.8403    0.8343    0.8368    333333\n",
      "\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, accuracy 0.830325\n",
      "f1_score 0.783197551434002\n",
      "auc 0.9013222013266642\n",
      "sensitivity 0.74416465682868 specificity 0.8582284178614454\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9120    0.8582    0.8843    151074\n",
      "           1     0.6296    0.7442    0.6821     48926\n",
      "\n",
      "    accuracy                         0.8303    200000\n",
      "   macro avg     0.7708    0.8012    0.7832    200000\n",
      "weighted avg     0.8429    0.8303    0.8348    200000\n",
      "\n",
      "accuracy 0.830325\n",
      "f1_score 0.783197551434002\n",
      "auc 0.9013222013266642\n",
      "specificity 0.8582284178614454\n",
      "sensitivity 0.74416465682868\n"
     ]
    }
   ],
   "source": [
    "under_sampling_ensemble_df, under_sampling_ensemble_predictions = ensemble(under_sampling_oofs, under_sampling_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"/Users/natalie/Desktop/DS Thesis/user-churn-prediction/checkpoints/nothing_ensemble_after_tune.pkl\", \"wb\") as f:\n",
    "    pickle.dump({\n",
    "            \"score_df\":nothing_ensemble_df,\n",
    "            \"predictions\":nothing_ensemble_predictions},f,protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open(f\"/Users/natalie/Desktop/DS Thesis/user-churn-prediction/checkpoints/smote_ensemble_after_tune.pkl\", \"wb\") as f:\n",
    "    pickle.dump({\n",
    "            \"score_df\":smote_ensemble_df,\n",
    "            \"predictions\":smote_ensemble_df},f,protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open(f\"/Users/natalie/Desktop/DS Thesis/user-churn-prediction/checkpoints/under_sampling_ensemble_after_tune.pkl\", \"wb\") as f:\n",
    "    pickle.dump({\n",
    "            \"score_df\":under_sampling_ensemble_df,\n",
    "            \"predictions\":under_sampling_ensemble_predictions},f,protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>accuracy_score</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>auc_score</th>\n",
       "      <th>specificity_score</th>\n",
       "      <th>sensitivity_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blend</td>\n",
       "      <td>0.832055</td>\n",
       "      <td>0.781559</td>\n",
       "      <td>0.899805</td>\n",
       "      <td>0.869011</td>\n",
       "      <td>0.717941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>stack</td>\n",
       "      <td>0.830325</td>\n",
       "      <td>0.783198</td>\n",
       "      <td>0.901322</td>\n",
       "      <td>0.858228</td>\n",
       "      <td>0.744165</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  model_name  accuracy_score  f1_score  auc_score  specificity_score  \\\n",
       "0      blend        0.832055  0.781559   0.899805           0.869011   \n",
       "1      stack        0.830325  0.783198   0.901322           0.858228   \n",
       "\n",
       "   sensitivity_score  \n",
       "0           0.717941  \n",
       "1           0.744165  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "under_sampling_ensemble_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "churn_prediction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
