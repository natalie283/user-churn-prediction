{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-29 18:33:00,175] A new study created in memory with name: no-name-2364dd95-5c47-4579-b73e-959f77497c25\n",
      "[W 2024-05-29 18:33:00,206] Trial 0 failed with parameters: {'booster': 'gbtree', 'lambda': 5.171775688696952e-05, 'alpha': 1.240042393827509e-07, 'max_depth': 7, 'eta': 4.0412946344311053e-07, 'gamma': 0.012655517156345403, 'grow_policy': 'depthwise'} because of the following error: ModuleNotFoundError(\"Optuna's integration modules for third-party libraries have started migrating from Optuna itself to a package called `optuna-integration`. The module you are trying to use has already been migrated to `optuna-integration`. Please install the package by running `pip install optuna-integration`.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/natalie/anaconda3/lib/python3.8/site-packages/optuna/integration/__init__.py\", line 141, in _get_module\n",
      "    return importlib.import_module(\".\" + module_name, self.__name__)\n",
      "  File \"/Users/natalie/anaconda3/lib/python3.8/importlib/__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 783, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/Users/natalie/anaconda3/lib/python3.8/site-packages/optuna/integration/xgboost.py\", line 1, in <module>\n",
      "    from optuna_integration.xgboost import XGBoostPruningCallback\n",
      "ModuleNotFoundError: No module named 'optuna_integration'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/natalie/anaconda3/lib/python3.8/site-packages/optuna/study/_optimize.py\", line 196, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/var/folders/k6/pmg4h8sn5dj349z5gdkp326m0000gn/T/ipykernel_12764/1788100655.py\", line 31, in objective\n",
      "    pruning_callback = optuna.integration.XGBoostPruningCallback(trial, \"test-auc\")\n",
      "  File \"/Users/natalie/anaconda3/lib/python3.8/site-packages/optuna/integration/__init__.py\", line 129, in __getattr__\n",
      "    module = self._get_module(self._class_to_module[name])\n",
      "  File \"/Users/natalie/anaconda3/lib/python3.8/site-packages/optuna/integration/__init__.py\", line 143, in _get_module\n",
      "    raise ModuleNotFoundError(\n",
      "ModuleNotFoundError: Optuna's integration modules for third-party libraries have started migrating from Optuna itself to a package called `optuna-integration`. The module you are trying to use has already been migrated to `optuna-integration`. Please install the package by running `pip install optuna-integration`.\n",
      "[W 2024-05-29 18:33:00,209] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "Optuna's integration modules for third-party libraries have started migrating from Optuna itself to a package called `optuna-integration`. The module you are trying to use has already been migrated to `optuna-integration`. Please install the package by running `pip install optuna-integration`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/optuna/integration/__init__.py:141\u001b[0m, in \u001b[0;36m_IntegrationModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/importlib/__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    126\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1014\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:991\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:975\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:671\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:783\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:219\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/optuna/integration/xgboost.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01moptuna_integration\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mxgboost\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m XGBoostPruningCallback\n\u001b[1;32m      4\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mXGBoostPruningCallback\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'optuna_integration'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [3], line 41\u001b[0m\n\u001b[1;32m     39\u001b[0m pruner \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mpruners\u001b[38;5;241m.\u001b[39mMedianPruner(n_warmup_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m     40\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(pruner\u001b[38;5;241m=\u001b[39mpruner, direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 41\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of finished trials: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mlen\u001b[39m(study\u001b[38;5;241m.\u001b[39mtrials)))\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest trial:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/optuna/study/study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    357\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    358\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \n\u001b[1;32m    361\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 451\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/optuna/study/_optimize.py:62\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 62\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/optuna/study/_optimize.py:159\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 159\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/optuna/study/_optimize.py:247\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    243\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    246\u001b[0m ):\n\u001b[0;32m--> 247\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/optuna/study/_optimize.py:196\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 196\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    199\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn [3], line 31\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     28\u001b[0m     param[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrate_drop\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m trial\u001b[38;5;241m.\u001b[39msuggest_float(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrate_drop\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1e-8\u001b[39m, \u001b[38;5;241m1.0\u001b[39m, log\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     29\u001b[0m     param[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip_drop\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m trial\u001b[38;5;241m.\u001b[39msuggest_float(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip_drop\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1e-8\u001b[39m, \u001b[38;5;241m1.0\u001b[39m, log\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 31\u001b[0m pruning_callback \u001b[38;5;241m=\u001b[39m \u001b[43moptuna\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintegration\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXGBoostPruningCallback\u001b[49m(trial, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest-auc\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     32\u001b[0m history \u001b[38;5;241m=\u001b[39m xgb\u001b[38;5;241m.\u001b[39mcv(param, dtrain, num_boost_round\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, callbacks\u001b[38;5;241m=\u001b[39m[pruning_callback])\n\u001b[1;32m     34\u001b[0m mean_auc \u001b[38;5;241m=\u001b[39m history[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest-auc-mean\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/optuna/integration/__init__.py:129\u001b[0m, in \u001b[0;36m_IntegrationModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    127\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(name)\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m--> 129\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/optuna/integration/__init__.py:143\u001b[0m, in \u001b[0;36m_IntegrationModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m:\n\u001b[0;32m--> 143\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m(\n\u001b[1;32m    144\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptuna\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms integration modules for third-party libraries have started \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmigrating from Optuna itself to a package called `optuna-integration`. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    146\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe module you are trying to use has already been migrated to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    147\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`optuna-integration`. Please install the package by running \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    148\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`pip install optuna-integration`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    149\u001b[0m     )\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: Optuna's integration modules for third-party libraries have started migrating from Optuna itself to a package called `optuna-integration`. The module you are trying to use has already been migrated to `optuna-integration`. Please install the package by running `pip install optuna-integration`."
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "\n",
    "import sklearn.datasets\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    train_x, train_y = sklearn.datasets.load_breast_cancer(return_X_y=True)\n",
    "    dtrain = xgb.DMatrix(train_x, label=train_y)\n",
    "\n",
    "    param = {\n",
    "        \"verbosity\": 0,\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"eval_metric\": \"auc\",\n",
    "        \"booster\": trial.suggest_categorical(\"booster\", [\"gbtree\", \"gblinear\", \"dart\"]),\n",
    "        \"lambda\": trial.suggest_float(\"lambda\", 1e-8, 1.0, log=True),\n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 1.0, log=True),\n",
    "    }\n",
    "\n",
    "    if param[\"booster\"] == \"gbtree\" or param[\"booster\"] == \"dart\":\n",
    "        param[\"max_depth\"] = trial.suggest_int(\"max_depth\", 1, 9)\n",
    "        param[\"eta\"] = trial.suggest_float(\"eta\", 1e-8, 1.0, log=True)\n",
    "        param[\"gamma\"] = trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True)\n",
    "        param[\"grow_policy\"] = trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"])\n",
    "    if param[\"booster\"] == \"dart\":\n",
    "        param[\"sample_type\"] = trial.suggest_categorical(\"sample_type\", [\"uniform\", \"weighted\"])\n",
    "        param[\"normalize_type\"] = trial.suggest_categorical(\"normalize_type\", [\"tree\", \"forest\"])\n",
    "        param[\"rate_drop\"] = trial.suggest_float(\"rate_drop\", 1e-8, 1.0, log=True)\n",
    "        param[\"skip_drop\"] = trial.suggest_float(\"skip_drop\", 1e-8, 1.0, log=True)\n",
    "\n",
    "    pruning_callback = optuna.integration.XGBoostPruningCallback(trial, \"test-auc\")\n",
    "    history = xgb.cv(param, dtrain, num_boost_round=100, callbacks=[pruning_callback])\n",
    "\n",
    "    mean_auc = history[\"test-auc-mean\"].values[-1]\n",
    "    return mean_auc\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pruner = optuna.pruners.MedianPruner(n_warmup_steps=5)\n",
    "    study = optuna.create_study(pruner=pruner, direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=100)\n",
    "\n",
    "    print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print(\"  Value: {}\".format(trial.value))\n",
    "\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/natalie/anaconda3/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder \n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import classification_report, roc_auc_score, f1_score, accuracy_score,confusion_matrix\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "   \n",
    "\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_parquet('/Users/natalie/Desktop/DS Thesis/Code/data/test.parquet')\n",
    "train = pd.read_parquet('/Users/natalie/Desktop/DS Thesis/Code/data/train.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET ='churn_user'\n",
    "CATEGORICAL_FEATURES  = ['os_name', 'age_group','gender', 'country', 'region', 'province_type',\n",
    "                         'province']\n",
    "DATETIME_FEATURES  = ['first_date', 'lastest_active_day']\n",
    "SEARCH_CC_FEATURES = [ 'clicks', 'search_volume', 'dating_search', 'videoclip_search', 'technical_search', 'housekeeping_family_search', 'marketing_search', 'other_search']\n",
    "SEARCH_GG_FEATURES = [ 'serp_click', 'search_volume_gg', 'search_clicks_gg', 'other_search_gg','housekeeping_family_search_gg','videoclip_search_gg', 'dating_search_gg', 'marketing_search_gg', 'technical_search_gg']\n",
    "ACTIVE_FEATURES = ['active_day', 'life_time',  'not_active_day', 'total_active_time']\n",
    "ADS_FEATURES =  ['ads_impression', 'ads_click', 'ads_revenue']\n",
    "OTHERS_FEATURES =[                      'newtab_count', 'download_count', 'pip_count', 'sidebar_count', 'incognito_count', 'signin_count', 'youtube_count',\n",
    "                    'work_count', 'social_count', 'news_count', 'entertainment_count', 'ecommerce_count']\n",
    "NUMERICAL_FEATURES = SEARCH_CC_FEATURES + SEARCH_GG_FEATURES + ACTIVE_FEATURES + ADS_FEATURES + OTHERS_FEATURES\n",
    "\n",
    "MODEL_NAMES = ['log_reg', 'randomforest','lightgbm', 'xgboost', 'mlp']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_threshold_f1_score(train_labels, oofs, average='macro'):\n",
    "    scores = []\n",
    "    thresholds = []\n",
    "    best_score = 0\n",
    "    best_threshold = 0\n",
    "    for threshold in np.arange(0.1, 0.9, 0.01):\n",
    "        print(f'{threshold:.02f}, ', end='')\n",
    "        preds = (oofs > threshold).astype('int')\n",
    "        m = f1_score(train_labels, preds, average=average)\n",
    "        scores.append(m)\n",
    "        thresholds.append(threshold)\n",
    "        if m > best_score:\n",
    "            best_score = m\n",
    "            best_threshold = threshold\n",
    "    return best_threshold, best_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_feature_engineer(df):\n",
    "    temp = pd.DataFrame()\n",
    "    temp['OTHERS_FEATURES_SUM'] = df[OTHERS_FEATURES].sum(axis=1)\n",
    "    temp['OTHERS_FEATURES_MIN'] = df[OTHERS_FEATURES].min(axis=1)\n",
    "    temp['OTHERS_FEATURES_MAX'] = df[OTHERS_FEATURES].max(axis=1)\n",
    "    temp['OTHERS_FEATURES_MEAN'] = df[OTHERS_FEATURES].mean(axis=1)\n",
    "    temp['ADS_CTR'] = df['ads_click'] / df['ads_impression']\n",
    "    temp['SEARCH_CC_FEATURES_SUM'] = df[SEARCH_CC_FEATURES[1:]].sum(axis=1)\n",
    "    temp['SEARCH_CC_FEATURES_MIN'] = df[SEARCH_CC_FEATURES[1:]].min(axis=1)\n",
    "    temp['SEARCH_CC_FEATURES_MAX'] = df[SEARCH_CC_FEATURES[1:]].max(axis=1)\n",
    "    temp['SEARCH_CC_FEATURES_MEAN'] = df[SEARCH_CC_FEATURES[1:]].mean(axis=1)\n",
    "    temp['SEARCH_GG_FEATURES_SUM'] = df[SEARCH_GG_FEATURES[1:]].sum(axis=1)\n",
    "    temp['SEARCH_GG_FEATURES_MIN'] = df[SEARCH_GG_FEATURES[1:]].min(axis=1)\n",
    "    temp['SEARCH_GG_FEATURES_MAX'] = df[SEARCH_GG_FEATURES[1:]].max(axis=1)\n",
    "    temp['SEARCH_GG_FEATURES_MEAN'] = df[SEARCH_GG_FEATURES[1:]].mean(axis=1)\n",
    "    temp['not_active_day_per_active_day'] = df['not_active_day'] / df['active_day']\n",
    "    temp['life_time_per_active_day'] = df['life_time'] / df['active_day']\n",
    "    return temp, list(temp.columns)\n",
    "\n",
    "def fillna(df):\n",
    "    df['total_active_time'] = df['total_active_time'].fillna(0)\n",
    "    df['ads_impression'] = df['ads_impression'].fillna(0)\n",
    "    df['ads_click'] = df['ads_click'].fillna(0)\n",
    "    df['ads_revenue'] = df['ads_revenue'].fillna(0)\n",
    "    df['clicks'] = df['clicks'].fillna(0)\n",
    "    for c in OTHERS_FEATURES:\n",
    "        df[c] = df[c].fillna(0)\n",
    "    return df\n",
    "\n",
    "def process_data(df,oh_encoder=None, robust_scaler=None,agg_features=None):\n",
    "    if not oh_encoder:\n",
    "        print(\"fit train OneHotEncoder\")\n",
    "        oh_encoder = OneHotEncoder()\n",
    "        oh_encoder.fit(df[CATEGORICAL_FEATURES])\n",
    "    else:\n",
    "        print(\"loadd onehot encoder\")\n",
    "    if not robust_scaler:\n",
    "        print(\"fit train RobustScaler\")\n",
    "        robust_scaler = RobustScaler()\n",
    "        robust_scaler.fit(df[NUMERICAL_FEATURES])\n",
    "    else:\n",
    "        print(\"loadd robust scaler\")\n",
    "    df_cat = pd.DataFrame(oh_encoder.transform(df[CATEGORICAL_FEATURES]).toarray())\n",
    "    new_cat_cols = oh_encoder.get_feature_names_out(CATEGORICAL_FEATURES)\n",
    "    df_cat.columns = new_cat_cols\n",
    "    df_num = pd.DataFrame(robust_scaler.transform(df[NUMERICAL_FEATURES]))\n",
    "    df_num.columns = NUMERICAL_FEATURES\n",
    "    new_df = pd.concat([df_cat.reset_index(drop=True), df_num.reset_index(drop=True)], axis=1)\n",
    "    new_df = fillna(new_df)\n",
    "    return new_df, oh_encoder, robust_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "def cross_validate_tune_hyperparameters(train, params, model_name='randomforest', callbacks=None):\n",
    "    auc_scores = []\n",
    "    for i, (train_index, valid_index) in enumerate(kfold.split(train, train[TARGET])):\n",
    "        print(f\"===========fold {i}================\")\n",
    "        X_train, oh_encoder, robust_scaler = process_data(train.iloc[train_index])\n",
    "        X_valid, _, _  = process_data(train.iloc[valid_index], oh_encoder,robust_scaler)\n",
    "        y_train = train.iloc[train_index][TARGET].values\n",
    "        y_valid = train.iloc[valid_index][TARGET].values\n",
    "        print(X_train.shape)\n",
    "        if model_name == \"rf\":\n",
    "            print(\"Random Forest--------------\")\n",
    "            rf_model = RandomForestClassifier(**params)\n",
    "            rf_model.fit(X_train, y_train)\n",
    "            rf_y_pred_proba = rf_model.predict_proba(X_valid)[:,1]\n",
    "            best_threshold, best_score = find_best_threshold_f1_score(y_valid, rf_y_pred_proba)\n",
    "            print(f\"\\n best_threshold {best_threshold} best_score {best_score}\")\n",
    "\n",
    "            auc_scores.append(roc_auc_score(y_valid, rf_y_pred_proba))\n",
    "        elif model_name == 'lgb':\n",
    "    #     models.append(model)\n",
    "            print(\"LGBModel--------------\")\n",
    "            lgb_model = LGBMClassifier(**params)\n",
    "            # callbacks = [lgb.early_stopping(200, verbose=50), lgb.log_evaluation(period=50)]\n",
    "            lgb_model.fit(X_train, y_train,\n",
    "                    eval_set=[(X_train, y_train), (X_valid, y_valid)],\n",
    "                    eval_metric=[\"logloss\", \"auc\"])\n",
    "\n",
    "            lgb_y_pred_proba = lgb_model.predict_proba(X_valid)[:,1]\n",
    "            auc_scores.append(roc_auc_score(y_valid, lgb_y_pred_proba))\n",
    "    #     models.append(model)\n",
    "        # display(pd.DataFrame({'score': lgb_model.feature_importances_, 'feature': lgb_model.feature_name_}).sort_values('score',ascending=False))\n",
    "        elif model_name =='xgb':\n",
    "            print(\"XGBoost--------------\")\n",
    "            xgb_model = XGBClassifier(**params)\n",
    "            xgb_model.fit(X_train, y_train,\n",
    "                    eval_set=[(X_train, y_train), (X_valid, y_valid)],\n",
    "                    verbose=50, callbacks=callbacks)\n",
    "            xgb_y_pred_proba = xgb_model.predict_proba(X_valid)[:,1]\n",
    "            auc_scores.append(roc_auc_score(y_valid, xgb_y_pred_proba))\n",
    "        elif model_name == 'mlp':        \n",
    "            print(\"MLP------------------\")\n",
    "            mlp_model = MLPClassifier(**params)\n",
    "            mlp_model.fit(X_train, y_train)\n",
    "            mlp_y_pred_proba = mlp_model.predict_proba(X_valid)[:,1]\n",
    "            auc_scores.append(roc_auc_score(y_valid, mlp_y_pred_proba))\n",
    "    return np.mean(auc_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "import sklearn.datasets\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "def objective(trial,train=None, model_name='rf'):\n",
    "\n",
    "    lgb_param = {\n",
    "            \"metric\": \"auc\",\n",
    "            \"n_estimators\": 200,\n",
    "            'learning_rate':0.03,\n",
    "            \"learning_rate\":trial.suggest_float(\"learning_rate\",0.01,0.1),\n",
    "            \"max_depth\":trial.suggest_int(\"max_depth\",1, 9),\n",
    "            'colsample_bytree':trial.suggest_discrete_uniform('colsample_bytree',0.1,1,0.1),\n",
    "            'subsample':trial.suggest_discrete_uniform('subsample',0.1,1,0.1),\n",
    "            \"lambda\": trial.suggest_float(\"lambda\", 1e-8, 1.0, log=True),\n",
    "            \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 1.0, log=True),\n",
    "            \"random_state\":42,\n",
    "            'device':'gpu',\n",
    "        #     \"class_weight\": \"balanced\"\n",
    "        }\n",
    "    xgb_param = {\n",
    "        \"verbosity\": 0,\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"eval_metric\": \"auc\",\n",
    "        \"n_estimators\":200,\n",
    "        \"learning_rate\":trial.suggest_float(\"learning_rate\",0.01,0.1),\n",
    "        \"max_depth\":trial.suggest_int(\"max_depth\",1, 9),\n",
    "        'colsample_bytree':trial.suggest_discrete_uniform('colsample_bytree',0.1,1,0.1),\n",
    "        'subsample':trial.suggest_discrete_uniform('subsample',0.1,1,0.1),\n",
    "        \"lambda\": trial.suggest_float(\"lambda\", 1e-8, 1.0, log=True),\n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 1.0, log=True),\n",
    "        'tree_method':'gpu_hist',\n",
    "        'seed':42,\n",
    "\n",
    "    }\n",
    "    rf_params = {\n",
    "        'n_estimators':200,\n",
    "        \"max_depth\":trial.suggest_int(\"max_depth\",1, 9),\n",
    "        'max_features':trial.suggest_discrete_uniform('colsample_bytree',0.1,1,0.1),\n",
    "    }\n",
    "    mlp_params = {'hidden_layer_sizes':(trial.suggest_int(\"hidden_layer_sizes\",2, 5),125), 'random_state':42, 'max_iter':100}\n",
    "    if model_name == 'rf':\n",
    "        mean_auc = cross_validate_tune_hyperparameters( train=train, params=rf_params, model_name=model_name)\n",
    "\n",
    "    elif model_name == 'xgb':\n",
    "        pruning_callback = optuna.integration.XGBoostPruningCallback(trial, \"validation_1-auc\")\n",
    "        mean_auc = cross_validate_tune_hyperparameters( train=train, params=xgb_param, model_name=model_name, callbacks=[pruning_callback])\n",
    "    elif model_name =='lgb':\n",
    "        pruning_callback = optuna.integration.LightGBMPruningCallback(trial, \"validation_1-auc\")\n",
    "        mean_auc = cross_validate_tune_hyperparameters( train=train, params=lgb_param, model_name=model_name, callbacks=[pruning_callback])\n",
    "    elif model_name == 'mlp':\n",
    "        mean_auc = cross_validate_tune_hyperparameters( train=train, params=mlp_params, model_name=model_name)\n",
    "    return mean_auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_trials = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 17:17:40,606] A new study created in memory with name: xgb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-auc:0.82962\tvalidation_1-auc:0.82910\n",
      "[50]\tvalidation_0-auc:0.88691\tvalidation_1-auc:0.88655\n",
      "[100]\tvalidation_0-auc:0.89097\tvalidation_1-auc:0.89060\n",
      "[150]\tvalidation_0-auc:0.89552\tvalidation_1-auc:0.89508\n",
      "[199]\tvalidation_0-auc:0.89749\tvalidation_1-auc:0.89701\n",
      "===========fold 1================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-auc:0.82929\tvalidation_1-auc:0.82976\n",
      "[50]\tvalidation_0-auc:0.88681\tvalidation_1-auc:0.88682\n",
      "[100]\tvalidation_0-auc:0.89057\tvalidation_1-auc:0.89050\n",
      "[150]\tvalidation_0-auc:0.89529\tvalidation_1-auc:0.89529\n",
      "[199]\tvalidation_0-auc:0.89716\tvalidation_1-auc:0.89717\n",
      "===========fold 2================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-auc:0.82943\tvalidation_1-auc:0.82949\n",
      "[50]\tvalidation_0-auc:0.88688\tvalidation_1-auc:0.88704\n",
      "[100]\tvalidation_0-auc:0.89097\tvalidation_1-auc:0.89120\n",
      "[150]\tvalidation_0-auc:0.89535\tvalidation_1-auc:0.89552\n",
      "[199]\tvalidation_0-auc:0.89723\tvalidation_1-auc:0.89742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 17:18:09,279] Trial 0 finished with value: 0.8972004564287293 and parameters: {'learning_rate': 0.020203586153278344, 'max_depth': 2, 'colsample_bytree': 0.8, 'subsample': 1.0, 'lambda': 0.00020859458052474336, 'alpha': 3.989162019829639e-06, 'hidden_layer_sizes': 5}. Best is trial 0 with value: 0.8972004564287293.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-auc:0.76312\tvalidation_1-auc:0.76305\n",
      "[50]\tvalidation_0-auc:0.88839\tvalidation_1-auc:0.88796\n",
      "[100]\tvalidation_0-auc:0.89164\tvalidation_1-auc:0.89129\n",
      "[150]\tvalidation_0-auc:0.89241\tvalidation_1-auc:0.89206\n",
      "[199]\tvalidation_0-auc:0.89303\tvalidation_1-auc:0.89263\n",
      "===========fold 1================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-auc:0.76273\tvalidation_1-auc:0.76382\n",
      "[50]\tvalidation_0-auc:0.88834\tvalidation_1-auc:0.88815\n",
      "[100]\tvalidation_0-auc:0.89166\tvalidation_1-auc:0.89141\n",
      "[150]\tvalidation_0-auc:0.89235\tvalidation_1-auc:0.89211\n",
      "[199]\tvalidation_0-auc:0.89292\tvalidation_1-auc:0.89268\n",
      "===========fold 2================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-auc:0.76343\tvalidation_1-auc:0.76241\n",
      "[50]\tvalidation_0-auc:0.88832\tvalidation_1-auc:0.88880\n",
      "[100]\tvalidation_0-auc:0.89140\tvalidation_1-auc:0.89187\n",
      "[150]\tvalidation_0-auc:0.89213\tvalidation_1-auc:0.89257\n",
      "[199]\tvalidation_0-auc:0.89272\tvalidation_1-auc:0.89313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 17:18:35,639] Trial 1 finished with value: 0.8928140591704867 and parameters: {'learning_rate': 0.08831108534182419, 'max_depth': 1, 'colsample_bytree': 0.6, 'subsample': 1.0, 'lambda': 1.3429382570873412e-07, 'alpha': 0.08317671635416692, 'hidden_layer_sizes': 5}. Best is trial 0 with value: 0.8972004564287293.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-auc:0.82962\tvalidation_1-auc:0.82910\n",
      "[50]\tvalidation_0-auc:0.89696\tvalidation_1-auc:0.89660\n",
      "[100]\tvalidation_0-auc:0.90022\tvalidation_1-auc:0.89972\n",
      "[150]\tvalidation_0-auc:0.90135\tvalidation_1-auc:0.90079\n",
      "[199]\tvalidation_0-auc:0.90208\tvalidation_1-auc:0.90145\n",
      "===========fold 1================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-auc:0.82929\tvalidation_1-auc:0.82976\n",
      "[50]\tvalidation_0-auc:0.89734\tvalidation_1-auc:0.89724\n",
      "[100]\tvalidation_0-auc:0.90029\tvalidation_1-auc:0.90027\n",
      "[150]\tvalidation_0-auc:0.90153\tvalidation_1-auc:0.90151\n",
      "[199]\tvalidation_0-auc:0.90214\tvalidation_1-auc:0.90209\n",
      "===========fold 2================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-auc:0.82943\tvalidation_1-auc:0.82949\n",
      "[50]\tvalidation_0-auc:0.89715\tvalidation_1-auc:0.89731\n",
      "[100]\tvalidation_0-auc:0.90034\tvalidation_1-auc:0.90041\n",
      "[150]\tvalidation_0-auc:0.90150\tvalidation_1-auc:0.90154\n",
      "[199]\tvalidation_0-auc:0.90221\tvalidation_1-auc:0.90221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 17:19:04,384] Trial 2 finished with value: 0.9019160970585496 and parameters: {'learning_rate': 0.08160968782407758, 'max_depth': 2, 'colsample_bytree': 0.7000000000000001, 'subsample': 0.8, 'lambda': 0.33753308943226945, 'alpha': 3.806887626348064e-06, 'hidden_layer_sizes': 3}. Best is trial 2 with value: 0.9019160970585496.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-auc:0.88804\tvalidation_1-auc:0.88742\n",
      "[50]\tvalidation_0-auc:0.90244\tvalidation_1-auc:0.90176\n",
      "[100]\tvalidation_0-auc:0.90459\tvalidation_1-auc:0.90372\n",
      "[150]\tvalidation_0-auc:0.90553\tvalidation_1-auc:0.90436\n",
      "[199]\tvalidation_0-auc:0.90602\tvalidation_1-auc:0.90458\n",
      "===========fold 1================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-auc:0.88637\tvalidation_1-auc:0.88669\n",
      "[50]\tvalidation_0-auc:0.90210\tvalidation_1-auc:0.90216\n",
      "[100]\tvalidation_0-auc:0.90423\tvalidation_1-auc:0.90412\n",
      "[150]\tvalidation_0-auc:0.90515\tvalidation_1-auc:0.90477\n",
      "[199]\tvalidation_0-auc:0.90573\tvalidation_1-auc:0.90507\n",
      "===========fold 2================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-auc:0.88640\tvalidation_1-auc:0.88611\n",
      "[50]\tvalidation_0-auc:0.90223\tvalidation_1-auc:0.90206\n",
      "[100]\tvalidation_0-auc:0.90436\tvalidation_1-auc:0.90398\n",
      "[150]\tvalidation_0-auc:0.90527\tvalidation_1-auc:0.90463\n",
      "[199]\tvalidation_0-auc:0.90583\tvalidation_1-auc:0.90494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 17:19:36,215] Trial 3 finished with value: 0.9048643188052465 and parameters: {'learning_rate': 0.06604275774209466, 'max_depth': 4, 'colsample_bytree': 0.9, 'subsample': 0.4, 'lambda': 0.0004428192426028734, 'alpha': 1.7091452830761304e-07, 'hidden_layer_sizes': 4}. Best is trial 3 with value: 0.9048643188052465.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-auc:0.87884\tvalidation_1-auc:0.87531\n",
      "[50]\tvalidation_0-auc:0.90046\tvalidation_1-auc:0.89681\n",
      "[100]\tvalidation_0-auc:0.90645\tvalidation_1-auc:0.90095\n",
      "[150]\tvalidation_0-auc:0.91036\tvalidation_1-auc:0.90344\n",
      "[199]\tvalidation_0-auc:0.91296\tvalidation_1-auc:0.90462\n",
      "===========fold 1================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-auc:0.87868\tvalidation_1-auc:0.87571\n",
      "[50]\tvalidation_0-auc:0.90032\tvalidation_1-auc:0.89663\n",
      "[100]\tvalidation_0-auc:0.90623\tvalidation_1-auc:0.90098\n",
      "[150]\tvalidation_0-auc:0.91007\tvalidation_1-auc:0.90369\n",
      "[199]\tvalidation_0-auc:0.91263\tvalidation_1-auc:0.90495\n",
      "===========fold 2================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-auc:0.87841\tvalidation_1-auc:0.87542\n",
      "[50]\tvalidation_0-auc:0.90032\tvalidation_1-auc:0.89734\n",
      "[100]\tvalidation_0-auc:0.90621\tvalidation_1-auc:0.90159\n",
      "[150]\tvalidation_0-auc:0.91012\tvalidation_1-auc:0.90408\n",
      "[199]\tvalidation_0-auc:0.91275\tvalidation_1-auc:0.90519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 17:20:24,169] Trial 4 finished with value: 0.9049171026839326 and parameters: {'learning_rate': 0.03614232389858331, 'max_depth': 9, 'colsample_bytree': 0.30000000000000004, 'subsample': 0.4, 'lambda': 0.0002067492786678936, 'alpha': 0.0053733256886313325, 'hidden_layer_sizes': 2}. Best is trial 4 with value: 0.9049171026839326.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-auc:0.89890\tvalidation_1-auc:0.89775\n",
      "[50]\tvalidation_0-auc:0.90797\tvalidation_1-auc:0.90538\n",
      "[100]\tvalidation_0-auc:0.91028\tvalidation_1-auc:0.90578\n",
      "[150]\tvalidation_0-auc:0.91203\tvalidation_1-auc:0.90584\n",
      "[199]\tvalidation_0-auc:0.91355\tvalidation_1-auc:0.90582\n",
      "===========fold 1================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-auc:0.89796\tvalidation_1-auc:0.89760\n",
      "[50]\tvalidation_0-auc:0.90776\tvalidation_1-auc:0.90580\n",
      "[100]\tvalidation_0-auc:0.91005\tvalidation_1-auc:0.90624\n",
      "[150]\tvalidation_0-auc:0.91183\tvalidation_1-auc:0.90625\n",
      "[199]\tvalidation_0-auc:0.91335\tvalidation_1-auc:0.90618\n",
      "===========fold 2================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-auc:0.89830\tvalidation_1-auc:0.89724\n",
      "[50]\tvalidation_0-auc:0.90792\tvalidation_1-auc:0.90555\n",
      "[100]\tvalidation_0-auc:0.91010\tvalidation_1-auc:0.90599\n",
      "[150]\tvalidation_0-auc:0.91191\tvalidation_1-auc:0.90607\n",
      "[199]\tvalidation_0-auc:0.91344\tvalidation_1-auc:0.90603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 17:21:01,599] Trial 5 finished with value: 0.9060082607606953 and parameters: {'learning_rate': 0.08232985054316262, 'max_depth': 7, 'colsample_bytree': 1.0, 'subsample': 0.5, 'lambda': 0.004071848931439218, 'alpha': 0.016597124424722224, 'hidden_layer_sizes': 5}. Best is trial 5 with value: 0.9060082607606953.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-auc:0.87961\tvalidation_1-auc:0.87918\n",
      "[50]\tvalidation_0-auc:0.90167\tvalidation_1-auc:0.90102\n",
      "[100]\tvalidation_0-auc:0.90414\tvalidation_1-auc:0.90332\n",
      "[150]\tvalidation_0-auc:0.90515\tvalidation_1-auc:0.90408\n",
      "[199]\tvalidation_0-auc:0.90574\tvalidation_1-auc:0.90444\n",
      "===========fold 1================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-auc:0.87795\tvalidation_1-auc:0.87827\n",
      "[50]\tvalidation_0-auc:0.90124\tvalidation_1-auc:0.90124\n",
      "[100]\tvalidation_0-auc:0.90375\tvalidation_1-auc:0.90368\n",
      "[150]\tvalidation_0-auc:0.90485\tvalidation_1-auc:0.90458\n",
      "[199]\tvalidation_0-auc:0.90542\tvalidation_1-auc:0.90492\n",
      "===========fold 2================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-auc:0.87724\tvalidation_1-auc:0.87703\n",
      "[50]\tvalidation_0-auc:0.90143\tvalidation_1-auc:0.90132\n",
      "[100]\tvalidation_0-auc:0.90389\tvalidation_1-auc:0.90361\n",
      "[150]\tvalidation_0-auc:0.90492\tvalidation_1-auc:0.90441\n",
      "[199]\tvalidation_0-auc:0.90552\tvalidation_1-auc:0.90481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 17:21:34,245] Trial 6 finished with value: 0.9047264867861949 and parameters: {'learning_rate': 0.057234384519163434, 'max_depth': 4, 'colsample_bytree': 0.8, 'subsample': 0.5, 'lambda': 0.24162574417337968, 'alpha': 2.891410235502141e-06, 'hidden_layer_sizes': 5}. Best is trial 5 with value: 0.9060082607606953.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-auc:0.77525\tvalidation_1-auc:0.77576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 17:21:39,501] Trial 7 pruned. Trial was pruned at iteration 5.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-auc:0.86472\tvalidation_1-auc:0.86440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 17:21:44,931] Trial 8 pruned. Trial was pruned at iteration 5.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-auc:0.80670\tvalidation_1-auc:0.80584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 17:21:50,391] Trial 9 pruned. Trial was pruned at iteration 5.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-auc:0.89564\tvalidation_1-auc:0.89415\n",
      "[50]\tvalidation_0-auc:0.90727\tvalidation_1-auc:0.90452\n",
      "[100]\tvalidation_0-auc:0.90898\tvalidation_1-auc:0.90538\n",
      "[150]\tvalidation_0-auc:0.91008\tvalidation_1-auc:0.90549\n",
      "[199]\tvalidation_0-auc:0.91102\tvalidation_1-auc:0.90545\n",
      "===========fold 1================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-auc:0.89466\tvalidation_1-auc:0.89414\n",
      "[50]\tvalidation_0-auc:0.90709\tvalidation_1-auc:0.90506\n",
      "[100]\tvalidation_0-auc:0.90883\tvalidation_1-auc:0.90575\n",
      "[150]\tvalidation_0-auc:0.90982\tvalidation_1-auc:0.90580\n",
      "[199]\tvalidation_0-auc:0.91067\tvalidation_1-auc:0.90570\n",
      "===========fold 2================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-auc:0.89629\tvalidation_1-auc:0.89544\n",
      "[50]\tvalidation_0-auc:0.90710\tvalidation_1-auc:0.90476\n",
      "[100]\tvalidation_0-auc:0.90885\tvalidation_1-auc:0.90559\n",
      "[150]\tvalidation_0-auc:0.90989\tvalidation_1-auc:0.90569\n",
      "[199]\tvalidation_0-auc:0.91087\tvalidation_1-auc:0.90566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 17:22:32,095] Trial 10 finished with value: 0.9056057535194135 and parameters: {'learning_rate': 0.044020550894558794, 'max_depth': 8, 'colsample_bytree': 1.0, 'subsample': 0.1, 'lambda': 0.007243916586851512, 'alpha': 0.6017739617622451, 'hidden_layer_sizes': 3}. Best is trial 5 with value: 0.9060082607606953.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-auc:0.89568\tvalidation_1-auc:0.89419\n",
      "[50]\tvalidation_0-auc:0.90707\tvalidation_1-auc:0.90443\n",
      "[100]\tvalidation_0-auc:0.90889\tvalidation_1-auc:0.90534\n",
      "[150]\tvalidation_0-auc:0.90991\tvalidation_1-auc:0.90552\n",
      "[199]\tvalidation_0-auc:0.91087\tvalidation_1-auc:0.90547\n",
      "===========fold 1================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-auc:0.89466\tvalidation_1-auc:0.89413\n",
      "[50]\tvalidation_0-auc:0.90688\tvalidation_1-auc:0.90505\n",
      "[100]\tvalidation_0-auc:0.90863\tvalidation_1-auc:0.90584\n",
      "[150]\tvalidation_0-auc:0.90963\tvalidation_1-auc:0.90595\n",
      "[199]\tvalidation_0-auc:0.91053\tvalidation_1-auc:0.90587\n",
      "===========fold 2================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-auc:0.89649\tvalidation_1-auc:0.89567\n",
      "[50]\tvalidation_0-auc:0.90697\tvalidation_1-auc:0.90480\n",
      "[100]\tvalidation_0-auc:0.90879\tvalidation_1-auc:0.90574\n",
      "[150]\tvalidation_0-auc:0.90983\tvalidation_1-auc:0.90584\n",
      "[199]\tvalidation_0-auc:0.91076\tvalidation_1-auc:0.90579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 17:23:13,972] Trial 11 finished with value: 0.9057102285303165 and parameters: {'learning_rate': 0.04156946990199564, 'max_depth': 8, 'colsample_bytree': 1.0, 'subsample': 0.1, 'lambda': 0.010955725852852262, 'alpha': 0.7443201169550195, 'hidden_layer_sizes': 3}. Best is trial 5 with value: 0.9060082607606953.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-auc:0.89925\tvalidation_1-auc:0.89818\n",
      "[50]\tvalidation_0-auc:0.90548\tvalidation_1-auc:0.90387\n",
      "[100]\tvalidation_0-auc:0.90738\tvalidation_1-auc:0.90509\n",
      "[150]\tvalidation_0-auc:0.90859\tvalidation_1-auc:0.90566\n",
      "[199]\tvalidation_0-auc:0.90960\tvalidation_1-auc:0.90591\n",
      "===========fold 1================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-auc:0.89829\tvalidation_1-auc:0.89784\n",
      "[50]\tvalidation_0-auc:0.90536\tvalidation_1-auc:0.90435\n",
      "[100]\tvalidation_0-auc:0.90719\tvalidation_1-auc:0.90551\n",
      "[150]\tvalidation_0-auc:0.90843\tvalidation_1-auc:0.90604\n",
      "[199]\tvalidation_0-auc:0.90942\tvalidation_1-auc:0.90624\n",
      "===========fold 2================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-auc:0.89875\tvalidation_1-auc:0.89771\n",
      "[50]\tvalidation_0-auc:0.90543\tvalidation_1-auc:0.90402\n",
      "[100]\tvalidation_0-auc:0.90729\tvalidation_1-auc:0.90533\n",
      "[150]\tvalidation_0-auc:0.90848\tvalidation_1-auc:0.90588\n",
      "[199]\tvalidation_0-auc:0.90948\tvalidation_1-auc:0.90612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 17:23:54,449] Trial 12 finished with value: 0.9060867632857025 and parameters: {'learning_rate': 0.033080903273603315, 'max_depth': 7, 'colsample_bytree': 1.0, 'subsample': 0.7000000000000001, 'lambda': 0.0042206466190608475, 'alpha': 0.022708859508907345, 'hidden_layer_sizes': 2}. Best is trial 12 with value: 0.9060867632857025.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-auc:0.89357\tvalidation_1-auc:0.89254\n",
      "[50]\tvalidation_0-auc:0.90349\tvalidation_1-auc:0.90224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 17:24:02,541] Trial 13 pruned. Trial was pruned at iteration 74.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-auc:0.88683\tvalidation_1-auc:0.88589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 17:24:08,946] Trial 14 pruned. Trial was pruned at iteration 34.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-auc:0.89661\tvalidation_1-auc:0.89578\n",
      "[50]\tvalidation_0-auc:0.90505\tvalidation_1-auc:0.90386\n",
      "[100]\tvalidation_0-auc:0.90695\tvalidation_1-auc:0.90509\n",
      "[150]\tvalidation_0-auc:0.90818\tvalidation_1-auc:0.90548\n",
      "[199]\tvalidation_0-auc:0.90909\tvalidation_1-auc:0.90566\n",
      "===========fold 1================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-auc:0.89559\tvalidation_1-auc:0.89575\n",
      "[50]\tvalidation_0-auc:0.90492\tvalidation_1-auc:0.90433\n",
      "[100]\tvalidation_0-auc:0.90676\tvalidation_1-auc:0.90549\n",
      "[150]\tvalidation_0-auc:0.90796\tvalidation_1-auc:0.90585\n",
      "[199]\tvalidation_0-auc:0.90892\tvalidation_1-auc:0.90604\n",
      "===========fold 2================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-auc:0.89611\tvalidation_1-auc:0.89555\n",
      "[50]\tvalidation_0-auc:0.90495\tvalidation_1-auc:0.90403\n",
      "[100]\tvalidation_0-auc:0.90687\tvalidation_1-auc:0.90536\n",
      "[150]\tvalidation_0-auc:0.90807\tvalidation_1-auc:0.90575\n",
      "[199]\tvalidation_0-auc:0.90896\tvalidation_1-auc:0.90592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 17:24:45,424] Trial 15 finished with value: 0.9058755946356939 and parameters: {'learning_rate': 0.05563616293142673, 'max_depth': 6, 'colsample_bytree': 0.9, 'subsample': 0.7000000000000001, 'lambda': 0.029532268433653395, 'alpha': 0.0008576368294088292, 'hidden_layer_sizes': 2}. Best is trial 12 with value: 0.9060867632857025.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-auc:0.89042\tvalidation_1-auc:0.88725\n",
      "[50]\tvalidation_0-auc:0.90583\tvalidation_1-auc:0.90217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 17:24:53,809] Trial 16 pruned. Trial was pruned at iteration 61.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-auc:0.81405\tvalidation_1-auc:0.81253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 17:24:59,185] Trial 17 pruned. Trial was pruned at iteration 5.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-auc:0.89632\tvalidation_1-auc:0.89558\n",
      "[50]\tvalidation_0-auc:0.90577\tvalidation_1-auc:0.90444\n",
      "[100]\tvalidation_0-auc:0.90764\tvalidation_1-auc:0.90538\n",
      "[150]\tvalidation_0-auc:0.90898\tvalidation_1-auc:0.90567\n",
      "[199]\tvalidation_0-auc:0.91002\tvalidation_1-auc:0.90582\n",
      "===========fold 1================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-auc:0.89601\tvalidation_1-auc:0.89615\n",
      "[50]\tvalidation_0-auc:0.90558\tvalidation_1-auc:0.90485\n",
      "[100]\tvalidation_0-auc:0.90746\tvalidation_1-auc:0.90582\n",
      "[150]\tvalidation_0-auc:0.90875\tvalidation_1-auc:0.90606\n",
      "[199]\tvalidation_0-auc:0.90990\tvalidation_1-auc:0.90622\n",
      "===========fold 2================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-auc:0.89604\tvalidation_1-auc:0.89542\n",
      "[50]\tvalidation_0-auc:0.90569\tvalidation_1-auc:0.90461\n",
      "[100]\tvalidation_0-auc:0.90754\tvalidation_1-auc:0.90553\n",
      "[150]\tvalidation_0-auc:0.90879\tvalidation_1-auc:0.90580\n",
      "[199]\tvalidation_0-auc:0.90986\tvalidation_1-auc:0.90594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 17:25:34,844] Trial 18 finished with value: 0.9059946669699706 and parameters: {'learning_rate': 0.07058678858594442, 'max_depth': 6, 'colsample_bytree': 0.9, 'subsample': 0.6, 'lambda': 1.71291568796439e-08, 'alpha': 1.1864241899545296e-08, 'hidden_layer_sizes': 2}. Best is trial 12 with value: 0.9060867632857025.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-auc:0.89297\tvalidation_1-auc:0.89175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 17:25:41,074] Trial 19 pruned. Trial was pruned at iteration 29.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-auc:0.87786\tvalidation_1-auc:0.87614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 17:25:46,612] Trial 20 pruned. Trial was pruned at iteration 5.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-auc:0.89632\tvalidation_1-auc:0.89558\n",
      "[50]\tvalidation_0-auc:0.90584\tvalidation_1-auc:0.90450\n",
      "[100]\tvalidation_0-auc:0.90773\tvalidation_1-auc:0.90539\n",
      "[150]\tvalidation_0-auc:0.90905\tvalidation_1-auc:0.90564\n",
      "[199]\tvalidation_0-auc:0.91012\tvalidation_1-auc:0.90577\n",
      "===========fold 1================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-auc:0.89601\tvalidation_1-auc:0.89615\n",
      "[50]\tvalidation_0-auc:0.90562\tvalidation_1-auc:0.90488\n",
      "[100]\tvalidation_0-auc:0.90750\tvalidation_1-auc:0.90582\n",
      "[150]\tvalidation_0-auc:0.90882\tvalidation_1-auc:0.90607\n",
      "[199]\tvalidation_0-auc:0.90991\tvalidation_1-auc:0.90617\n",
      "===========fold 2================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-auc:0.89604\tvalidation_1-auc:0.89542\n",
      "[50]\tvalidation_0-auc:0.90578\tvalidation_1-auc:0.90466\n",
      "[100]\tvalidation_0-auc:0.90761\tvalidation_1-auc:0.90555\n",
      "[150]\tvalidation_0-auc:0.90883\tvalidation_1-auc:0.90580\n",
      "[199]\tvalidation_0-auc:0.90988\tvalidation_1-auc:0.90594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 17:26:22,655] Trial 21 finished with value: 0.905959600060382 and parameters: {'learning_rate': 0.0719217345004031, 'max_depth': 6, 'colsample_bytree': 0.9, 'subsample': 0.6, 'lambda': 3.357680317616199e-08, 'alpha': 2.6148483345462426e-08, 'hidden_layer_sizes': 2}. Best is trial 12 with value: 0.9060867632857025.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-auc:0.89298\tvalidation_1-auc:0.89228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 17:26:27,886] Trial 22 pruned. Trial was pruned at iteration 5.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-auc:0.89293\tvalidation_1-auc:0.89230\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 17:26:33,279] Trial 23 pruned. Trial was pruned at iteration 5.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-auc:0.89888\tvalidation_1-auc:0.89774\n",
      "[50]\tvalidation_0-auc:0.90767\tvalidation_1-auc:0.90520\n",
      "[100]\tvalidation_0-auc:0.90989\tvalidation_1-auc:0.90581\n",
      "[150]\tvalidation_0-auc:0.91154\tvalidation_1-auc:0.90594\n",
      "[199]\tvalidation_0-auc:0.91291\tvalidation_1-auc:0.90593\n",
      "===========fold 1================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-auc:0.89796\tvalidation_1-auc:0.89760\n",
      "[50]\tvalidation_0-auc:0.90753\tvalidation_1-auc:0.90570\n",
      "[100]\tvalidation_0-auc:0.90968\tvalidation_1-auc:0.90623\n",
      "[150]\tvalidation_0-auc:0.91134\tvalidation_1-auc:0.90628\n",
      "[199]\tvalidation_0-auc:0.91276\tvalidation_1-auc:0.90625\n",
      "===========fold 2================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-auc:0.89830\tvalidation_1-auc:0.89724\n",
      "[50]\tvalidation_0-auc:0.90762\tvalidation_1-auc:0.90538\n",
      "[100]\tvalidation_0-auc:0.90974\tvalidation_1-auc:0.90594\n",
      "[150]\tvalidation_0-auc:0.91140\tvalidation_1-auc:0.90604\n",
      "[199]\tvalidation_0-auc:0.91288\tvalidation_1-auc:0.90606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 17:27:11,242] Trial 24 finished with value: 0.9060800283471071 and parameters: {'learning_rate': 0.07356863389187984, 'max_depth': 7, 'colsample_bytree': 1.0, 'subsample': 0.5, 'lambda': 1.2386776481369823e-08, 'alpha': 0.00021933746465387416, 'hidden_layer_sizes': 2}. Best is trial 12 with value: 0.9060867632857025.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-auc:0.89774\tvalidation_1-auc:0.89692\n",
      "[50]\tvalidation_0-auc:0.90779\tvalidation_1-auc:0.90522\n",
      "[100]\tvalidation_0-auc:0.90972\tvalidation_1-auc:0.90564\n",
      "[150]\tvalidation_0-auc:0.91107\tvalidation_1-auc:0.90565\n",
      "[199]\tvalidation_0-auc:0.91240\tvalidation_1-auc:0.90560\n",
      "===========fold 1================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-auc:0.89741\tvalidation_1-auc:0.89705\n",
      "[50]\tvalidation_0-auc:0.90752\tvalidation_1-auc:0.90558\n",
      "[100]\tvalidation_0-auc:0.90931\tvalidation_1-auc:0.90600\n",
      "[150]\tvalidation_0-auc:0.91079\tvalidation_1-auc:0.90599\n",
      "[199]\tvalidation_0-auc:0.91207\tvalidation_1-auc:0.90589\n",
      "===========fold 2================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-auc:0.89742\tvalidation_1-auc:0.89623\n",
      "[50]\tvalidation_0-auc:0.90768\tvalidation_1-auc:0.90550\n",
      "[100]\tvalidation_0-auc:0.90958\tvalidation_1-auc:0.90595\n",
      "[150]\tvalidation_0-auc:0.91100\tvalidation_1-auc:0.90596\n",
      "[199]\tvalidation_0-auc:0.91235\tvalidation_1-auc:0.90588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 17:27:48,399] Trial 25 finished with value: 0.9057899680655873 and parameters: {'learning_rate': 0.07656945689575372, 'max_depth': 7, 'colsample_bytree': 1.0, 'subsample': 0.30000000000000004, 'lambda': 0.002085548528957363, 'alpha': 0.00012960724372994764, 'hidden_layer_sizes': 3}. Best is trial 12 with value: 0.9060867632857025.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-auc:0.89540\tvalidation_1-auc:0.89358\n",
      "[50]\tvalidation_0-auc:0.90979\tvalidation_1-auc:0.90542\n",
      "[100]\tvalidation_0-auc:0.91302\tvalidation_1-auc:0.90570\n",
      "[150]\tvalidation_0-auc:0.91548\tvalidation_1-auc:0.90569\n",
      "[199]\tvalidation_0-auc:0.91775\tvalidation_1-auc:0.90560\n",
      "===========fold 1================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-auc:0.89504\tvalidation_1-auc:0.89387\n",
      "[50]\tvalidation_0-auc:0.90971\tvalidation_1-auc:0.90591\n",
      "[100]\tvalidation_0-auc:0.91270\tvalidation_1-auc:0.90620\n",
      "[150]\tvalidation_0-auc:0.91502\tvalidation_1-auc:0.90620\n",
      "[199]\tvalidation_0-auc:0.91727\tvalidation_1-auc:0.90598\n",
      "===========fold 2================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-auc:0.89473\tvalidation_1-auc:0.89357\n",
      "[50]\tvalidation_0-auc:0.90987\tvalidation_1-auc:0.90584\n",
      "[100]\tvalidation_0-auc:0.91293\tvalidation_1-auc:0.90615\n",
      "[150]\tvalidation_0-auc:0.91533\tvalidation_1-auc:0.90608\n",
      "[199]\tvalidation_0-auc:0.91754\tvalidation_1-auc:0.90595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 17:28:29,073] Trial 26 finished with value: 0.9058432091593586 and parameters: {'learning_rate': 0.0897361720888959, 'max_depth': 8, 'colsample_bytree': 0.7000000000000001, 'subsample': 0.5, 'lambda': 0.07000357255103236, 'alpha': 0.018050929051355514, 'hidden_layer_sizes': 2}. Best is trial 12 with value: 0.9060867632857025.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-auc:0.89641\tvalidation_1-auc:0.89358\n",
      "[50]\tvalidation_0-auc:0.91024\tvalidation_1-auc:0.90527\n",
      "[100]\tvalidation_0-auc:0.91318\tvalidation_1-auc:0.90594\n",
      "[150]\tvalidation_0-auc:0.91535\tvalidation_1-auc:0.90607\n",
      "[199]\tvalidation_0-auc:0.91721\tvalidation_1-auc:0.90605\n",
      "===========fold 1================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-auc:0.89627\tvalidation_1-auc:0.89410\n",
      "[50]\tvalidation_0-auc:0.91005\tvalidation_1-auc:0.90562\n",
      "[100]\tvalidation_0-auc:0.91296\tvalidation_1-auc:0.90630\n",
      "[150]\tvalidation_0-auc:0.91512\tvalidation_1-auc:0.90641\n",
      "[199]\tvalidation_0-auc:0.91693\tvalidation_1-auc:0.90640\n",
      "===========fold 2================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-auc:0.89674\tvalidation_1-auc:0.89427\n",
      "[50]\tvalidation_0-auc:0.91013\tvalidation_1-auc:0.90551\n",
      "[100]\tvalidation_0-auc:0.91305\tvalidation_1-auc:0.90620\n",
      "[150]\tvalidation_0-auc:0.91511\tvalidation_1-auc:0.90633\n",
      "[199]\tvalidation_0-auc:0.91694\tvalidation_1-auc:0.90630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 17:29:15,731] Trial 27 finished with value: 0.9062466271915007 and parameters: {'learning_rate': 0.04882946468495032, 'max_depth': 9, 'colsample_bytree': 0.8, 'subsample': 0.4, 'lambda': 4.046484326167589e-06, 'alpha': 0.0012186132264861006, 'hidden_layer_sizes': 3}. Best is trial 27 with value: 0.9062466271915007.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-auc:0.89514\tvalidation_1-auc:0.89209\n",
      "[50]\tvalidation_0-auc:0.90993\tvalidation_1-auc:0.90514\n",
      "[100]\tvalidation_0-auc:0.91263\tvalidation_1-auc:0.90574\n",
      "[150]\tvalidation_0-auc:0.91460\tvalidation_1-auc:0.90588\n",
      "[199]\tvalidation_0-auc:0.91621\tvalidation_1-auc:0.90586\n",
      "===========fold 1================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-auc:0.89507\tvalidation_1-auc:0.89281\n",
      "[50]\tvalidation_0-auc:0.90977\tvalidation_1-auc:0.90545\n",
      "[100]\tvalidation_0-auc:0.91236\tvalidation_1-auc:0.90614\n",
      "[150]\tvalidation_0-auc:0.91435\tvalidation_1-auc:0.90627\n",
      "[199]\tvalidation_0-auc:0.91589\tvalidation_1-auc:0.90630\n",
      "===========fold 2================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-auc:0.89517\tvalidation_1-auc:0.89220\n",
      "[50]\tvalidation_0-auc:0.90985\tvalidation_1-auc:0.90545\n",
      "[100]\tvalidation_0-auc:0.91250\tvalidation_1-auc:0.90610\n",
      "[150]\tvalidation_0-auc:0.91441\tvalidation_1-auc:0.90617\n",
      "[199]\tvalidation_0-auc:0.91606\tvalidation_1-auc:0.90612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 17:30:02,203] Trial 28 finished with value: 0.9060949171802202 and parameters: {'learning_rate': 0.048390232690592894, 'max_depth': 9, 'colsample_bytree': 0.8, 'subsample': 0.30000000000000004, 'lambda': 1.1999708289412384e-07, 'alpha': 0.00015114264081311082, 'hidden_layer_sizes': 3}. Best is trial 27 with value: 0.9062466271915007.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-auc:0.89514\tvalidation_1-auc:0.89209\n",
      "[50]\tvalidation_0-auc:0.90980\tvalidation_1-auc:0.90508\n",
      "[100]\tvalidation_0-auc:0.91247\tvalidation_1-auc:0.90574\n",
      "[150]\tvalidation_0-auc:0.91442\tvalidation_1-auc:0.90594\n",
      "[199]\tvalidation_0-auc:0.91608\tvalidation_1-auc:0.90592\n",
      "===========fold 1================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-auc:0.89507\tvalidation_1-auc:0.89281\n",
      "[50]\tvalidation_0-auc:0.90968\tvalidation_1-auc:0.90540\n",
      "[100]\tvalidation_0-auc:0.91229\tvalidation_1-auc:0.90610\n",
      "[150]\tvalidation_0-auc:0.91428\tvalidation_1-auc:0.90628\n",
      "[199]\tvalidation_0-auc:0.91582\tvalidation_1-auc:0.90625\n",
      "===========fold 2================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-auc:0.89517\tvalidation_1-auc:0.89220\n",
      "[50]\tvalidation_0-auc:0.90966\tvalidation_1-auc:0.90542\n",
      "[100]\tvalidation_0-auc:0.91231\tvalidation_1-auc:0.90610\n",
      "[150]\tvalidation_0-auc:0.91412\tvalidation_1-auc:0.90623\n",
      "[199]\tvalidation_0-auc:0.91583\tvalidation_1-auc:0.90618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 17:30:48,668] Trial 29 finished with value: 0.9061191247544769 and parameters: {'learning_rate': 0.04649462425436133, 'max_depth': 9, 'colsample_bytree': 0.8, 'subsample': 0.30000000000000004, 'lambda': 3.264512352502342e-07, 'alpha': 2.8523363099130964e-05, 'hidden_layer_sizes': 3}. Best is trial 27 with value: 0.9062466271915007.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-auc:0.89250\tvalidation_1-auc:0.88961\n",
      "[50]\tvalidation_0-auc:0.90933\tvalidation_1-auc:0.90497\n",
      "[100]\tvalidation_0-auc:0.91175\tvalidation_1-auc:0.90564\n",
      "[150]\tvalidation_0-auc:0.91331\tvalidation_1-auc:0.90574\n",
      "[199]\tvalidation_0-auc:0.91475\tvalidation_1-auc:0.90566\n",
      "===========fold 1================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-auc:0.89291\tvalidation_1-auc:0.89049\n",
      "[50]\tvalidation_0-auc:0.90914\tvalidation_1-auc:0.90524\n",
      "[100]\tvalidation_0-auc:0.91146\tvalidation_1-auc:0.90584\n",
      "[150]\tvalidation_0-auc:0.91304\tvalidation_1-auc:0.90597\n",
      "[199]\tvalidation_0-auc:0.91443\tvalidation_1-auc:0.90589\n",
      "===========fold 2================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-auc:0.89273\tvalidation_1-auc:0.88974\n",
      "[50]\tvalidation_0-auc:0.90919\tvalidation_1-auc:0.90526\n",
      "[100]\tvalidation_0-auc:0.91150\tvalidation_1-auc:0.90591\n",
      "[150]\tvalidation_0-auc:0.91314\tvalidation_1-auc:0.90597\n",
      "[199]\tvalidation_0-auc:0.91453\tvalidation_1-auc:0.90594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 17:31:34,573] Trial 30 finished with value: 0.9058313259265464 and parameters: {'learning_rate': 0.04609863337387689, 'max_depth': 9, 'colsample_bytree': 0.8, 'subsample': 0.2, 'lambda': 2.0493519357370048e-07, 'alpha': 3.163610555335946e-05, 'hidden_layer_sizes': 3}. Best is trial 27 with value: 0.9062466271915007.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-auc:0.89514\tvalidation_1-auc:0.89209\n",
      "[50]\tvalidation_0-auc:0.90920\tvalidation_1-auc:0.90494\n",
      "[100]\tvalidation_0-auc:0.91168\tvalidation_1-auc:0.90569\n",
      "[150]\tvalidation_0-auc:0.91348\tvalidation_1-auc:0.90596\n",
      "[199]\tvalidation_0-auc:0.91490\tvalidation_1-auc:0.90603\n",
      "===========fold 1================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-auc:0.89507\tvalidation_1-auc:0.89281\n",
      "[50]\tvalidation_0-auc:0.90896\tvalidation_1-auc:0.90517\n",
      "[100]\tvalidation_0-auc:0.91139\tvalidation_1-auc:0.90602\n",
      "[150]\tvalidation_0-auc:0.91318\tvalidation_1-auc:0.90638\n",
      "[199]\tvalidation_0-auc:0.91450\tvalidation_1-auc:0.90640\n",
      "===========fold 2================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-auc:0.89524\tvalidation_1-auc:0.89232\n",
      "[50]\tvalidation_0-auc:0.90909\tvalidation_1-auc:0.90514\n",
      "[100]\tvalidation_0-auc:0.91154\tvalidation_1-auc:0.90594\n",
      "[150]\tvalidation_0-auc:0.91320\tvalidation_1-auc:0.90624\n",
      "[199]\tvalidation_0-auc:0.91464\tvalidation_1-auc:0.90632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 17:32:22,140] Trial 31 finished with value: 0.9062492387505982 and parameters: {'learning_rate': 0.037173484372915955, 'max_depth': 9, 'colsample_bytree': 0.8, 'subsample': 0.30000000000000004, 'lambda': 2.6415733233649638e-06, 'alpha': 0.0015156315522086203, 'hidden_layer_sizes': 3}. Best is trial 31 with value: 0.9062492387505982.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-auc:0.89514\tvalidation_1-auc:0.89209\n",
      "[50]\tvalidation_0-auc:0.90999\tvalidation_1-auc:0.90512\n",
      "[100]\tvalidation_0-auc:0.91274\tvalidation_1-auc:0.90571\n",
      "[150]\tvalidation_0-auc:0.91475\tvalidation_1-auc:0.90583\n",
      "[199]\tvalidation_0-auc:0.91644\tvalidation_1-auc:0.90574\n",
      "===========fold 1================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-auc:0.89507\tvalidation_1-auc:0.89281\n",
      "[50]\tvalidation_0-auc:0.90982\tvalidation_1-auc:0.90542\n",
      "[100]\tvalidation_0-auc:0.91245\tvalidation_1-auc:0.90608\n",
      "[150]\tvalidation_0-auc:0.91442\tvalidation_1-auc:0.90622\n",
      "[199]\tvalidation_0-auc:0.91610\tvalidation_1-auc:0.90617\n",
      "===========fold 2================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-auc:0.89525\tvalidation_1-auc:0.89233\n",
      "[50]\tvalidation_0-auc:0.90992\tvalidation_1-auc:0.90544\n",
      "[100]\tvalidation_0-auc:0.91256\tvalidation_1-auc:0.90603\n",
      "[150]\tvalidation_0-auc:0.91450\tvalidation_1-auc:0.90615\n",
      "[199]\tvalidation_0-auc:0.91627\tvalidation_1-auc:0.90609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 17:33:08,558] Trial 32 finished with value: 0.9060037890108599 and parameters: {'learning_rate': 0.04957981846025007, 'max_depth': 9, 'colsample_bytree': 0.8, 'subsample': 0.30000000000000004, 'lambda': 2.477819868810429e-06, 'alpha': 0.001675477686171408, 'hidden_layer_sizes': 3}. Best is trial 31 with value: 0.9062492387505982.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-auc:0.89233\tvalidation_1-auc:0.89039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 17:33:14,154] Trial 33 pruned. Trial was pruned at iteration 6.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-auc:0.89000\tvalidation_1-auc:0.88790\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 17:33:19,656] Trial 34 pruned. Trial was pruned at iteration 5.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-auc:0.89641\tvalidation_1-auc:0.89358\n",
      "[50]\tvalidation_0-auc:0.91114\tvalidation_1-auc:0.90548\n",
      "[100]\tvalidation_0-auc:0.91447\tvalidation_1-auc:0.90591\n",
      "[150]\tvalidation_0-auc:0.91699\tvalidation_1-auc:0.90587\n",
      "[199]\tvalidation_0-auc:0.91913\tvalidation_1-auc:0.90576\n",
      "===========fold 1================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-auc:0.89628\tvalidation_1-auc:0.89411\n",
      "[50]\tvalidation_0-auc:0.91090\tvalidation_1-auc:0.90579\n",
      "[100]\tvalidation_0-auc:0.91403\tvalidation_1-auc:0.90623\n",
      "[150]\tvalidation_0-auc:0.91656\tvalidation_1-auc:0.90625\n",
      "[199]\tvalidation_0-auc:0.91890\tvalidation_1-auc:0.90616\n",
      "===========fold 2================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-auc:0.89674\tvalidation_1-auc:0.89426\n",
      "[50]\tvalidation_0-auc:0.91102\tvalidation_1-auc:0.90570\n",
      "[100]\tvalidation_0-auc:0.91416\tvalidation_1-auc:0.90619\n",
      "[150]\tvalidation_0-auc:0.91656\tvalidation_1-auc:0.90616\n",
      "[199]\tvalidation_0-auc:0.91881\tvalidation_1-auc:0.90607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 17:34:05,588] Trial 35 finished with value: 0.9059949628712252 and parameters: {'learning_rate': 0.0620885232707738, 'max_depth': 9, 'colsample_bytree': 0.8, 'subsample': 0.4, 'lambda': 8.56037028020915e-06, 'alpha': 8.392913687390024e-05, 'hidden_layer_sizes': 3}. Best is trial 31 with value: 0.9062492387505982.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-auc:0.89160\tvalidation_1-auc:0.88835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 17:34:10,926] Trial 36 pruned. Trial was pruned at iteration 5.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-auc:0.89219\tvalidation_1-auc:0.89082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 17:34:16,469] Trial 37 pruned. Trial was pruned at iteration 5.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-auc:0.89035\tvalidation_1-auc:0.88726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 17:34:22,299] Trial 38 pruned. Trial was pruned at iteration 5.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-auc:0.82962\tvalidation_1-auc:0.82910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 17:34:27,494] Trial 39 pruned. Trial was pruned at iteration 5.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-auc:0.89684\tvalidation_1-auc:0.89495\n",
      "[50]\tvalidation_0-auc:0.90850\tvalidation_1-auc:0.90505\n",
      "[100]\tvalidation_0-auc:0.91055\tvalidation_1-auc:0.90550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 17:34:37,926] Trial 40 pruned. Trial was pruned at iteration 124.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-auc:0.89516\tvalidation_1-auc:0.89210\n",
      "[50]\tvalidation_0-auc:0.90899\tvalidation_1-auc:0.90484\n",
      "[100]\tvalidation_0-auc:0.91135\tvalidation_1-auc:0.90566\n",
      "[150]\tvalidation_0-auc:0.91308\tvalidation_1-auc:0.90597\n",
      "[199]\tvalidation_0-auc:0.91448\tvalidation_1-auc:0.90605\n",
      "===========fold 1================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-auc:0.89507\tvalidation_1-auc:0.89281\n",
      "[50]\tvalidation_0-auc:0.90882\tvalidation_1-auc:0.90504\n",
      "[100]\tvalidation_0-auc:0.91112\tvalidation_1-auc:0.90591\n",
      "[150]\tvalidation_0-auc:0.91283\tvalidation_1-auc:0.90634\n",
      "[199]\tvalidation_0-auc:0.91413\tvalidation_1-auc:0.90644\n",
      "===========fold 2================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-auc:0.89526\tvalidation_1-auc:0.89235\n",
      "[50]\tvalidation_0-auc:0.90892\tvalidation_1-auc:0.90510\n",
      "[100]\tvalidation_0-auc:0.91128\tvalidation_1-auc:0.90592\n",
      "[150]\tvalidation_0-auc:0.91295\tvalidation_1-auc:0.90628\n",
      "[199]\tvalidation_0-auc:0.91428\tvalidation_1-auc:0.90635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 17:35:26,210] Trial 41 finished with value: 0.9062830425363898 and parameters: {'learning_rate': 0.034630277480196384, 'max_depth': 9, 'colsample_bytree': 0.8, 'subsample': 0.30000000000000004, 'lambda': 1.3270228907353322e-06, 'alpha': 0.0020136244579038245, 'hidden_layer_sizes': 3}. Best is trial 41 with value: 0.9062830425363898.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-auc:0.89514\tvalidation_1-auc:0.89209\n",
      "[50]\tvalidation_0-auc:0.90917\tvalidation_1-auc:0.90492\n",
      "[100]\tvalidation_0-auc:0.91160\tvalidation_1-auc:0.90569\n",
      "[150]\tvalidation_0-auc:0.91339\tvalidation_1-auc:0.90595\n",
      "[199]\tvalidation_0-auc:0.91480\tvalidation_1-auc:0.90598\n",
      "===========fold 1================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-auc:0.89507\tvalidation_1-auc:0.89281\n",
      "[50]\tvalidation_0-auc:0.90897\tvalidation_1-auc:0.90514\n",
      "[100]\tvalidation_0-auc:0.91138\tvalidation_1-auc:0.90602\n",
      "[150]\tvalidation_0-auc:0.91316\tvalidation_1-auc:0.90635\n",
      "[199]\tvalidation_0-auc:0.91452\tvalidation_1-auc:0.90639\n",
      "===========fold 2================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-auc:0.89526\tvalidation_1-auc:0.89234\n",
      "[50]\tvalidation_0-auc:0.90909\tvalidation_1-auc:0.90516\n",
      "[100]\tvalidation_0-auc:0.91153\tvalidation_1-auc:0.90596\n",
      "[150]\tvalidation_0-auc:0.91323\tvalidation_1-auc:0.90623\n",
      "[199]\tvalidation_0-auc:0.91471\tvalidation_1-auc:0.90627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 17:36:14,203] Trial 42 finished with value: 0.9062120949380533 and parameters: {'learning_rate': 0.037062324684941035, 'max_depth': 9, 'colsample_bytree': 0.8, 'subsample': 0.30000000000000004, 'lambda': 1.0171787514618927e-06, 'alpha': 0.0017208545597974087, 'hidden_layer_sizes': 3}. Best is trial 41 with value: 0.9062830425363898.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-auc:0.88646\tvalidation_1-auc:0.88428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 17:36:19,786] Trial 43 pruned. Trial was pruned at iteration 5.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-auc:0.90034\tvalidation_1-auc:0.89861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 17:36:25,619] Trial 44 pruned. Trial was pruned at iteration 11.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-auc:0.86769\tvalidation_1-auc:0.86697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 17:36:31,053] Trial 45 pruned. Trial was pruned at iteration 5.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-auc:0.89475\tvalidation_1-auc:0.89301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 17:36:37,273] Trial 46 pruned. Trial was pruned at iteration 20.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-auc:0.90110\tvalidation_1-auc:0.89815\n",
      "[50]\tvalidation_0-auc:0.90904\tvalidation_1-auc:0.90488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 17:36:45,918] Trial 47 pruned. Trial was pruned at iteration 56.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-auc:0.88852\tvalidation_1-auc:0.88622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 17:36:51,253] Trial 48 pruned. Trial was pruned at iteration 5.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-auc:0.89691\tvalidation_1-auc:0.89495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 17:36:57,696] Trial 49 pruned. Trial was pruned at iteration 20.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of finished trials: 50\n",
      "Best trial:\n",
      "  Value: 0.9062830425363898\n",
      "  Params: \n",
      "    learning_rate: 0.034630277480196384\n",
      "    max_depth: 9\n",
      "    colsample_bytree: 0.8\n",
      "    subsample: 0.30000000000000004\n",
      "    lambda: 1.3270228907353322e-06\n",
      "    alpha: 0.0020136244579038245\n",
      "    hidden_layer_sizes: 3\n"
     ]
    }
   ],
   "source": [
    "pruner = optuna.pruners.MedianPruner(n_warmup_steps=5)\n",
    "study = optuna.create_study(study_name='xgb',pruner=pruner, direction=\"maximize\")\n",
    "study.optimize(lambda trial: objective(trial, train=train, model_name='xgb'), n_trials=50)\n",
    "\n",
    "print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))\n",
    "all_trials['xgb'] = trial.params.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 17:36:57,793] A new study created in memory with name: lgb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.239404753906364e-07. Current value: lambda_l2=1.239404753906364e-07\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.239404753906364e-07. Current value: lambda_l2=1.239404753906364e-07\n",
      "[LightGBM] [Info] Number of positive: 162885, number of negative: 503781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7208\n",
      "[LightGBM] [Info] Number of data points in the train set: 666666, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.013522 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.239404753906364e-07. Current value: lambda_l2=1.239404753906364e-07\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244328 -> initscore=-1.129097\n",
      "[LightGBM] [Info] Start training from score -1.129097\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.239404753906364e-07. Current value: lambda_l2=1.239404753906364e-07\n",
      "===========fold 1================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.239404753906364e-07. Current value: lambda_l2=1.239404753906364e-07\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.239404753906364e-07. Current value: lambda_l2=1.239404753906364e-07\n",
      "[LightGBM] [Info] Number of positive: 162886, number of negative: 503781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7222\n",
      "[LightGBM] [Info] Number of data points in the train set: 666667, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.013136 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.239404753906364e-07. Current value: lambda_l2=1.239404753906364e-07\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244329 -> initscore=-1.129091\n",
      "[LightGBM] [Info] Start training from score -1.129091\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.239404753906364e-07. Current value: lambda_l2=1.239404753906364e-07\n",
      "===========fold 2================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.239404753906364e-07. Current value: lambda_l2=1.239404753906364e-07\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.239404753906364e-07. Current value: lambda_l2=1.239404753906364e-07\n",
      "[LightGBM] [Info] Number of positive: 162885, number of negative: 503782\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7207\n",
      "[LightGBM] [Info] Number of data points in the train set: 666667, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.013090 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.239404753906364e-07. Current value: lambda_l2=1.239404753906364e-07\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244327 -> initscore=-1.129099\n",
      "[LightGBM] [Info] Start training from score -1.129099\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.239404753906364e-07. Current value: lambda_l2=1.239404753906364e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 17:37:39,564] Trial 0 finished with value: 0.8754446994948714 and parameters: {'learning_rate': 0.05159575385157755, 'max_depth': 1, 'colsample_bytree': 0.1, 'subsample': 0.4, 'lambda': 1.239404753906364e-07, 'alpha': 2.681823137198342e-07, 'hidden_layer_sizes': 3}. Best is trial 0 with value: 0.8754446994948714.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=4.5401716432636326e-08. Current value: lambda_l2=4.5401716432636326e-08\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=4.5401716432636326e-08. Current value: lambda_l2=4.5401716432636326e-08\n",
      "[LightGBM] [Info] Number of positive: 162885, number of negative: 503781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7208\n",
      "[LightGBM] [Info] Number of data points in the train set: 666666, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.011522 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=4.5401716432636326e-08. Current value: lambda_l2=4.5401716432636326e-08\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244328 -> initscore=-1.129097\n",
      "[LightGBM] [Info] Start training from score -1.129097\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=4.5401716432636326e-08. Current value: lambda_l2=4.5401716432636326e-08\n",
      "===========fold 1================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=4.5401716432636326e-08. Current value: lambda_l2=4.5401716432636326e-08\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=4.5401716432636326e-08. Current value: lambda_l2=4.5401716432636326e-08\n",
      "[LightGBM] [Info] Number of positive: 162886, number of negative: 503781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7222\n",
      "[LightGBM] [Info] Number of data points in the train set: 666667, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.013696 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=4.5401716432636326e-08. Current value: lambda_l2=4.5401716432636326e-08\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244329 -> initscore=-1.129091\n",
      "[LightGBM] [Info] Start training from score -1.129091\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=4.5401716432636326e-08. Current value: lambda_l2=4.5401716432636326e-08\n",
      "===========fold 2================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=4.5401716432636326e-08. Current value: lambda_l2=4.5401716432636326e-08\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=4.5401716432636326e-08. Current value: lambda_l2=4.5401716432636326e-08\n",
      "[LightGBM] [Info] Number of positive: 162885, number of negative: 503782\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7207\n",
      "[LightGBM] [Info] Number of data points in the train set: 666667, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.012937 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=4.5401716432636326e-08. Current value: lambda_l2=4.5401716432636326e-08\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244327 -> initscore=-1.129099\n",
      "[LightGBM] [Info] Start training from score -1.129099\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=4.5401716432636326e-08. Current value: lambda_l2=4.5401716432636326e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 17:38:27,455] Trial 1 finished with value: 0.9005112448849425 and parameters: {'learning_rate': 0.07318806304212372, 'max_depth': 2, 'colsample_bytree': 0.30000000000000004, 'subsample': 0.9, 'lambda': 4.5401716432636326e-08, 'alpha': 3.734719921036922e-08, 'hidden_layer_sizes': 5}. Best is trial 1 with value: 0.9005112448849425.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=7.084916227011512e-08. Current value: lambda_l2=7.084916227011512e-08\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=7.084916227011512e-08. Current value: lambda_l2=7.084916227011512e-08\n",
      "[LightGBM] [Info] Number of positive: 162885, number of negative: 503781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7208\n",
      "[LightGBM] [Info] Number of data points in the train set: 666666, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.012472 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=7.084916227011512e-08. Current value: lambda_l2=7.084916227011512e-08\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244328 -> initscore=-1.129097\n",
      "[LightGBM] [Info] Start training from score -1.129097\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=7.084916227011512e-08. Current value: lambda_l2=7.084916227011512e-08\n",
      "===========fold 1================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=7.084916227011512e-08. Current value: lambda_l2=7.084916227011512e-08\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=7.084916227011512e-08. Current value: lambda_l2=7.084916227011512e-08\n",
      "[LightGBM] [Info] Number of positive: 162886, number of negative: 503781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7222\n",
      "[LightGBM] [Info] Number of data points in the train set: 666667, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.012771 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=7.084916227011512e-08. Current value: lambda_l2=7.084916227011512e-08\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244329 -> initscore=-1.129091\n",
      "[LightGBM] [Info] Start training from score -1.129091\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=7.084916227011512e-08. Current value: lambda_l2=7.084916227011512e-08\n",
      "===========fold 2================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=7.084916227011512e-08. Current value: lambda_l2=7.084916227011512e-08\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=7.084916227011512e-08. Current value: lambda_l2=7.084916227011512e-08\n",
      "[LightGBM] [Info] Number of positive: 162885, number of negative: 503782\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7207\n",
      "[LightGBM] [Info] Number of data points in the train set: 666667, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.012056 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=7.084916227011512e-08. Current value: lambda_l2=7.084916227011512e-08\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244327 -> initscore=-1.129099\n",
      "[LightGBM] [Info] Start training from score -1.129099\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=7.084916227011512e-08. Current value: lambda_l2=7.084916227011512e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 17:39:24,324] Trial 2 finished with value: 0.9033540395699454 and parameters: {'learning_rate': 0.07490388860592595, 'max_depth': 4, 'colsample_bytree': 0.2, 'subsample': 0.7000000000000001, 'lambda': 7.084916227011512e-08, 'alpha': 1.1700461763673442e-08, 'hidden_layer_sizes': 2}. Best is trial 2 with value: 0.9033540395699454.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.18315717990398372. Current value: lambda_l2=0.18315717990398372\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.18315717990398372. Current value: lambda_l2=0.18315717990398372\n",
      "[LightGBM] [Info] Number of positive: 162885, number of negative: 503781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7208\n",
      "[LightGBM] [Info] Number of data points in the train set: 666666, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.013093 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.18315717990398372. Current value: lambda_l2=0.18315717990398372\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244328 -> initscore=-1.129097\n",
      "[LightGBM] [Info] Start training from score -1.129097\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.18315717990398372. Current value: lambda_l2=0.18315717990398372\n",
      "===========fold 1================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.18315717990398372. Current value: lambda_l2=0.18315717990398372\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.18315717990398372. Current value: lambda_l2=0.18315717990398372\n",
      "[LightGBM] [Info] Number of positive: 162886, number of negative: 503781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7222\n",
      "[LightGBM] [Info] Number of data points in the train set: 666667, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.013912 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.18315717990398372. Current value: lambda_l2=0.18315717990398372\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244329 -> initscore=-1.129091\n",
      "[LightGBM] [Info] Start training from score -1.129091\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.18315717990398372. Current value: lambda_l2=0.18315717990398372\n",
      "===========fold 2================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.18315717990398372. Current value: lambda_l2=0.18315717990398372\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.18315717990398372. Current value: lambda_l2=0.18315717990398372\n",
      "[LightGBM] [Info] Number of positive: 162885, number of negative: 503782\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7207\n",
      "[LightGBM] [Info] Number of data points in the train set: 666667, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.014005 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.18315717990398372. Current value: lambda_l2=0.18315717990398372\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244327 -> initscore=-1.129099\n",
      "[LightGBM] [Info] Start training from score -1.129099\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.18315717990398372. Current value: lambda_l2=0.18315717990398372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 17:40:11,178] Trial 3 finished with value: 0.898442669439263 and parameters: {'learning_rate': 0.03754431982234925, 'max_depth': 2, 'colsample_bytree': 0.4, 'subsample': 0.9, 'lambda': 0.18315717990398372, 'alpha': 0.019802901461469298, 'hidden_layer_sizes': 5}. Best is trial 2 with value: 0.9033540395699454.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.0005495679883947764. Current value: lambda_l2=0.0005495679883947764\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.0005495679883947764. Current value: lambda_l2=0.0005495679883947764\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162885, number of negative: 503781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7208\n",
      "[LightGBM] [Info] Number of data points in the train set: 666666, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.012649 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.0005495679883947764. Current value: lambda_l2=0.0005495679883947764\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244328 -> initscore=-1.129097\n",
      "[LightGBM] [Info] Start training from score -1.129097\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.0005495679883947764. Current value: lambda_l2=0.0005495679883947764\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "===========fold 1================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.0005495679883947764. Current value: lambda_l2=0.0005495679883947764\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.0005495679883947764. Current value: lambda_l2=0.0005495679883947764\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162886, number of negative: 503781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7222\n",
      "[LightGBM] [Info] Number of data points in the train set: 666667, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.011451 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.0005495679883947764. Current value: lambda_l2=0.0005495679883947764\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244329 -> initscore=-1.129091\n",
      "[LightGBM] [Info] Start training from score -1.129091\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.0005495679883947764. Current value: lambda_l2=0.0005495679883947764\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "===========fold 2================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.0005495679883947764. Current value: lambda_l2=0.0005495679883947764\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.0005495679883947764. Current value: lambda_l2=0.0005495679883947764\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162885, number of negative: 503782\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7207\n",
      "[LightGBM] [Info] Number of data points in the train set: 666667, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.013418 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.0005495679883947764. Current value: lambda_l2=0.0005495679883947764\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244327 -> initscore=-1.129099\n",
      "[LightGBM] [Info] Start training from score -1.129099\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.0005495679883947764. Current value: lambda_l2=0.0005495679883947764\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 17:41:14,271] Trial 4 finished with value: 0.904550410267838 and parameters: {'learning_rate': 0.0744083963152459, 'max_depth': 7, 'colsample_bytree': 0.2, 'subsample': 0.4, 'lambda': 0.0005495679883947764, 'alpha': 2.9638543170843122e-05, 'hidden_layer_sizes': 2}. Best is trial 4 with value: 0.904550410267838.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.08976745486624027. Current value: lambda_l2=0.08976745486624027\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.08976745486624027. Current value: lambda_l2=0.08976745486624027\n",
      "[LightGBM] [Info] Number of positive: 162885, number of negative: 503781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7208\n",
      "[LightGBM] [Info] Number of data points in the train set: 666666, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.012739 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.08976745486624027. Current value: lambda_l2=0.08976745486624027\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244328 -> initscore=-1.129097\n",
      "[LightGBM] [Info] Start training from score -1.129097\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.08976745486624027. Current value: lambda_l2=0.08976745486624027\n",
      "===========fold 1================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.08976745486624027. Current value: lambda_l2=0.08976745486624027\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.08976745486624027. Current value: lambda_l2=0.08976745486624027\n",
      "[LightGBM] [Info] Number of positive: 162886, number of negative: 503781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7222\n",
      "[LightGBM] [Info] Number of data points in the train set: 666667, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.013705 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.08976745486624027. Current value: lambda_l2=0.08976745486624027\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244329 -> initscore=-1.129091\n",
      "[LightGBM] [Info] Start training from score -1.129091\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.08976745486624027. Current value: lambda_l2=0.08976745486624027\n",
      "===========fold 2================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.08976745486624027. Current value: lambda_l2=0.08976745486624027\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.08976745486624027. Current value: lambda_l2=0.08976745486624027\n",
      "[LightGBM] [Info] Number of positive: 162885, number of negative: 503782\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7207\n",
      "[LightGBM] [Info] Number of data points in the train set: 666667, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.013651 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.08976745486624027. Current value: lambda_l2=0.08976745486624027\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244327 -> initscore=-1.129099\n",
      "[LightGBM] [Info] Start training from score -1.129099\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.08976745486624027. Current value: lambda_l2=0.08976745486624027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 17:41:50,070] Trial 5 finished with value: 0.8993058772169805 and parameters: {'learning_rate': 0.014755181886358531, 'max_depth': 3, 'colsample_bytree': 1.0, 'subsample': 0.6, 'lambda': 0.08976745486624027, 'alpha': 2.979555343499092e-07, 'hidden_layer_sizes': 5}. Best is trial 4 with value: 0.904550410267838.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=2.4000739160118012e-06. Current value: lambda_l2=2.4000739160118012e-06\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=2.4000739160118012e-06. Current value: lambda_l2=2.4000739160118012e-06\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162885, number of negative: 503781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7208\n",
      "[LightGBM] [Info] Number of data points in the train set: 666666, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.011845 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=2.4000739160118012e-06. Current value: lambda_l2=2.4000739160118012e-06\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244328 -> initscore=-1.129097\n",
      "[LightGBM] [Info] Start training from score -1.129097\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=2.4000739160118012e-06. Current value: lambda_l2=2.4000739160118012e-06\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "===========fold 1================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=2.4000739160118012e-06. Current value: lambda_l2=2.4000739160118012e-06\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=2.4000739160118012e-06. Current value: lambda_l2=2.4000739160118012e-06\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162886, number of negative: 503781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7222\n",
      "[LightGBM] [Info] Number of data points in the train set: 666667, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.012672 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=2.4000739160118012e-06. Current value: lambda_l2=2.4000739160118012e-06\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244329 -> initscore=-1.129091\n",
      "[LightGBM] [Info] Start training from score -1.129091\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=2.4000739160118012e-06. Current value: lambda_l2=2.4000739160118012e-06\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "===========fold 2================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=2.4000739160118012e-06. Current value: lambda_l2=2.4000739160118012e-06\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=2.4000739160118012e-06. Current value: lambda_l2=2.4000739160118012e-06\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162885, number of negative: 503782\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7207\n",
      "[LightGBM] [Info] Number of data points in the train set: 666667, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.012495 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=2.4000739160118012e-06. Current value: lambda_l2=2.4000739160118012e-06\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244327 -> initscore=-1.129099\n",
      "[LightGBM] [Info] Start training from score -1.129099\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=2.4000739160118012e-06. Current value: lambda_l2=2.4000739160118012e-06\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 17:42:53,791] Trial 6 finished with value: 0.9055787054465337 and parameters: {'learning_rate': 0.056461055348658515, 'max_depth': 8, 'colsample_bytree': 0.4, 'subsample': 0.5, 'lambda': 2.4000739160118012e-06, 'alpha': 1.0024608992441064e-07, 'hidden_layer_sizes': 2}. Best is trial 6 with value: 0.9055787054465337.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.11500372375130032. Current value: lambda_l2=0.11500372375130032\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.11500372375130032. Current value: lambda_l2=0.11500372375130032\n",
      "[LightGBM] [Info] Number of positive: 162885, number of negative: 503781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7208\n",
      "[LightGBM] [Info] Number of data points in the train set: 666666, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.013366 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.11500372375130032. Current value: lambda_l2=0.11500372375130032\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244328 -> initscore=-1.129097\n",
      "[LightGBM] [Info] Start training from score -1.129097\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.11500372375130032. Current value: lambda_l2=0.11500372375130032\n",
      "===========fold 1================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.11500372375130032. Current value: lambda_l2=0.11500372375130032\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.11500372375130032. Current value: lambda_l2=0.11500372375130032\n",
      "[LightGBM] [Info] Number of positive: 162886, number of negative: 503781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7222\n",
      "[LightGBM] [Info] Number of data points in the train set: 666667, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.013721 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.11500372375130032. Current value: lambda_l2=0.11500372375130032\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244329 -> initscore=-1.129091\n",
      "[LightGBM] [Info] Start training from score -1.129091\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.11500372375130032. Current value: lambda_l2=0.11500372375130032\n",
      "===========fold 2================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.11500372375130032. Current value: lambda_l2=0.11500372375130032\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.11500372375130032. Current value: lambda_l2=0.11500372375130032\n",
      "[LightGBM] [Info] Number of positive: 162885, number of negative: 503782\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7207\n",
      "[LightGBM] [Info] Number of data points in the train set: 666667, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.012183 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.11500372375130032. Current value: lambda_l2=0.11500372375130032\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244327 -> initscore=-1.129099\n",
      "[LightGBM] [Info] Start training from score -1.129099\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.11500372375130032. Current value: lambda_l2=0.11500372375130032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 17:43:50,132] Trial 7 finished with value: 0.9045838411831619 and parameters: {'learning_rate': 0.07909869895964622, 'max_depth': 4, 'colsample_bytree': 0.4, 'subsample': 0.30000000000000004, 'lambda': 0.11500372375130032, 'alpha': 4.5785539740277486e-05, 'hidden_layer_sizes': 5}. Best is trial 6 with value: 0.9055787054465337.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.011670522030040268. Current value: lambda_l2=0.011670522030040268\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.011670522030040268. Current value: lambda_l2=0.011670522030040268\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162885, number of negative: 503781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7208\n",
      "[LightGBM] [Info] Number of data points in the train set: 666666, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.013095 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.011670522030040268. Current value: lambda_l2=0.011670522030040268\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244328 -> initscore=-1.129097\n",
      "[LightGBM] [Info] Start training from score -1.129097\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.011670522030040268. Current value: lambda_l2=0.011670522030040268\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "===========fold 1================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.011670522030040268. Current value: lambda_l2=0.011670522030040268\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.011670522030040268. Current value: lambda_l2=0.011670522030040268\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162886, number of negative: 503781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7222\n",
      "[LightGBM] [Info] Number of data points in the train set: 666667, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.011912 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.011670522030040268. Current value: lambda_l2=0.011670522030040268\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244329 -> initscore=-1.129091\n",
      "[LightGBM] [Info] Start training from score -1.129091\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.011670522030040268. Current value: lambda_l2=0.011670522030040268\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "===========fold 2================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.011670522030040268. Current value: lambda_l2=0.011670522030040268\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.011670522030040268. Current value: lambda_l2=0.011670522030040268\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162885, number of negative: 503782\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7207\n",
      "[LightGBM] [Info] Number of data points in the train set: 666667, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.013636 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.011670522030040268. Current value: lambda_l2=0.011670522030040268\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244327 -> initscore=-1.129099\n",
      "[LightGBM] [Info] Start training from score -1.129099\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.011670522030040268. Current value: lambda_l2=0.011670522030040268\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 17:44:47,538] Trial 8 finished with value: 0.9056928928479867 and parameters: {'learning_rate': 0.04973533272614951, 'max_depth': 7, 'colsample_bytree': 0.8, 'subsample': 0.6, 'lambda': 0.011670522030040268, 'alpha': 0.000810164372379314, 'hidden_layer_sizes': 3}. Best is trial 8 with value: 0.9056928928479867.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.0002552577175911713. Current value: lambda_l2=0.0002552577175911713\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.0002552577175911713. Current value: lambda_l2=0.0002552577175911713\n",
      "[LightGBM] [Info] Number of positive: 162885, number of negative: 503781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7208\n",
      "[LightGBM] [Info] Number of data points in the train set: 666666, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.014269 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.0002552577175911713. Current value: lambda_l2=0.0002552577175911713\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244328 -> initscore=-1.129097\n",
      "[LightGBM] [Info] Start training from score -1.129097\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.0002552577175911713. Current value: lambda_l2=0.0002552577175911713\n",
      "===========fold 1================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.0002552577175911713. Current value: lambda_l2=0.0002552577175911713\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.0002552577175911713. Current value: lambda_l2=0.0002552577175911713\n",
      "[LightGBM] [Info] Number of positive: 162886, number of negative: 503781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7222\n",
      "[LightGBM] [Info] Number of data points in the train set: 666667, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.012923 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.0002552577175911713. Current value: lambda_l2=0.0002552577175911713\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244329 -> initscore=-1.129091\n",
      "[LightGBM] [Info] Start training from score -1.129091\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.0002552577175911713. Current value: lambda_l2=0.0002552577175911713\n",
      "===========fold 2================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.0002552577175911713. Current value: lambda_l2=0.0002552577175911713\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.0002552577175911713. Current value: lambda_l2=0.0002552577175911713\n",
      "[LightGBM] [Info] Number of positive: 162885, number of negative: 503782\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7207\n",
      "[LightGBM] [Info] Number of data points in the train set: 666667, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.013376 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.0002552577175911713. Current value: lambda_l2=0.0002552577175911713\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244327 -> initscore=-1.129099\n",
      "[LightGBM] [Info] Start training from score -1.129099\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.0002552577175911713. Current value: lambda_l2=0.0002552577175911713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 17:45:30,051] Trial 9 finished with value: 0.8753513690839801 and parameters: {'learning_rate': 0.051145827986049816, 'max_depth': 1, 'colsample_bytree': 0.1, 'subsample': 0.4, 'lambda': 0.0002552577175911713, 'alpha': 0.31574536210052956, 'hidden_layer_sizes': 2}. Best is trial 8 with value: 0.9056928928479867.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.002181830597198747. Current value: lambda_l2=0.002181830597198747\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.002181830597198747. Current value: lambda_l2=0.002181830597198747\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162885, number of negative: 503781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7208\n",
      "[LightGBM] [Info] Number of data points in the train set: 666666, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.014273 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.002181830597198747. Current value: lambda_l2=0.002181830597198747\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244328 -> initscore=-1.129097\n",
      "[LightGBM] [Info] Start training from score -1.129097\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.002181830597198747. Current value: lambda_l2=0.002181830597198747\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "===========fold 1================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.002181830597198747. Current value: lambda_l2=0.002181830597198747\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.002181830597198747. Current value: lambda_l2=0.002181830597198747\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162886, number of negative: 503781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7222\n",
      "[LightGBM] [Info] Number of data points in the train set: 666667, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.013563 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.002181830597198747. Current value: lambda_l2=0.002181830597198747\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244329 -> initscore=-1.129091\n",
      "[LightGBM] [Info] Start training from score -1.129091\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.002181830597198747. Current value: lambda_l2=0.002181830597198747\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "===========fold 2================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.002181830597198747. Current value: lambda_l2=0.002181830597198747\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.002181830597198747. Current value: lambda_l2=0.002181830597198747\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162885, number of negative: 503782\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7207\n",
      "[LightGBM] [Info] Number of data points in the train set: 666667, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.013494 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.002181830597198747. Current value: lambda_l2=0.002181830597198747\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244327 -> initscore=-1.129099\n",
      "[LightGBM] [Info] Start training from score -1.129099\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.002181830597198747. Current value: lambda_l2=0.002181830597198747\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 17:46:27,175] Trial 10 finished with value: 0.9046147450300034 and parameters: {'learning_rate': 0.022632189143717667, 'max_depth': 6, 'colsample_bytree': 0.8, 'subsample': 0.2, 'lambda': 0.002181830597198747, 'alpha': 0.003207373521634328, 'hidden_layer_sizes': 4}. Best is trial 8 with value: 0.9056928928479867.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=3.129842709423685e-06. Current value: lambda_l2=3.129842709423685e-06\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=3.129842709423685e-06. Current value: lambda_l2=3.129842709423685e-06\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162885, number of negative: 503781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7208\n",
      "[LightGBM] [Info] Number of data points in the train set: 666666, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.012356 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=3.129842709423685e-06. Current value: lambda_l2=3.129842709423685e-06\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244328 -> initscore=-1.129097\n",
      "[LightGBM] [Info] Start training from score -1.129097\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=3.129842709423685e-06. Current value: lambda_l2=3.129842709423685e-06\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "===========fold 1================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=3.129842709423685e-06. Current value: lambda_l2=3.129842709423685e-06\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=3.129842709423685e-06. Current value: lambda_l2=3.129842709423685e-06\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162886, number of negative: 503781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7222\n",
      "[LightGBM] [Info] Number of data points in the train set: 666667, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.013194 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=3.129842709423685e-06. Current value: lambda_l2=3.129842709423685e-06\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244329 -> initscore=-1.129091\n",
      "[LightGBM] [Info] Start training from score -1.129091\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=3.129842709423685e-06. Current value: lambda_l2=3.129842709423685e-06\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "===========fold 2================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=3.129842709423685e-06. Current value: lambda_l2=3.129842709423685e-06\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=3.129842709423685e-06. Current value: lambda_l2=3.129842709423685e-06\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162885, number of negative: 503782\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7207\n",
      "[LightGBM] [Info] Number of data points in the train set: 666667, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.014890 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=3.129842709423685e-06. Current value: lambda_l2=3.129842709423685e-06\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244327 -> initscore=-1.129099\n",
      "[LightGBM] [Info] Start training from score -1.129099\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=3.129842709423685e-06. Current value: lambda_l2=3.129842709423685e-06\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 17:47:22,753] Trial 11 finished with value: 0.9057535598749437 and parameters: {'learning_rate': 0.09974911460660776, 'max_depth': 9, 'colsample_bytree': 0.7000000000000001, 'subsample': 0.7000000000000001, 'lambda': 3.129842709423685e-06, 'alpha': 0.0015147086516244857, 'hidden_layer_sizes': 3}. Best is trial 11 with value: 0.9057535598749437.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=4.674928225027324e-06. Current value: lambda_l2=4.674928225027324e-06\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=4.674928225027324e-06. Current value: lambda_l2=4.674928225027324e-06\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162885, number of negative: 503781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7208\n",
      "[LightGBM] [Info] Number of data points in the train set: 666666, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.012950 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=4.674928225027324e-06. Current value: lambda_l2=4.674928225027324e-06\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244328 -> initscore=-1.129097\n",
      "[LightGBM] [Info] Start training from score -1.129097\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=4.674928225027324e-06. Current value: lambda_l2=4.674928225027324e-06\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "===========fold 1================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=4.674928225027324e-06. Current value: lambda_l2=4.674928225027324e-06\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=4.674928225027324e-06. Current value: lambda_l2=4.674928225027324e-06\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162886, number of negative: 503781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7222\n",
      "[LightGBM] [Info] Number of data points in the train set: 666667, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.012876 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=4.674928225027324e-06. Current value: lambda_l2=4.674928225027324e-06\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244329 -> initscore=-1.129091\n",
      "[LightGBM] [Info] Start training from score -1.129091\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=4.674928225027324e-06. Current value: lambda_l2=4.674928225027324e-06\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "===========fold 2================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=4.674928225027324e-06. Current value: lambda_l2=4.674928225027324e-06\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=4.674928225027324e-06. Current value: lambda_l2=4.674928225027324e-06\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162885, number of negative: 503782\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7207\n",
      "[LightGBM] [Info] Number of data points in the train set: 666667, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.013389 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=4.674928225027324e-06. Current value: lambda_l2=4.674928225027324e-06\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244327 -> initscore=-1.129099\n",
      "[LightGBM] [Info] Start training from score -1.129099\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=4.674928225027324e-06. Current value: lambda_l2=4.674928225027324e-06\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 17:48:18,218] Trial 12 finished with value: 0.905658698853879 and parameters: {'learning_rate': 0.09794289366159341, 'max_depth': 9, 'colsample_bytree': 0.7000000000000001, 'subsample': 0.7000000000000001, 'lambda': 4.674928225027324e-06, 'alpha': 0.0008010599044886133, 'hidden_layer_sizes': 3}. Best is trial 11 with value: 0.9057535598749437.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.2674600166409832e-05. Current value: lambda_l2=1.2674600166409832e-05\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.2674600166409832e-05. Current value: lambda_l2=1.2674600166409832e-05\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162885, number of negative: 503781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7208\n",
      "[LightGBM] [Info] Number of data points in the train set: 666666, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.013226 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.2674600166409832e-05. Current value: lambda_l2=1.2674600166409832e-05\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244328 -> initscore=-1.129097\n",
      "[LightGBM] [Info] Start training from score -1.129097\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.2674600166409832e-05. Current value: lambda_l2=1.2674600166409832e-05\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "===========fold 1================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.2674600166409832e-05. Current value: lambda_l2=1.2674600166409832e-05\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.2674600166409832e-05. Current value: lambda_l2=1.2674600166409832e-05\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162886, number of negative: 503781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7222\n",
      "[LightGBM] [Info] Number of data points in the train set: 666667, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.012188 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.2674600166409832e-05. Current value: lambda_l2=1.2674600166409832e-05\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244329 -> initscore=-1.129091\n",
      "[LightGBM] [Info] Start training from score -1.129091\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.2674600166409832e-05. Current value: lambda_l2=1.2674600166409832e-05\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "===========fold 2================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.2674600166409832e-05. Current value: lambda_l2=1.2674600166409832e-05\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.2674600166409832e-05. Current value: lambda_l2=1.2674600166409832e-05\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162885, number of negative: 503782\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7207\n",
      "[LightGBM] [Info] Number of data points in the train set: 666667, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.014213 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.2674600166409832e-05. Current value: lambda_l2=1.2674600166409832e-05\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244327 -> initscore=-1.129099\n",
      "[LightGBM] [Info] Start training from score -1.129099\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.2674600166409832e-05. Current value: lambda_l2=1.2674600166409832e-05\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 17:49:13,808] Trial 13 finished with value: 0.905667403997079 and parameters: {'learning_rate': 0.09550526100829605, 'max_depth': 9, 'colsample_bytree': 0.7000000000000001, 'subsample': 0.8, 'lambda': 1.2674600166409832e-05, 'alpha': 7.060759165856776e-06, 'hidden_layer_sizes': 3}. Best is trial 11 with value: 0.9057535598749437.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.006569355742167307. Current value: lambda_l2=0.006569355742167307\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.006569355742167307. Current value: lambda_l2=0.006569355742167307\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162885, number of negative: 503781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7208\n",
      "[LightGBM] [Info] Number of data points in the train set: 666666, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.012926 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.006569355742167307. Current value: lambda_l2=0.006569355742167307\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244328 -> initscore=-1.129097\n",
      "[LightGBM] [Info] Start training from score -1.129097\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.006569355742167307. Current value: lambda_l2=0.006569355742167307\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "===========fold 1================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.006569355742167307. Current value: lambda_l2=0.006569355742167307\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.006569355742167307. Current value: lambda_l2=0.006569355742167307\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162886, number of negative: 503781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7222\n",
      "[LightGBM] [Info] Number of data points in the train set: 666667, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.012896 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.006569355742167307. Current value: lambda_l2=0.006569355742167307\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244329 -> initscore=-1.129091\n",
      "[LightGBM] [Info] Start training from score -1.129091\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.006569355742167307. Current value: lambda_l2=0.006569355742167307\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "===========fold 2================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.006569355742167307. Current value: lambda_l2=0.006569355742167307\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.006569355742167307. Current value: lambda_l2=0.006569355742167307\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162885, number of negative: 503782\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7207\n",
      "[LightGBM] [Info] Number of data points in the train set: 666667, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.012524 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.006569355742167307. Current value: lambda_l2=0.006569355742167307\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244327 -> initscore=-1.129099\n",
      "[LightGBM] [Info] Start training from score -1.129099\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.006569355742167307. Current value: lambda_l2=0.006569355742167307\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 17:50:11,711] Trial 14 finished with value: 0.9054895143381921 and parameters: {'learning_rate': 0.03696815284395776, 'max_depth': 7, 'colsample_bytree': 0.9, 'subsample': 1.0, 'lambda': 0.006569355742167307, 'alpha': 0.05460635461460687, 'hidden_layer_sizes': 4}. Best is trial 11 with value: 0.9057535598749437.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=4.051331361815287e-05. Current value: lambda_l2=4.051331361815287e-05\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=4.051331361815287e-05. Current value: lambda_l2=4.051331361815287e-05\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162885, number of negative: 503781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7208\n",
      "[LightGBM] [Info] Number of data points in the train set: 666666, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.013064 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=4.051331361815287e-05. Current value: lambda_l2=4.051331361815287e-05\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244328 -> initscore=-1.129097\n",
      "[LightGBM] [Info] Start training from score -1.129097\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=4.051331361815287e-05. Current value: lambda_l2=4.051331361815287e-05\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "===========fold 1================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=4.051331361815287e-05. Current value: lambda_l2=4.051331361815287e-05\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=4.051331361815287e-05. Current value: lambda_l2=4.051331361815287e-05\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162886, number of negative: 503781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7222\n",
      "[LightGBM] [Info] Number of data points in the train set: 666667, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.013833 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=4.051331361815287e-05. Current value: lambda_l2=4.051331361815287e-05\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244329 -> initscore=-1.129091\n",
      "[LightGBM] [Info] Start training from score -1.129091\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=4.051331361815287e-05. Current value: lambda_l2=4.051331361815287e-05\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "===========fold 2================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=4.051331361815287e-05. Current value: lambda_l2=4.051331361815287e-05\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=4.051331361815287e-05. Current value: lambda_l2=4.051331361815287e-05\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162885, number of negative: 503782\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7207\n",
      "[LightGBM] [Info] Number of data points in the train set: 666667, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.014520 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=4.051331361815287e-05. Current value: lambda_l2=4.051331361815287e-05\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244327 -> initscore=-1.129099\n",
      "[LightGBM] [Info] Start training from score -1.129099\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=4.051331361815287e-05. Current value: lambda_l2=4.051331361815287e-05\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 17:51:11,256] Trial 15 finished with value: 0.905705778990271 and parameters: {'learning_rate': 0.059126711442404965, 'max_depth': 6, 'colsample_bytree': 0.6, 'subsample': 0.6, 'lambda': 4.051331361815287e-05, 'alpha': 0.0003854373650465689, 'hidden_layer_sizes': 3}. Best is trial 11 with value: 0.9057535598749437.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=5.67040193521957e-05. Current value: lambda_l2=5.67040193521957e-05\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=5.67040193521957e-05. Current value: lambda_l2=5.67040193521957e-05\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162885, number of negative: 503781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7208\n",
      "[LightGBM] [Info] Number of data points in the train set: 666666, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.012392 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=5.67040193521957e-05. Current value: lambda_l2=5.67040193521957e-05\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244328 -> initscore=-1.129097\n",
      "[LightGBM] [Info] Start training from score -1.129097\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=5.67040193521957e-05. Current value: lambda_l2=5.67040193521957e-05\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "===========fold 1================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=5.67040193521957e-05. Current value: lambda_l2=5.67040193521957e-05\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=5.67040193521957e-05. Current value: lambda_l2=5.67040193521957e-05\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162886, number of negative: 503781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7222\n",
      "[LightGBM] [Info] Number of data points in the train set: 666667, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.013923 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=5.67040193521957e-05. Current value: lambda_l2=5.67040193521957e-05\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244329 -> initscore=-1.129091\n",
      "[LightGBM] [Info] Start training from score -1.129091\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=5.67040193521957e-05. Current value: lambda_l2=5.67040193521957e-05\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "===========fold 2================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=5.67040193521957e-05. Current value: lambda_l2=5.67040193521957e-05\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=5.67040193521957e-05. Current value: lambda_l2=5.67040193521957e-05\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162885, number of negative: 503782\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7207\n",
      "[LightGBM] [Info] Number of data points in the train set: 666667, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.013030 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=5.67040193521957e-05. Current value: lambda_l2=5.67040193521957e-05\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244327 -> initscore=-1.129099\n",
      "[LightGBM] [Info] Start training from score -1.129099\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=5.67040193521957e-05. Current value: lambda_l2=5.67040193521957e-05\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 17:52:08,662] Trial 16 finished with value: 0.9054933094567262 and parameters: {'learning_rate': 0.08612633747780703, 'max_depth': 5, 'colsample_bytree': 0.6, 'subsample': 0.7000000000000001, 'lambda': 5.67040193521957e-05, 'alpha': 3.966539676244127e-06, 'hidden_layer_sizes': 4}. Best is trial 11 with value: 0.9057535598749437.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=9.550584059522892e-07. Current value: lambda_l2=9.550584059522892e-07\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=9.550584059522892e-07. Current value: lambda_l2=9.550584059522892e-07\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162885, number of negative: 503781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7208\n",
      "[LightGBM] [Info] Number of data points in the train set: 666666, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.012291 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=9.550584059522892e-07. Current value: lambda_l2=9.550584059522892e-07\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244328 -> initscore=-1.129097\n",
      "[LightGBM] [Info] Start training from score -1.129097\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=9.550584059522892e-07. Current value: lambda_l2=9.550584059522892e-07\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "===========fold 1================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=9.550584059522892e-07. Current value: lambda_l2=9.550584059522892e-07\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=9.550584059522892e-07. Current value: lambda_l2=9.550584059522892e-07\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162886, number of negative: 503781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7222\n",
      "[LightGBM] [Info] Number of data points in the train set: 666667, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.012564 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=9.550584059522892e-07. Current value: lambda_l2=9.550584059522892e-07\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244329 -> initscore=-1.129091\n",
      "[LightGBM] [Info] Start training from score -1.129091\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=9.550584059522892e-07. Current value: lambda_l2=9.550584059522892e-07\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "===========fold 2================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=9.550584059522892e-07. Current value: lambda_l2=9.550584059522892e-07\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=9.550584059522892e-07. Current value: lambda_l2=9.550584059522892e-07\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162885, number of negative: 503782\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7207\n",
      "[LightGBM] [Info] Number of data points in the train set: 666667, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.014422 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=9.550584059522892e-07. Current value: lambda_l2=9.550584059522892e-07\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244327 -> initscore=-1.129099\n",
      "[LightGBM] [Info] Start training from score -1.129099\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=9.550584059522892e-07. Current value: lambda_l2=9.550584059522892e-07\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 17:53:08,635] Trial 17 finished with value: 0.9057393536414708 and parameters: {'learning_rate': 0.06477449934088249, 'max_depth': 6, 'colsample_bytree': 0.6, 'subsample': 0.5, 'lambda': 9.550584059522892e-07, 'alpha': 0.0003424973119431319, 'hidden_layer_sizes': 3}. Best is trial 11 with value: 0.9057535598749437.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.0784253964194445e-06. Current value: lambda_l2=1.0784253964194445e-06\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.0784253964194445e-06. Current value: lambda_l2=1.0784253964194445e-06\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162885, number of negative: 503781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7208\n",
      "[LightGBM] [Info] Number of data points in the train set: 666666, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.012604 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.0784253964194445e-06. Current value: lambda_l2=1.0784253964194445e-06\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244328 -> initscore=-1.129097\n",
      "[LightGBM] [Info] Start training from score -1.129097\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.0784253964194445e-06. Current value: lambda_l2=1.0784253964194445e-06\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "===========fold 1================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.0784253964194445e-06. Current value: lambda_l2=1.0784253964194445e-06\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.0784253964194445e-06. Current value: lambda_l2=1.0784253964194445e-06\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162886, number of negative: 503781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7222\n",
      "[LightGBM] [Info] Number of data points in the train set: 666667, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.012765 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.0784253964194445e-06. Current value: lambda_l2=1.0784253964194445e-06\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244329 -> initscore=-1.129091\n",
      "[LightGBM] [Info] Start training from score -1.129091\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.0784253964194445e-06. Current value: lambda_l2=1.0784253964194445e-06\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "===========fold 2================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.0784253964194445e-06. Current value: lambda_l2=1.0784253964194445e-06\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.0784253964194445e-06. Current value: lambda_l2=1.0784253964194445e-06\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162885, number of negative: 503782\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7207\n",
      "[LightGBM] [Info] Number of data points in the train set: 666667, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.013613 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.0784253964194445e-06. Current value: lambda_l2=1.0784253964194445e-06\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244327 -> initscore=-1.129099\n",
      "[LightGBM] [Info] Start training from score -1.129099\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.0784253964194445e-06. Current value: lambda_l2=1.0784253964194445e-06\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 17:54:07,901] Trial 18 finished with value: 0.9058233890210564 and parameters: {'learning_rate': 0.0658137038780867, 'max_depth': 8, 'colsample_bytree': 0.7000000000000001, 'subsample': 0.1, 'lambda': 1.0784253964194445e-06, 'alpha': 0.006328483113721023, 'hidden_layer_sizes': 4}. Best is trial 18 with value: 0.9058233890210564.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=3.9750112418272603e-07. Current value: lambda_l2=3.9750112418272603e-07\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=3.9750112418272603e-07. Current value: lambda_l2=3.9750112418272603e-07\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162885, number of negative: 503781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7208\n",
      "[LightGBM] [Info] Number of data points in the train set: 666666, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.012049 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=3.9750112418272603e-07. Current value: lambda_l2=3.9750112418272603e-07\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244328 -> initscore=-1.129097\n",
      "[LightGBM] [Info] Start training from score -1.129097\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=3.9750112418272603e-07. Current value: lambda_l2=3.9750112418272603e-07\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "===========fold 1================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=3.9750112418272603e-07. Current value: lambda_l2=3.9750112418272603e-07\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=3.9750112418272603e-07. Current value: lambda_l2=3.9750112418272603e-07\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162886, number of negative: 503781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7222\n",
      "[LightGBM] [Info] Number of data points in the train set: 666667, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.013021 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=3.9750112418272603e-07. Current value: lambda_l2=3.9750112418272603e-07\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244329 -> initscore=-1.129091\n",
      "[LightGBM] [Info] Start training from score -1.129091\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=3.9750112418272603e-07. Current value: lambda_l2=3.9750112418272603e-07\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "===========fold 2================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=3.9750112418272603e-07. Current value: lambda_l2=3.9750112418272603e-07\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=3.9750112418272603e-07. Current value: lambda_l2=3.9750112418272603e-07\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162885, number of negative: 503782\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7207\n",
      "[LightGBM] [Info] Number of data points in the train set: 666667, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.013296 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=3.9750112418272603e-07. Current value: lambda_l2=3.9750112418272603e-07\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244327 -> initscore=-1.129099\n",
      "[LightGBM] [Info] Start training from score -1.129099\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=3.9750112418272603e-07. Current value: lambda_l2=3.9750112418272603e-07\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 17:55:02,744] Trial 19 finished with value: 0.9057865420093917 and parameters: {'learning_rate': 0.08772356475873519, 'max_depth': 8, 'colsample_bytree': 1.0, 'subsample': 0.1, 'lambda': 3.9750112418272603e-07, 'alpha': 0.007705629078377228, 'hidden_layer_sizes': 4}. Best is trial 18 with value: 0.9058233890210564.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.379510769043844e-08. Current value: lambda_l2=1.379510769043844e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.379510769043844e-08. Current value: lambda_l2=1.379510769043844e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162885, number of negative: 503781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7208\n",
      "[LightGBM] [Info] Number of data points in the train set: 666666, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.013663 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.379510769043844e-08. Current value: lambda_l2=1.379510769043844e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244328 -> initscore=-1.129097\n",
      "[LightGBM] [Info] Start training from score -1.129097\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.379510769043844e-08. Current value: lambda_l2=1.379510769043844e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "===========fold 1================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.379510769043844e-08. Current value: lambda_l2=1.379510769043844e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.379510769043844e-08. Current value: lambda_l2=1.379510769043844e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162886, number of negative: 503781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7222\n",
      "[LightGBM] [Info] Number of data points in the train set: 666667, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.013348 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.379510769043844e-08. Current value: lambda_l2=1.379510769043844e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244329 -> initscore=-1.129091\n",
      "[LightGBM] [Info] Start training from score -1.129091\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.379510769043844e-08. Current value: lambda_l2=1.379510769043844e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "===========fold 2================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.379510769043844e-08. Current value: lambda_l2=1.379510769043844e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.379510769043844e-08. Current value: lambda_l2=1.379510769043844e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162885, number of negative: 503782\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7207\n",
      "[LightGBM] [Info] Number of data points in the train set: 666667, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.013510 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.379510769043844e-08. Current value: lambda_l2=1.379510769043844e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244327 -> initscore=-1.129099\n",
      "[LightGBM] [Info] Start training from score -1.129099\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.379510769043844e-08. Current value: lambda_l2=1.379510769043844e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 17:55:57,993] Trial 20 finished with value: 0.905774051321924 and parameters: {'learning_rate': 0.08771065366467211, 'max_depth': 8, 'colsample_bytree': 1.0, 'subsample': 0.1, 'lambda': 1.379510769043844e-08, 'alpha': 0.7476926249911091, 'hidden_layer_sizes': 4}. Best is trial 18 with value: 0.9058233890210564.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.561929742080257e-08. Current value: lambda_l2=1.561929742080257e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.561929742080257e-08. Current value: lambda_l2=1.561929742080257e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162885, number of negative: 503781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7208\n",
      "[LightGBM] [Info] Number of data points in the train set: 666666, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.012531 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.561929742080257e-08. Current value: lambda_l2=1.561929742080257e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244328 -> initscore=-1.129097\n",
      "[LightGBM] [Info] Start training from score -1.129097\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.561929742080257e-08. Current value: lambda_l2=1.561929742080257e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "===========fold 1================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.561929742080257e-08. Current value: lambda_l2=1.561929742080257e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.561929742080257e-08. Current value: lambda_l2=1.561929742080257e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162886, number of negative: 503781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7222\n",
      "[LightGBM] [Info] Number of data points in the train set: 666667, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.013722 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.561929742080257e-08. Current value: lambda_l2=1.561929742080257e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244329 -> initscore=-1.129091\n",
      "[LightGBM] [Info] Start training from score -1.129091\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.561929742080257e-08. Current value: lambda_l2=1.561929742080257e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "===========fold 2================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.561929742080257e-08. Current value: lambda_l2=1.561929742080257e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.561929742080257e-08. Current value: lambda_l2=1.561929742080257e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162885, number of negative: 503782\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7207\n",
      "[LightGBM] [Info] Number of data points in the train set: 666667, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.013476 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.561929742080257e-08. Current value: lambda_l2=1.561929742080257e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244327 -> initscore=-1.129099\n",
      "[LightGBM] [Info] Start training from score -1.129099\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.561929742080257e-08. Current value: lambda_l2=1.561929742080257e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 17:56:52,785] Trial 21 finished with value: 0.9057865437368687 and parameters: {'learning_rate': 0.08587358734535355, 'max_depth': 8, 'colsample_bytree': 1.0, 'subsample': 0.1, 'lambda': 1.561929742080257e-08, 'alpha': 0.6774322334136937, 'hidden_layer_sizes': 4}. Best is trial 18 with value: 0.9058233890210564.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=3.0910762478628306e-07. Current value: lambda_l2=3.0910762478628306e-07\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=3.0910762478628306e-07. Current value: lambda_l2=3.0910762478628306e-07\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162885, number of negative: 503781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7208\n",
      "[LightGBM] [Info] Number of data points in the train set: 666666, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.012857 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=3.0910762478628306e-07. Current value: lambda_l2=3.0910762478628306e-07\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244328 -> initscore=-1.129097\n",
      "[LightGBM] [Info] Start training from score -1.129097\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=3.0910762478628306e-07. Current value: lambda_l2=3.0910762478628306e-07\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "===========fold 1================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=3.0910762478628306e-07. Current value: lambda_l2=3.0910762478628306e-07\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=3.0910762478628306e-07. Current value: lambda_l2=3.0910762478628306e-07\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162886, number of negative: 503781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7222\n",
      "[LightGBM] [Info] Number of data points in the train set: 666667, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.013708 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=3.0910762478628306e-07. Current value: lambda_l2=3.0910762478628306e-07\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244329 -> initscore=-1.129091\n",
      "[LightGBM] [Info] Start training from score -1.129091\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=3.0910762478628306e-07. Current value: lambda_l2=3.0910762478628306e-07\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "===========fold 2================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=3.0910762478628306e-07. Current value: lambda_l2=3.0910762478628306e-07\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=3.0910762478628306e-07. Current value: lambda_l2=3.0910762478628306e-07\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162885, number of negative: 503782\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7207\n",
      "[LightGBM] [Info] Number of data points in the train set: 666667, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.014386 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=3.0910762478628306e-07. Current value: lambda_l2=3.0910762478628306e-07\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244327 -> initscore=-1.129099\n",
      "[LightGBM] [Info] Start training from score -1.129099\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=3.0910762478628306e-07. Current value: lambda_l2=3.0910762478628306e-07\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 17:57:48,512] Trial 22 finished with value: 0.9057494767963808 and parameters: {'learning_rate': 0.08657861128320248, 'max_depth': 8, 'colsample_bytree': 0.9, 'subsample': 0.1, 'lambda': 3.0910762478628306e-07, 'alpha': 0.017557064165218295, 'hidden_layer_sizes': 4}. Best is trial 18 with value: 0.9058233890210564.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.842329949054407e-08. Current value: lambda_l2=1.842329949054407e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.842329949054407e-08. Current value: lambda_l2=1.842329949054407e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162885, number of negative: 503781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7208\n",
      "[LightGBM] [Info] Number of data points in the train set: 666666, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.012809 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.842329949054407e-08. Current value: lambda_l2=1.842329949054407e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244328 -> initscore=-1.129097\n",
      "[LightGBM] [Info] Start training from score -1.129097\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.842329949054407e-08. Current value: lambda_l2=1.842329949054407e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "===========fold 1================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.842329949054407e-08. Current value: lambda_l2=1.842329949054407e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.842329949054407e-08. Current value: lambda_l2=1.842329949054407e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162886, number of negative: 503781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7222\n",
      "[LightGBM] [Info] Number of data points in the train set: 666667, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.014313 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.842329949054407e-08. Current value: lambda_l2=1.842329949054407e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244329 -> initscore=-1.129091\n",
      "[LightGBM] [Info] Start training from score -1.129091\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.842329949054407e-08. Current value: lambda_l2=1.842329949054407e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "===========fold 2================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.842329949054407e-08. Current value: lambda_l2=1.842329949054407e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.842329949054407e-08. Current value: lambda_l2=1.842329949054407e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162885, number of negative: 503782\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7207\n",
      "[LightGBM] [Info] Number of data points in the train set: 666667, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.016567 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.842329949054407e-08. Current value: lambda_l2=1.842329949054407e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244327 -> initscore=-1.129099\n",
      "[LightGBM] [Info] Start training from score -1.129099\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.842329949054407e-08. Current value: lambda_l2=1.842329949054407e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 17:58:45,006] Trial 23 finished with value: 0.9057950663136604 and parameters: {'learning_rate': 0.06655864091241893, 'max_depth': 8, 'colsample_bytree': 0.9, 'subsample': 0.2, 'lambda': 1.842329949054407e-08, 'alpha': 0.09901831457079101, 'hidden_layer_sizes': 4}. Best is trial 18 with value: 0.9058233890210564.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.538025052609531e-08. Current value: lambda_l2=1.538025052609531e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.538025052609531e-08. Current value: lambda_l2=1.538025052609531e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162885, number of negative: 503781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7208\n",
      "[LightGBM] [Info] Number of data points in the train set: 666666, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.013428 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.538025052609531e-08. Current value: lambda_l2=1.538025052609531e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244328 -> initscore=-1.129097\n",
      "[LightGBM] [Info] Start training from score -1.129097\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.538025052609531e-08. Current value: lambda_l2=1.538025052609531e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "===========fold 1================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.538025052609531e-08. Current value: lambda_l2=1.538025052609531e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.538025052609531e-08. Current value: lambda_l2=1.538025052609531e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162886, number of negative: 503781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7222\n",
      "[LightGBM] [Info] Number of data points in the train set: 666667, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.012932 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.538025052609531e-08. Current value: lambda_l2=1.538025052609531e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244329 -> initscore=-1.129091\n",
      "[LightGBM] [Info] Start training from score -1.129091\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.538025052609531e-08. Current value: lambda_l2=1.538025052609531e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "===========fold 2================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.538025052609531e-08. Current value: lambda_l2=1.538025052609531e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.538025052609531e-08. Current value: lambda_l2=1.538025052609531e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162885, number of negative: 503782\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7207\n",
      "[LightGBM] [Info] Number of data points in the train set: 666667, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.013031 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.538025052609531e-08. Current value: lambda_l2=1.538025052609531e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244327 -> initscore=-1.129099\n",
      "[LightGBM] [Info] Start training from score -1.129099\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.538025052609531e-08. Current value: lambda_l2=1.538025052609531e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 17:59:41,389] Trial 24 finished with value: 0.9057437698933238 and parameters: {'learning_rate': 0.06303154222273687, 'max_depth': 7, 'colsample_bytree': 0.9, 'subsample': 0.2, 'lambda': 1.538025052609531e-08, 'alpha': 0.12196789800517992, 'hidden_layer_sizes': 4}. Best is trial 18 with value: 0.9058233890210564.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.686174990949594e-08. Current value: lambda_l2=1.686174990949594e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.686174990949594e-08. Current value: lambda_l2=1.686174990949594e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162885, number of negative: 503781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7208\n",
      "[LightGBM] [Info] Number of data points in the train set: 666666, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.013434 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.686174990949594e-08. Current value: lambda_l2=1.686174990949594e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244328 -> initscore=-1.129097\n",
      "[LightGBM] [Info] Start training from score -1.129097\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.686174990949594e-08. Current value: lambda_l2=1.686174990949594e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "===========fold 1================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.686174990949594e-08. Current value: lambda_l2=1.686174990949594e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.686174990949594e-08. Current value: lambda_l2=1.686174990949594e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162886, number of negative: 503781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7222\n",
      "[LightGBM] [Info] Number of data points in the train set: 666667, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.012612 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.686174990949594e-08. Current value: lambda_l2=1.686174990949594e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244329 -> initscore=-1.129091\n",
      "[LightGBM] [Info] Start training from score -1.129091\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.686174990949594e-08. Current value: lambda_l2=1.686174990949594e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "===========fold 2================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.686174990949594e-08. Current value: lambda_l2=1.686174990949594e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.686174990949594e-08. Current value: lambda_l2=1.686174990949594e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162885, number of negative: 503782\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7207\n",
      "[LightGBM] [Info] Number of data points in the train set: 666667, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.012511 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.686174990949594e-08. Current value: lambda_l2=1.686174990949594e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244327 -> initscore=-1.129099\n",
      "[LightGBM] [Info] Start training from score -1.129099\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.686174990949594e-08. Current value: lambda_l2=1.686174990949594e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 18:00:38,559] Trial 25 finished with value: 0.9057747129439632 and parameters: {'learning_rate': 0.0678169211078777, 'max_depth': 9, 'colsample_bytree': 0.8, 'subsample': 0.2, 'lambda': 1.686174990949594e-08, 'alpha': 0.1126496017606244, 'hidden_layer_sizes': 4}. Best is trial 18 with value: 0.9058233890210564.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=2.0789762863678023e-07. Current value: lambda_l2=2.0789762863678023e-07\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=2.0789762863678023e-07. Current value: lambda_l2=2.0789762863678023e-07\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162885, number of negative: 503781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7208\n",
      "[LightGBM] [Info] Number of data points in the train set: 666666, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.012601 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=2.0789762863678023e-07. Current value: lambda_l2=2.0789762863678023e-07\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244328 -> initscore=-1.129097\n",
      "[LightGBM] [Info] Start training from score -1.129097\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=2.0789762863678023e-07. Current value: lambda_l2=2.0789762863678023e-07\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "===========fold 1================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=2.0789762863678023e-07. Current value: lambda_l2=2.0789762863678023e-07\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=2.0789762863678023e-07. Current value: lambda_l2=2.0789762863678023e-07\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162886, number of negative: 503781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7222\n",
      "[LightGBM] [Info] Number of data points in the train set: 666667, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.012676 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=2.0789762863678023e-07. Current value: lambda_l2=2.0789762863678023e-07\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244329 -> initscore=-1.129091\n",
      "[LightGBM] [Info] Start training from score -1.129091\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=2.0789762863678023e-07. Current value: lambda_l2=2.0789762863678023e-07\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "===========fold 2================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=2.0789762863678023e-07. Current value: lambda_l2=2.0789762863678023e-07\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=2.0789762863678023e-07. Current value: lambda_l2=2.0789762863678023e-07\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162885, number of negative: 503782\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7207\n",
      "[LightGBM] [Info] Number of data points in the train set: 666667, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.012461 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=2.0789762863678023e-07. Current value: lambda_l2=2.0789762863678023e-07\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244327 -> initscore=-1.129099\n",
      "[LightGBM] [Info] Start training from score -1.129099\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=2.0789762863678023e-07. Current value: lambda_l2=2.0789762863678023e-07\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 18:01:33,433] Trial 26 finished with value: 0.9051266371850927 and parameters: {'learning_rate': 0.04071793090030894, 'max_depth': 5, 'colsample_bytree': 0.9, 'subsample': 0.30000000000000004, 'lambda': 2.0789762863678023e-07, 'alpha': 0.35777512303540654, 'hidden_layer_sizes': 5}. Best is trial 18 with value: 0.9058233890210564.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=8.29944846779051e-07. Current value: lambda_l2=8.29944846779051e-07\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=8.29944846779051e-07. Current value: lambda_l2=8.29944846779051e-07\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162885, number of negative: 503781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7208\n",
      "[LightGBM] [Info] Number of data points in the train set: 666666, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.013779 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=8.29944846779051e-07. Current value: lambda_l2=8.29944846779051e-07\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244328 -> initscore=-1.129097\n",
      "[LightGBM] [Info] Start training from score -1.129097\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=8.29944846779051e-07. Current value: lambda_l2=8.29944846779051e-07\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "===========fold 1================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=8.29944846779051e-07. Current value: lambda_l2=8.29944846779051e-07\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=8.29944846779051e-07. Current value: lambda_l2=8.29944846779051e-07\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162886, number of negative: 503781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7222\n",
      "[LightGBM] [Info] Number of data points in the train set: 666667, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.012441 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=8.29944846779051e-07. Current value: lambda_l2=8.29944846779051e-07\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244329 -> initscore=-1.129091\n",
      "[LightGBM] [Info] Start training from score -1.129091\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=8.29944846779051e-07. Current value: lambda_l2=8.29944846779051e-07\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "===========fold 2================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=8.29944846779051e-07. Current value: lambda_l2=8.29944846779051e-07\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=8.29944846779051e-07. Current value: lambda_l2=8.29944846779051e-07\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162885, number of negative: 503782\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7207\n",
      "[LightGBM] [Info] Number of data points in the train set: 666667, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.012728 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=8.29944846779051e-07. Current value: lambda_l2=8.29944846779051e-07\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244327 -> initscore=-1.129099\n",
      "[LightGBM] [Info] Start training from score -1.129099\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=8.29944846779051e-07. Current value: lambda_l2=8.29944846779051e-07\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 18:02:29,021] Trial 27 finished with value: 0.9057895968226388 and parameters: {'learning_rate': 0.08032197948277842, 'max_depth': 8, 'colsample_bytree': 0.8, 'subsample': 0.30000000000000004, 'lambda': 8.29944846779051e-07, 'alpha': 0.0446132271020494, 'hidden_layer_sizes': 4}. Best is trial 18 with value: 0.9058233890210564.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.6231514113746232e-05. Current value: lambda_l2=1.6231514113746232e-05\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.6231514113746232e-05. Current value: lambda_l2=1.6231514113746232e-05\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162885, number of negative: 503781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7208\n",
      "[LightGBM] [Info] Number of data points in the train set: 666666, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.013903 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.6231514113746232e-05. Current value: lambda_l2=1.6231514113746232e-05\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244328 -> initscore=-1.129097\n",
      "[LightGBM] [Info] Start training from score -1.129097\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.6231514113746232e-05. Current value: lambda_l2=1.6231514113746232e-05\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "===========fold 1================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.6231514113746232e-05. Current value: lambda_l2=1.6231514113746232e-05\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.6231514113746232e-05. Current value: lambda_l2=1.6231514113746232e-05\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162886, number of negative: 503781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7222\n",
      "[LightGBM] [Info] Number of data points in the train set: 666667, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.013922 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.6231514113746232e-05. Current value: lambda_l2=1.6231514113746232e-05\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244329 -> initscore=-1.129091\n",
      "[LightGBM] [Info] Start training from score -1.129091\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.6231514113746232e-05. Current value: lambda_l2=1.6231514113746232e-05\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "===========fold 2================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.6231514113746232e-05. Current value: lambda_l2=1.6231514113746232e-05\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.6231514113746232e-05. Current value: lambda_l2=1.6231514113746232e-05\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162885, number of negative: 503782\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7207\n",
      "[LightGBM] [Info] Number of data points in the train set: 666667, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.013740 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.6231514113746232e-05. Current value: lambda_l2=1.6231514113746232e-05\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244327 -> initscore=-1.129099\n",
      "[LightGBM] [Info] Start training from score -1.129099\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.6231514113746232e-05. Current value: lambda_l2=1.6231514113746232e-05\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 18:03:30,616] Trial 28 finished with value: 0.9056687030709462 and parameters: {'learning_rate': 0.06983938285508655, 'max_depth': 6, 'colsample_bytree': 0.5, 'subsample': 0.30000000000000004, 'lambda': 1.6231514113746232e-05, 'alpha': 0.02797962248725204, 'hidden_layer_sizes': 4}. Best is trial 18 with value: 0.9058233890210564.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.316244006044968e-06. Current value: lambda_l2=1.316244006044968e-06\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.316244006044968e-06. Current value: lambda_l2=1.316244006044968e-06\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162885, number of negative: 503781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7208\n",
      "[LightGBM] [Info] Number of data points in the train set: 666666, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.013752 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.316244006044968e-06. Current value: lambda_l2=1.316244006044968e-06\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244328 -> initscore=-1.129097\n",
      "[LightGBM] [Info] Start training from score -1.129097\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.316244006044968e-06. Current value: lambda_l2=1.316244006044968e-06\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "===========fold 1================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.316244006044968e-06. Current value: lambda_l2=1.316244006044968e-06\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.316244006044968e-06. Current value: lambda_l2=1.316244006044968e-06\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162886, number of negative: 503781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7222\n",
      "[LightGBM] [Info] Number of data points in the train set: 666667, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.013886 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.316244006044968e-06. Current value: lambda_l2=1.316244006044968e-06\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244329 -> initscore=-1.129091\n",
      "[LightGBM] [Info] Start training from score -1.129091\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.316244006044968e-06. Current value: lambda_l2=1.316244006044968e-06\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "===========fold 2================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.316244006044968e-06. Current value: lambda_l2=1.316244006044968e-06\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.316244006044968e-06. Current value: lambda_l2=1.316244006044968e-06\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162885, number of negative: 503782\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7207\n",
      "[LightGBM] [Info] Number of data points in the train set: 666667, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.013238 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.316244006044968e-06. Current value: lambda_l2=1.316244006044968e-06\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244327 -> initscore=-1.129099\n",
      "[LightGBM] [Info] Start training from score -1.129099\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.316244006044968e-06. Current value: lambda_l2=1.316244006044968e-06\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 18:04:26,353] Trial 29 finished with value: 0.9057751152844672 and parameters: {'learning_rate': 0.07876297991947573, 'max_depth': 7, 'colsample_bytree': 0.8, 'subsample': 0.2, 'lambda': 1.316244006044968e-06, 'alpha': 0.0057825158088062505, 'hidden_layer_sizes': 5}. Best is trial 18 with value: 0.9058233890210564.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.011302224237326e-07. Current value: lambda_l2=1.011302224237326e-07\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.011302224237326e-07. Current value: lambda_l2=1.011302224237326e-07\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162885, number of negative: 503781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7208\n",
      "[LightGBM] [Info] Number of data points in the train set: 666666, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.013366 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.011302224237326e-07. Current value: lambda_l2=1.011302224237326e-07\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244328 -> initscore=-1.129097\n",
      "[LightGBM] [Info] Start training from score -1.129097\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.011302224237326e-07. Current value: lambda_l2=1.011302224237326e-07\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "===========fold 1================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.011302224237326e-07. Current value: lambda_l2=1.011302224237326e-07\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.011302224237326e-07. Current value: lambda_l2=1.011302224237326e-07\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162886, number of negative: 503781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7222\n",
      "[LightGBM] [Info] Number of data points in the train set: 666667, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.013488 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.011302224237326e-07. Current value: lambda_l2=1.011302224237326e-07\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244329 -> initscore=-1.129091\n",
      "[LightGBM] [Info] Start training from score -1.129091\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.011302224237326e-07. Current value: lambda_l2=1.011302224237326e-07\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "===========fold 2================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.011302224237326e-07. Current value: lambda_l2=1.011302224237326e-07\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.011302224237326e-07. Current value: lambda_l2=1.011302224237326e-07\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162885, number of negative: 503782\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7207\n",
      "[LightGBM] [Info] Number of data points in the train set: 666667, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.012385 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.011302224237326e-07. Current value: lambda_l2=1.011302224237326e-07\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244327 -> initscore=-1.129099\n",
      "[LightGBM] [Info] Start training from score -1.129099\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.011302224237326e-07. Current value: lambda_l2=1.011302224237326e-07\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 18:05:26,971] Trial 30 finished with value: 0.905744211932678 and parameters: {'learning_rate': 0.04431018035970935, 'max_depth': 9, 'colsample_bytree': 0.7000000000000001, 'subsample': 0.30000000000000004, 'lambda': 1.011302224237326e-07, 'alpha': 0.09864571537121782, 'hidden_layer_sizes': 4}. Best is trial 18 with value: 0.9058233890210564.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=7.022815570592384e-08. Current value: lambda_l2=7.022815570592384e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=7.022815570592384e-08. Current value: lambda_l2=7.022815570592384e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162885, number of negative: 503781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7208\n",
      "[LightGBM] [Info] Number of data points in the train set: 666666, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.013020 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=7.022815570592384e-08. Current value: lambda_l2=7.022815570592384e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244328 -> initscore=-1.129097\n",
      "[LightGBM] [Info] Start training from score -1.129097\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=7.022815570592384e-08. Current value: lambda_l2=7.022815570592384e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "===========fold 1================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=7.022815570592384e-08. Current value: lambda_l2=7.022815570592384e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=7.022815570592384e-08. Current value: lambda_l2=7.022815570592384e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162886, number of negative: 503781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7222\n",
      "[LightGBM] [Info] Number of data points in the train set: 666667, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.014290 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=7.022815570592384e-08. Current value: lambda_l2=7.022815570592384e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244329 -> initscore=-1.129091\n",
      "[LightGBM] [Info] Start training from score -1.129091\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=7.022815570592384e-08. Current value: lambda_l2=7.022815570592384e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "===========fold 2================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=7.022815570592384e-08. Current value: lambda_l2=7.022815570592384e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=7.022815570592384e-08. Current value: lambda_l2=7.022815570592384e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162885, number of negative: 503782\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7207\n",
      "[LightGBM] [Info] Number of data points in the train set: 666667, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.012009 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=7.022815570592384e-08. Current value: lambda_l2=7.022815570592384e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244327 -> initscore=-1.129099\n",
      "[LightGBM] [Info] Start training from score -1.129099\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=7.022815570592384e-08. Current value: lambda_l2=7.022815570592384e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 18:06:22,151] Trial 31 finished with value: 0.9057898827809492 and parameters: {'learning_rate': 0.07923019115115545, 'max_depth': 8, 'colsample_bytree': 1.0, 'subsample': 0.1, 'lambda': 7.022815570592384e-08, 'alpha': 0.8858744832767761, 'hidden_layer_sizes': 4}. Best is trial 18 with value: 0.9058233890210564.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=7.449359332799856e-08. Current value: lambda_l2=7.449359332799856e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=7.449359332799856e-08. Current value: lambda_l2=7.449359332799856e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162885, number of negative: 503781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7208\n",
      "[LightGBM] [Info] Number of data points in the train set: 666666, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.013035 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=7.449359332799856e-08. Current value: lambda_l2=7.449359332799856e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244328 -> initscore=-1.129097\n",
      "[LightGBM] [Info] Start training from score -1.129097\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=7.449359332799856e-08. Current value: lambda_l2=7.449359332799856e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "===========fold 1================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=7.449359332799856e-08. Current value: lambda_l2=7.449359332799856e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=7.449359332799856e-08. Current value: lambda_l2=7.449359332799856e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162886, number of negative: 503781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7222\n",
      "[LightGBM] [Info] Number of data points in the train set: 666667, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.013088 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=7.449359332799856e-08. Current value: lambda_l2=7.449359332799856e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244329 -> initscore=-1.129091\n",
      "[LightGBM] [Info] Start training from score -1.129091\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=7.449359332799856e-08. Current value: lambda_l2=7.449359332799856e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "===========fold 2================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=7.449359332799856e-08. Current value: lambda_l2=7.449359332799856e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=7.449359332799856e-08. Current value: lambda_l2=7.449359332799856e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162885, number of negative: 503782\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7207\n",
      "[LightGBM] [Info] Number of data points in the train set: 666667, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.013367 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=7.449359332799856e-08. Current value: lambda_l2=7.449359332799856e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244327 -> initscore=-1.129099\n",
      "[LightGBM] [Info] Start training from score -1.129099\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=7.449359332799856e-08. Current value: lambda_l2=7.449359332799856e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 18:07:18,045] Trial 32 finished with value: 0.9057738494853259 and parameters: {'learning_rate': 0.07957159084728271, 'max_depth': 8, 'colsample_bytree': 0.9, 'subsample': 0.2, 'lambda': 7.449359332799856e-08, 'alpha': 0.22298327007125338, 'hidden_layer_sizes': 4}. Best is trial 18 with value: 0.9058233890210564.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=6.179941120720259e-07. Current value: lambda_l2=6.179941120720259e-07\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=6.179941120720259e-07. Current value: lambda_l2=6.179941120720259e-07\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162885, number of negative: 503781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7208\n",
      "[LightGBM] [Info] Number of data points in the train set: 666666, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.013535 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=6.179941120720259e-07. Current value: lambda_l2=6.179941120720259e-07\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244328 -> initscore=-1.129097\n",
      "[LightGBM] [Info] Start training from score -1.129097\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=6.179941120720259e-07. Current value: lambda_l2=6.179941120720259e-07\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "===========fold 1================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=6.179941120720259e-07. Current value: lambda_l2=6.179941120720259e-07\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=6.179941120720259e-07. Current value: lambda_l2=6.179941120720259e-07\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162886, number of negative: 503781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7222\n",
      "[LightGBM] [Info] Number of data points in the train set: 666667, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.012879 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=6.179941120720259e-07. Current value: lambda_l2=6.179941120720259e-07\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244329 -> initscore=-1.129091\n",
      "[LightGBM] [Info] Start training from score -1.129091\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=6.179941120720259e-07. Current value: lambda_l2=6.179941120720259e-07\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "===========fold 2================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=6.179941120720259e-07. Current value: lambda_l2=6.179941120720259e-07\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=6.179941120720259e-07. Current value: lambda_l2=6.179941120720259e-07\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162885, number of negative: 503782\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7207\n",
      "[LightGBM] [Info] Number of data points in the train set: 666667, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.013355 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=6.179941120720259e-07. Current value: lambda_l2=6.179941120720259e-07\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244327 -> initscore=-1.129099\n",
      "[LightGBM] [Info] Start training from score -1.129099\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=6.179941120720259e-07. Current value: lambda_l2=6.179941120720259e-07\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 18:08:15,705] Trial 33 finished with value: 0.90574851319052 and parameters: {'learning_rate': 0.06252179500744545, 'max_depth': 8, 'colsample_bytree': 0.8, 'subsample': 0.1, 'lambda': 6.179941120720259e-07, 'alpha': 0.9858292069993992, 'hidden_layer_sizes': 4}. Best is trial 18 with value: 0.9058233890210564.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=4.677848031013297e-08. Current value: lambda_l2=4.677848031013297e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=4.677848031013297e-08. Current value: lambda_l2=4.677848031013297e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162885, number of negative: 503781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7208\n",
      "[LightGBM] [Info] Number of data points in the train set: 666666, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.012276 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=4.677848031013297e-08. Current value: lambda_l2=4.677848031013297e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244328 -> initscore=-1.129097\n",
      "[LightGBM] [Info] Start training from score -1.129097\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=4.677848031013297e-08. Current value: lambda_l2=4.677848031013297e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "===========fold 1================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=4.677848031013297e-08. Current value: lambda_l2=4.677848031013297e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=4.677848031013297e-08. Current value: lambda_l2=4.677848031013297e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162886, number of negative: 503781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7222\n",
      "[LightGBM] [Info] Number of data points in the train set: 666667, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.012054 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=4.677848031013297e-08. Current value: lambda_l2=4.677848031013297e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244329 -> initscore=-1.129091\n",
      "[LightGBM] [Info] Start training from score -1.129091\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=4.677848031013297e-08. Current value: lambda_l2=4.677848031013297e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "===========fold 2================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=4.677848031013297e-08. Current value: lambda_l2=4.677848031013297e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=4.677848031013297e-08. Current value: lambda_l2=4.677848031013297e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162885, number of negative: 503782\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7207\n",
      "[LightGBM] [Info] Number of data points in the train set: 666667, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.013053 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=4.677848031013297e-08. Current value: lambda_l2=4.677848031013297e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244327 -> initscore=-1.129099\n",
      "[LightGBM] [Info] Start training from score -1.129099\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=4.677848031013297e-08. Current value: lambda_l2=4.677848031013297e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 18:09:11,824] Trial 34 finished with value: 0.9057778085988509 and parameters: {'learning_rate': 0.0715020397317393, 'max_depth': 7, 'colsample_bytree': 0.9, 'subsample': 0.2, 'lambda': 4.677848031013297e-08, 'alpha': 0.049756891446745666, 'hidden_layer_sizes': 5}. Best is trial 18 with value: 0.9058233890210564.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.5684046331451845e-07. Current value: lambda_l2=1.5684046331451845e-07\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.5684046331451845e-07. Current value: lambda_l2=1.5684046331451845e-07\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162885, number of negative: 503781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7208\n",
      "[LightGBM] [Info] Number of data points in the train set: 666666, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.012034 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.5684046331451845e-07. Current value: lambda_l2=1.5684046331451845e-07\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244328 -> initscore=-1.129097\n",
      "[LightGBM] [Info] Start training from score -1.129097\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.5684046331451845e-07. Current value: lambda_l2=1.5684046331451845e-07\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "===========fold 1================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.5684046331451845e-07. Current value: lambda_l2=1.5684046331451845e-07\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.5684046331451845e-07. Current value: lambda_l2=1.5684046331451845e-07\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162886, number of negative: 503781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7222\n",
      "[LightGBM] [Info] Number of data points in the train set: 666667, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.013839 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.5684046331451845e-07. Current value: lambda_l2=1.5684046331451845e-07\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244329 -> initscore=-1.129091\n",
      "[LightGBM] [Info] Start training from score -1.129091\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.5684046331451845e-07. Current value: lambda_l2=1.5684046331451845e-07\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "===========fold 2================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.5684046331451845e-07. Current value: lambda_l2=1.5684046331451845e-07\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.5684046331451845e-07. Current value: lambda_l2=1.5684046331451845e-07\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162885, number of negative: 503782\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7207\n",
      "[LightGBM] [Info] Number of data points in the train set: 666667, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.013632 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.5684046331451845e-07. Current value: lambda_l2=1.5684046331451845e-07\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244327 -> initscore=-1.129099\n",
      "[LightGBM] [Info] Start training from score -1.129099\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.5684046331451845e-07. Current value: lambda_l2=1.5684046331451845e-07\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 18:10:06,818] Trial 35 finished with value: 0.9057786752991923 and parameters: {'learning_rate': 0.07758334937209085, 'max_depth': 9, 'colsample_bytree': 1.0, 'subsample': 0.4, 'lambda': 1.5684046331451845e-07, 'alpha': 0.011289640002879965, 'hidden_layer_sizes': 4}. Best is trial 18 with value: 0.9058233890210564.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=8.734133238369725e-06. Current value: lambda_l2=8.734133238369725e-06\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=8.734133238369725e-06. Current value: lambda_l2=8.734133238369725e-06\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162885, number of negative: 503781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7208\n",
      "[LightGBM] [Info] Number of data points in the train set: 666666, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.012561 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=8.734133238369725e-06. Current value: lambda_l2=8.734133238369725e-06\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244328 -> initscore=-1.129097\n",
      "[LightGBM] [Info] Start training from score -1.129097\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=8.734133238369725e-06. Current value: lambda_l2=8.734133238369725e-06\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "===========fold 1================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=8.734133238369725e-06. Current value: lambda_l2=8.734133238369725e-06\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=8.734133238369725e-06. Current value: lambda_l2=8.734133238369725e-06\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162886, number of negative: 503781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7222\n",
      "[LightGBM] [Info] Number of data points in the train set: 666667, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.012725 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=8.734133238369725e-06. Current value: lambda_l2=8.734133238369725e-06\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244329 -> initscore=-1.129091\n",
      "[LightGBM] [Info] Start training from score -1.129091\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=8.734133238369725e-06. Current value: lambda_l2=8.734133238369725e-06\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "===========fold 2================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=8.734133238369725e-06. Current value: lambda_l2=8.734133238369725e-06\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=8.734133238369725e-06. Current value: lambda_l2=8.734133238369725e-06\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162885, number of negative: 503782\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7207\n",
      "[LightGBM] [Info] Number of data points in the train set: 666667, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.012915 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=8.734133238369725e-06. Current value: lambda_l2=8.734133238369725e-06\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244327 -> initscore=-1.129099\n",
      "[LightGBM] [Info] Start training from score -1.129099\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=8.734133238369725e-06. Current value: lambda_l2=8.734133238369725e-06\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 18:11:08,997] Trial 36 finished with value: 0.9057427890996328 and parameters: {'learning_rate': 0.06646654116486815, 'max_depth': 7, 'colsample_bytree': 0.5, 'subsample': 0.30000000000000004, 'lambda': 8.734133238369725e-06, 'alpha': 0.0038229349323723573, 'hidden_layer_sizes': 5}. Best is trial 18 with value: 0.9058233890210564.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=6.404295605650428e-08. Current value: lambda_l2=6.404295605650428e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=6.404295605650428e-08. Current value: lambda_l2=6.404295605650428e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162885, number of negative: 503781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7208\n",
      "[LightGBM] [Info] Number of data points in the train set: 666666, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.014234 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=6.404295605650428e-08. Current value: lambda_l2=6.404295605650428e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244328 -> initscore=-1.129097\n",
      "[LightGBM] [Info] Start training from score -1.129097\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=6.404295605650428e-08. Current value: lambda_l2=6.404295605650428e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "===========fold 1================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=6.404295605650428e-08. Current value: lambda_l2=6.404295605650428e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=6.404295605650428e-08. Current value: lambda_l2=6.404295605650428e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162886, number of negative: 503781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7222\n",
      "[LightGBM] [Info] Number of data points in the train set: 666667, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.013350 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=6.404295605650428e-08. Current value: lambda_l2=6.404295605650428e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244329 -> initscore=-1.129091\n",
      "[LightGBM] [Info] Start training from score -1.129091\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=6.404295605650428e-08. Current value: lambda_l2=6.404295605650428e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "===========fold 2================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=6.404295605650428e-08. Current value: lambda_l2=6.404295605650428e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=6.404295605650428e-08. Current value: lambda_l2=6.404295605650428e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162885, number of negative: 503782\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7207\n",
      "[LightGBM] [Info] Number of data points in the train set: 666667, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.015367 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=6.404295605650428e-08. Current value: lambda_l2=6.404295605650428e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244327 -> initscore=-1.129099\n",
      "[LightGBM] [Info] Start training from score -1.129099\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=6.404295605650428e-08. Current value: lambda_l2=6.404295605650428e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 18:12:05,453] Trial 37 finished with value: 0.9057262889673389 and parameters: {'learning_rate': 0.09202412856797382, 'max_depth': 8, 'colsample_bytree': 0.8, 'subsample': 0.1, 'lambda': 6.404295605650428e-08, 'alpha': 0.028141736924204516, 'hidden_layer_sizes': 4}. Best is trial 18 with value: 0.9058233890210564.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.4559122809047318e-06. Current value: lambda_l2=1.4559122809047318e-06\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.4559122809047318e-06. Current value: lambda_l2=1.4559122809047318e-06\n",
      "[LightGBM] [Info] Number of positive: 162885, number of negative: 503781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7208\n",
      "[LightGBM] [Info] Number of data points in the train set: 666666, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.013043 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.4559122809047318e-06. Current value: lambda_l2=1.4559122809047318e-06\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244328 -> initscore=-1.129097\n",
      "[LightGBM] [Info] Start training from score -1.129097\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.4559122809047318e-06. Current value: lambda_l2=1.4559122809047318e-06\n",
      "===========fold 1================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.4559122809047318e-06. Current value: lambda_l2=1.4559122809047318e-06\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.4559122809047318e-06. Current value: lambda_l2=1.4559122809047318e-06\n",
      "[LightGBM] [Info] Number of positive: 162886, number of negative: 503781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7222\n",
      "[LightGBM] [Info] Number of data points in the train set: 666667, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.013289 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.4559122809047318e-06. Current value: lambda_l2=1.4559122809047318e-06\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244329 -> initscore=-1.129091\n",
      "[LightGBM] [Info] Start training from score -1.129091\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.4559122809047318e-06. Current value: lambda_l2=1.4559122809047318e-06\n",
      "===========fold 2================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.4559122809047318e-06. Current value: lambda_l2=1.4559122809047318e-06\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.4559122809047318e-06. Current value: lambda_l2=1.4559122809047318e-06\n",
      "[LightGBM] [Info] Number of positive: 162885, number of negative: 503782\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7207\n",
      "[LightGBM] [Info] Number of data points in the train set: 666667, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.012877 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.4559122809047318e-06. Current value: lambda_l2=1.4559122809047318e-06\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244327 -> initscore=-1.129099\n",
      "[LightGBM] [Info] Start training from score -1.129099\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1.4559122809047318e-06. Current value: lambda_l2=1.4559122809047318e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 18:12:58,903] Trial 38 finished with value: 0.9047875461819747 and parameters: {'learning_rate': 0.07326524605982208, 'max_depth': 4, 'colsample_bytree': 0.7000000000000001, 'subsample': 0.2, 'lambda': 1.4559122809047318e-06, 'alpha': 0.22021832224581964, 'hidden_layer_sizes': 5}. Best is trial 18 with value: 0.9058233890210564.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=3.073843666432696e-08. Current value: lambda_l2=3.073843666432696e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=3.073843666432696e-08. Current value: lambda_l2=3.073843666432696e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162885, number of negative: 503781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7208\n",
      "[LightGBM] [Info] Number of data points in the train set: 666666, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.013063 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=3.073843666432696e-08. Current value: lambda_l2=3.073843666432696e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244328 -> initscore=-1.129097\n",
      "[LightGBM] [Info] Start training from score -1.129097\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=3.073843666432696e-08. Current value: lambda_l2=3.073843666432696e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "===========fold 1================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=3.073843666432696e-08. Current value: lambda_l2=3.073843666432696e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=3.073843666432696e-08. Current value: lambda_l2=3.073843666432696e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162886, number of negative: 503781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7222\n",
      "[LightGBM] [Info] Number of data points in the train set: 666667, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.016736 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=3.073843666432696e-08. Current value: lambda_l2=3.073843666432696e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244329 -> initscore=-1.129091\n",
      "[LightGBM] [Info] Start training from score -1.129091\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=3.073843666432696e-08. Current value: lambda_l2=3.073843666432696e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "===========fold 2================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=3.073843666432696e-08. Current value: lambda_l2=3.073843666432696e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=3.073843666432696e-08. Current value: lambda_l2=3.073843666432696e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162885, number of negative: 503782\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7207\n",
      "[LightGBM] [Info] Number of data points in the train set: 666667, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.013424 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=3.073843666432696e-08. Current value: lambda_l2=3.073843666432696e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244327 -> initscore=-1.129099\n",
      "[LightGBM] [Info] Start training from score -1.129099\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=3.073843666432696e-08. Current value: lambda_l2=3.073843666432696e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 18:13:54,290] Trial 39 finished with value: 0.9056365391471367 and parameters: {'learning_rate': 0.05627045616667916, 'max_depth': 6, 'colsample_bytree': 1.0, 'subsample': 0.30000000000000004, 'lambda': 3.073843666432696e-08, 'alpha': 0.00012538197122412905, 'hidden_layer_sizes': 3}. Best is trial 18 with value: 0.9058233890210564.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=5.460014982773113e-07. Current value: lambda_l2=5.460014982773113e-07\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=5.460014982773113e-07. Current value: lambda_l2=5.460014982773113e-07\n",
      "[LightGBM] [Info] Number of positive: 162885, number of negative: 503781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7208\n",
      "[LightGBM] [Info] Number of data points in the train set: 666666, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.013717 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=5.460014982773113e-07. Current value: lambda_l2=5.460014982773113e-07\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244328 -> initscore=-1.129097\n",
      "[LightGBM] [Info] Start training from score -1.129097\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=5.460014982773113e-07. Current value: lambda_l2=5.460014982773113e-07\n",
      "===========fold 1================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=5.460014982773113e-07. Current value: lambda_l2=5.460014982773113e-07\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=5.460014982773113e-07. Current value: lambda_l2=5.460014982773113e-07\n",
      "[LightGBM] [Info] Number of positive: 162886, number of negative: 503781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7222\n",
      "[LightGBM] [Info] Number of data points in the train set: 666667, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.013315 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=5.460014982773113e-07. Current value: lambda_l2=5.460014982773113e-07\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244329 -> initscore=-1.129091\n",
      "[LightGBM] [Info] Start training from score -1.129091\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=5.460014982773113e-07. Current value: lambda_l2=5.460014982773113e-07\n",
      "===========fold 2================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=5.460014982773113e-07. Current value: lambda_l2=5.460014982773113e-07\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=5.460014982773113e-07. Current value: lambda_l2=5.460014982773113e-07\n",
      "[LightGBM] [Info] Number of positive: 162885, number of negative: 503782\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7207\n",
      "[LightGBM] [Info] Number of data points in the train set: 666667, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.012310 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=5.460014982773113e-07. Current value: lambda_l2=5.460014982773113e-07\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244327 -> initscore=-1.129099\n",
      "[LightGBM] [Info] Start training from score -1.129099\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=5.460014982773113e-07. Current value: lambda_l2=5.460014982773113e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 18:14:42,090] Trial 40 finished with value: 0.9040128784076021 and parameters: {'learning_rate': 0.08168069908107777, 'max_depth': 3, 'colsample_bytree': 0.9, 'subsample': 0.4, 'lambda': 5.460014982773113e-07, 'alpha': 0.05636484645829591, 'hidden_layer_sizes': 4}. Best is trial 18 with value: 0.9058233890210564.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=3.376112077382462e-08. Current value: lambda_l2=3.376112077382462e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=3.376112077382462e-08. Current value: lambda_l2=3.376112077382462e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162885, number of negative: 503781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7208\n",
      "[LightGBM] [Info] Number of data points in the train set: 666666, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.013387 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=3.376112077382462e-08. Current value: lambda_l2=3.376112077382462e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244328 -> initscore=-1.129097\n",
      "[LightGBM] [Info] Start training from score -1.129097\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=3.376112077382462e-08. Current value: lambda_l2=3.376112077382462e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "===========fold 1================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=3.376112077382462e-08. Current value: lambda_l2=3.376112077382462e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=3.376112077382462e-08. Current value: lambda_l2=3.376112077382462e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162886, number of negative: 503781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7222\n",
      "[LightGBM] [Info] Number of data points in the train set: 666667, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.015863 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=3.376112077382462e-08. Current value: lambda_l2=3.376112077382462e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244329 -> initscore=-1.129091\n",
      "[LightGBM] [Info] Start training from score -1.129091\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=3.376112077382462e-08. Current value: lambda_l2=3.376112077382462e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "===========fold 2================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=3.376112077382462e-08. Current value: lambda_l2=3.376112077382462e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=3.376112077382462e-08. Current value: lambda_l2=3.376112077382462e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162885, number of negative: 503782\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7207\n",
      "[LightGBM] [Info] Number of data points in the train set: 666667, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.013697 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=3.376112077382462e-08. Current value: lambda_l2=3.376112077382462e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244327 -> initscore=-1.129099\n",
      "[LightGBM] [Info] Start training from score -1.129099\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=3.376112077382462e-08. Current value: lambda_l2=3.376112077382462e-08\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 18:15:38,370] Trial 41 finished with value: 0.9057509741626136 and parameters: {'learning_rate': 0.08295682158503087, 'max_depth': 8, 'colsample_bytree': 1.0, 'subsample': 0.1, 'lambda': 3.376112077382462e-08, 'alpha': 0.5579709841639292, 'hidden_layer_sizes': 4}. Best is trial 18 with value: 0.9058233890210564.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.9648085464373999. Current value: lambda_l2=0.9648085464373999\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.9648085464373999. Current value: lambda_l2=0.9648085464373999\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162885, number of negative: 503781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7208\n",
      "[LightGBM] [Info] Number of data points in the train set: 666666, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.012335 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.9648085464373999. Current value: lambda_l2=0.9648085464373999\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244328 -> initscore=-1.129097\n",
      "[LightGBM] [Info] Start training from score -1.129097\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.9648085464373999. Current value: lambda_l2=0.9648085464373999\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "===========fold 1================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.9648085464373999. Current value: lambda_l2=0.9648085464373999\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.9648085464373999. Current value: lambda_l2=0.9648085464373999\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162886, number of negative: 503781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7222\n",
      "[LightGBM] [Info] Number of data points in the train set: 666667, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.013220 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.9648085464373999. Current value: lambda_l2=0.9648085464373999\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244329 -> initscore=-1.129091\n",
      "[LightGBM] [Info] Start training from score -1.129091\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.9648085464373999. Current value: lambda_l2=0.9648085464373999\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "===========fold 2================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.9648085464373999. Current value: lambda_l2=0.9648085464373999\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.9648085464373999. Current value: lambda_l2=0.9648085464373999\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162885, number of negative: 503782\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7207\n",
      "[LightGBM] [Info] Number of data points in the train set: 666667, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.013747 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.9648085464373999. Current value: lambda_l2=0.9648085464373999\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244327 -> initscore=-1.129099\n",
      "[LightGBM] [Info] Start training from score -1.129099\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.9648085464373999. Current value: lambda_l2=0.9648085464373999\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 18:16:31,205] Trial 42 finished with value: 0.9058925272147199 and parameters: {'learning_rate': 0.09249779656872704, 'max_depth': 8, 'colsample_bytree': 1.0, 'subsample': 0.1, 'lambda': 0.9648085464373999, 'alpha': 0.9903394475964249, 'hidden_layer_sizes': 4}. Best is trial 42 with value: 0.9058925272147199.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.0005761901108965973. Current value: lambda_l2=0.0005761901108965973\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.0005761901108965973. Current value: lambda_l2=0.0005761901108965973\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162885, number of negative: 503781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7208\n",
      "[LightGBM] [Info] Number of data points in the train set: 666666, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.012864 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.0005761901108965973. Current value: lambda_l2=0.0005761901108965973\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244328 -> initscore=-1.129097\n",
      "[LightGBM] [Info] Start training from score -1.129097\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.0005761901108965973. Current value: lambda_l2=0.0005761901108965973\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "===========fold 1================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.0005761901108965973. Current value: lambda_l2=0.0005761901108965973\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.0005761901108965973. Current value: lambda_l2=0.0005761901108965973\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162886, number of negative: 503781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7222\n",
      "[LightGBM] [Info] Number of data points in the train set: 666667, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.011869 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.0005761901108965973. Current value: lambda_l2=0.0005761901108965973\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244329 -> initscore=-1.129091\n",
      "[LightGBM] [Info] Start training from score -1.129091\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.0005761901108965973. Current value: lambda_l2=0.0005761901108965973\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "===========fold 2================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.0005761901108965973. Current value: lambda_l2=0.0005761901108965973\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.0005761901108965973. Current value: lambda_l2=0.0005761901108965973\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162885, number of negative: 503782\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7207\n",
      "[LightGBM] [Info] Number of data points in the train set: 666667, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.013104 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.0005761901108965973. Current value: lambda_l2=0.0005761901108965973\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244327 -> initscore=-1.129099\n",
      "[LightGBM] [Info] Start training from score -1.129099\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.0005761901108965973. Current value: lambda_l2=0.0005761901108965973\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 18:17:24,734] Trial 43 finished with value: 0.9057367904520089 and parameters: {'learning_rate': 0.09167804917499879, 'max_depth': 9, 'colsample_bytree': 0.8, 'subsample': 0.1, 'lambda': 0.0005761901108965973, 'alpha': 0.20173981955145184, 'hidden_layer_sizes': 4}. Best is trial 42 with value: 0.9058925272147199.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=3.275169529809201e-06. Current value: lambda_l2=3.275169529809201e-06\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=3.275169529809201e-06. Current value: lambda_l2=3.275169529809201e-06\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162885, number of negative: 503781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7208\n",
      "[LightGBM] [Info] Number of data points in the train set: 666666, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.012856 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=3.275169529809201e-06. Current value: lambda_l2=3.275169529809201e-06\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244328 -> initscore=-1.129097\n",
      "[LightGBM] [Info] Start training from score -1.129097\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=3.275169529809201e-06. Current value: lambda_l2=3.275169529809201e-06\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "===========fold 1================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=3.275169529809201e-06. Current value: lambda_l2=3.275169529809201e-06\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=3.275169529809201e-06. Current value: lambda_l2=3.275169529809201e-06\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162886, number of negative: 503781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7222\n",
      "[LightGBM] [Info] Number of data points in the train set: 666667, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.013255 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=3.275169529809201e-06. Current value: lambda_l2=3.275169529809201e-06\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244329 -> initscore=-1.129091\n",
      "[LightGBM] [Info] Start training from score -1.129091\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=3.275169529809201e-06. Current value: lambda_l2=3.275169529809201e-06\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "===========fold 2================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=3.275169529809201e-06. Current value: lambda_l2=3.275169529809201e-06\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=3.275169529809201e-06. Current value: lambda_l2=3.275169529809201e-06\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162885, number of negative: 503782\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7207\n",
      "[LightGBM] [Info] Number of data points in the train set: 666667, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.013596 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=3.275169529809201e-06. Current value: lambda_l2=3.275169529809201e-06\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244327 -> initscore=-1.129099\n",
      "[LightGBM] [Info] Start training from score -1.129099\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=3.275169529809201e-06. Current value: lambda_l2=3.275169529809201e-06\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 18:18:18,220] Trial 44 finished with value: 0.9057753485827501 and parameters: {'learning_rate': 0.07390124119355176, 'max_depth': 7, 'colsample_bytree': 0.9, 'subsample': 0.2, 'lambda': 3.275169529809201e-06, 'alpha': 0.07736285143719192, 'hidden_layer_sizes': 4}. Best is trial 42 with value: 0.9058925272147199.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.012377560159709346. Current value: lambda_l2=0.012377560159709346\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.012377560159709346. Current value: lambda_l2=0.012377560159709346\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162885, number of negative: 503781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7208\n",
      "[LightGBM] [Info] Number of data points in the train set: 666666, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.013191 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.012377560159709346. Current value: lambda_l2=0.012377560159709346\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244328 -> initscore=-1.129097\n",
      "[LightGBM] [Info] Start training from score -1.129097\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.012377560159709346. Current value: lambda_l2=0.012377560159709346\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "===========fold 1================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.012377560159709346. Current value: lambda_l2=0.012377560159709346\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.012377560159709346. Current value: lambda_l2=0.012377560159709346\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162886, number of negative: 503781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7222\n",
      "[LightGBM] [Info] Number of data points in the train set: 666667, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.013079 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.012377560159709346. Current value: lambda_l2=0.012377560159709346\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244329 -> initscore=-1.129091\n",
      "[LightGBM] [Info] Start training from score -1.129091\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.012377560159709346. Current value: lambda_l2=0.012377560159709346\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "===========fold 2================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.012377560159709346. Current value: lambda_l2=0.012377560159709346\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.012377560159709346. Current value: lambda_l2=0.012377560159709346\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162885, number of negative: 503782\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7207\n",
      "[LightGBM] [Info] Number of data points in the train set: 666667, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.013088 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.012377560159709346. Current value: lambda_l2=0.012377560159709346\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244327 -> initscore=-1.129099\n",
      "[LightGBM] [Info] Start training from score -1.129099\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.012377560159709346. Current value: lambda_l2=0.012377560159709346\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 18:19:11,719] Trial 45 finished with value: 0.9057837778580943 and parameters: {'learning_rate': 0.0933558716158065, 'max_depth': 8, 'colsample_bytree': 1.0, 'subsample': 0.1, 'lambda': 0.012377560159709346, 'alpha': 0.3004894680181134, 'hidden_layer_sizes': 5}. Best is trial 42 with value: 0.9058925272147199.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.40465715533799107. Current value: lambda_l2=0.40465715533799107\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.40465715533799107. Current value: lambda_l2=0.40465715533799107\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162885, number of negative: 503781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7208\n",
      "[LightGBM] [Info] Number of data points in the train set: 666666, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.013247 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.40465715533799107. Current value: lambda_l2=0.40465715533799107\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244328 -> initscore=-1.129097\n",
      "[LightGBM] [Info] Start training from score -1.129097\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.40465715533799107. Current value: lambda_l2=0.40465715533799107\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "===========fold 1================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.40465715533799107. Current value: lambda_l2=0.40465715533799107\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.40465715533799107. Current value: lambda_l2=0.40465715533799107\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162886, number of negative: 503781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7222\n",
      "[LightGBM] [Info] Number of data points in the train set: 666667, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.012818 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.40465715533799107. Current value: lambda_l2=0.40465715533799107\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244329 -> initscore=-1.129091\n",
      "[LightGBM] [Info] Start training from score -1.129091\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.40465715533799107. Current value: lambda_l2=0.40465715533799107\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "===========fold 2================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.40465715533799107. Current value: lambda_l2=0.40465715533799107\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.40465715533799107. Current value: lambda_l2=0.40465715533799107\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162885, number of negative: 503782\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7207\n",
      "[LightGBM] [Info] Number of data points in the train set: 666667, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.012930 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.40465715533799107. Current value: lambda_l2=0.40465715533799107\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244327 -> initscore=-1.129099\n",
      "[LightGBM] [Info] Start training from score -1.129099\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.40465715533799107. Current value: lambda_l2=0.40465715533799107\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 18:20:12,624] Trial 46 finished with value: 0.9055779424791183 and parameters: {'learning_rate': 0.07622320040387551, 'max_depth': 9, 'colsample_bytree': 0.30000000000000004, 'subsample': 0.2, 'lambda': 0.40465715533799107, 'alpha': 0.0019718033009214903, 'hidden_layer_sizes': 4}. Best is trial 42 with value: 0.9058925272147199.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=2.2468709820880756e-07. Current value: lambda_l2=2.2468709820880756e-07\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=2.2468709820880756e-07. Current value: lambda_l2=2.2468709820880756e-07\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162885, number of negative: 503781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7208\n",
      "[LightGBM] [Info] Number of data points in the train set: 666666, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.012847 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=2.2468709820880756e-07. Current value: lambda_l2=2.2468709820880756e-07\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244328 -> initscore=-1.129097\n",
      "[LightGBM] [Info] Start training from score -1.129097\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=2.2468709820880756e-07. Current value: lambda_l2=2.2468709820880756e-07\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "===========fold 1================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=2.2468709820880756e-07. Current value: lambda_l2=2.2468709820880756e-07\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=2.2468709820880756e-07. Current value: lambda_l2=2.2468709820880756e-07\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162886, number of negative: 503781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7222\n",
      "[LightGBM] [Info] Number of data points in the train set: 666667, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.013199 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=2.2468709820880756e-07. Current value: lambda_l2=2.2468709820880756e-07\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244329 -> initscore=-1.129091\n",
      "[LightGBM] [Info] Start training from score -1.129091\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=2.2468709820880756e-07. Current value: lambda_l2=2.2468709820880756e-07\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "===========fold 2================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=2.2468709820880756e-07. Current value: lambda_l2=2.2468709820880756e-07\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=2.2468709820880756e-07. Current value: lambda_l2=2.2468709820880756e-07\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162885, number of negative: 503782\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7207\n",
      "[LightGBM] [Info] Number of data points in the train set: 666667, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.013459 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=2.2468709820880756e-07. Current value: lambda_l2=2.2468709820880756e-07\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244327 -> initscore=-1.129099\n",
      "[LightGBM] [Info] Start training from score -1.129099\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=2.2468709820880756e-07. Current value: lambda_l2=2.2468709820880756e-07\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 18:21:07,368] Trial 47 finished with value: 0.9057330797070359 and parameters: {'learning_rate': 0.05882903121984253, 'max_depth': 7, 'colsample_bytree': 0.8, 'subsample': 0.30000000000000004, 'lambda': 2.2468709820880756e-07, 'alpha': 0.016260091444293943, 'hidden_layer_sizes': 3}. Best is trial 42 with value: 0.9058925272147199.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.00019999312851095498. Current value: lambda_l2=0.00019999312851095498\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.00019999312851095498. Current value: lambda_l2=0.00019999312851095498\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162885, number of negative: 503781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7208\n",
      "[LightGBM] [Info] Number of data points in the train set: 666666, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.013669 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.00019999312851095498. Current value: lambda_l2=0.00019999312851095498\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244328 -> initscore=-1.129097\n",
      "[LightGBM] [Info] Start training from score -1.129097\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.00019999312851095498. Current value: lambda_l2=0.00019999312851095498\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "===========fold 1================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.00019999312851095498. Current value: lambda_l2=0.00019999312851095498\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.00019999312851095498. Current value: lambda_l2=0.00019999312851095498\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162886, number of negative: 503781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7222\n",
      "[LightGBM] [Info] Number of data points in the train set: 666667, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.012767 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.00019999312851095498. Current value: lambda_l2=0.00019999312851095498\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244329 -> initscore=-1.129091\n",
      "[LightGBM] [Info] Start training from score -1.129091\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.00019999312851095498. Current value: lambda_l2=0.00019999312851095498\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "===========fold 2================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.00019999312851095498. Current value: lambda_l2=0.00019999312851095498\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.00019999312851095498. Current value: lambda_l2=0.00019999312851095498\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162885, number of negative: 503782\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7207\n",
      "[LightGBM] [Info] Number of data points in the train set: 666667, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.013082 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.00019999312851095498. Current value: lambda_l2=0.00019999312851095498\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244327 -> initscore=-1.129099\n",
      "[LightGBM] [Info] Start training from score -1.129099\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.00019999312851095498. Current value: lambda_l2=0.00019999312851095498\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 18:22:02,830] Trial 48 finished with value: 0.9052558080254807 and parameters: {'learning_rate': 0.02705225481126833, 'max_depth': 8, 'colsample_bytree': 0.9, 'subsample': 0.2, 'lambda': 0.00019999312851095498, 'alpha': 0.03994262151801138, 'hidden_layer_sizes': 2}. Best is trial 42 with value: 0.9058925272147199.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.9532901287692351. Current value: lambda_l2=0.9532901287692351\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.9532901287692351. Current value: lambda_l2=0.9532901287692351\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162885, number of negative: 503781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7208\n",
      "[LightGBM] [Info] Number of data points in the train set: 666666, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.012626 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.9532901287692351. Current value: lambda_l2=0.9532901287692351\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244328 -> initscore=-1.129097\n",
      "[LightGBM] [Info] Start training from score -1.129097\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.9532901287692351. Current value: lambda_l2=0.9532901287692351\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "===========fold 1================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.9532901287692351. Current value: lambda_l2=0.9532901287692351\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.9532901287692351. Current value: lambda_l2=0.9532901287692351\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162886, number of negative: 503781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7222\n",
      "[LightGBM] [Info] Number of data points in the train set: 666667, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.013383 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.9532901287692351. Current value: lambda_l2=0.9532901287692351\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244329 -> initscore=-1.129091\n",
      "[LightGBM] [Info] Start training from score -1.129091\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.9532901287692351. Current value: lambda_l2=0.9532901287692351\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "===========fold 2================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.9532901287692351. Current value: lambda_l2=0.9532901287692351\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.9532901287692351. Current value: lambda_l2=0.9532901287692351\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162885, number of negative: 503782\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7207\n",
      "[LightGBM] [Info] Number of data points in the train set: 666667, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.013573 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.9532901287692351. Current value: lambda_l2=0.9532901287692351\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244327 -> initscore=-1.129099\n",
      "[LightGBM] [Info] Start training from score -1.129099\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.9532901287692351. Current value: lambda_l2=0.9532901287692351\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 18:23:00,697] Trial 49 finished with value: 0.9058059034247047 and parameters: {'learning_rate': 0.04811883709715961, 'max_depth': 9, 'colsample_bytree': 0.7000000000000001, 'subsample': 0.1, 'lambda': 0.9532901287692351, 'alpha': 5.961727545824861e-07, 'hidden_layer_sizes': 4}. Best is trial 42 with value: 0.9058925272147199.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of finished trials: 50\n",
      "Best trial:\n",
      "  Value: 0.9058925272147199\n",
      "  Params: \n",
      "    learning_rate: 0.09249779656872704\n",
      "    max_depth: 8\n",
      "    colsample_bytree: 1.0\n",
      "    subsample: 0.1\n",
      "    lambda: 0.9648085464373999\n",
      "    alpha: 0.9903394475964249\n",
      "    hidden_layer_sizes: 4\n"
     ]
    }
   ],
   "source": [
    "pruner = optuna.pruners.MedianPruner(n_warmup_steps=5)\n",
    "study = optuna.create_study(study_name='lgb',pruner=pruner, direction=\"maximize\")\n",
    "study.optimize(lambda trial: objective(trial, train=train, model_name='lgb'), n_trials=50)\n",
    "\n",
    "print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))\n",
    "all_trials['lgb'] = trial.params.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 11:07:01,386] A new study created in memory with name: mlp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "MLP------------------\n",
      "===========fold 1================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "MLP------------------\n",
      "===========fold 2================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "MLP------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 11:15:24,162] Trial 0 finished with value: 0.899397668877507 and parameters: {'learning_rate': 0.06293631247951315, 'max_depth': 4, 'colsample_bytree': 1.0, 'subsample': 0.2, 'lambda': 0.655164120003187, 'alpha': 0.00023388054209915076, 'hidden_layer_sizes': 2}. Best is trial 0 with value: 0.899397668877507.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "MLP------------------\n",
      "===========fold 1================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "MLP------------------\n",
      "===========fold 2================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "MLP------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 11:26:21,126] Trial 1 finished with value: 0.9011388708201604 and parameters: {'learning_rate': 0.010062311420025734, 'max_depth': 9, 'colsample_bytree': 0.7000000000000001, 'subsample': 0.8, 'lambda': 2.808309456046311e-05, 'alpha': 0.004971233814508131, 'hidden_layer_sizes': 3}. Best is trial 1 with value: 0.9011388708201604.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "MLP------------------\n",
      "===========fold 1================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "MLP------------------\n",
      "===========fold 2================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "MLP------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 11:35:37,976] Trial 2 finished with value: 0.9020526070928083 and parameters: {'learning_rate': 0.09489887655374188, 'max_depth': 5, 'colsample_bytree': 0.8, 'subsample': 0.1, 'lambda': 0.000231391542151867, 'alpha': 0.6459845987300605, 'hidden_layer_sizes': 5}. Best is trial 2 with value: 0.9020526070928083.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "MLP------------------\n",
      "===========fold 1================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "MLP------------------\n",
      "===========fold 2================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "MLP------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 11:46:46,158] Trial 3 finished with value: 0.9011388708201604 and parameters: {'learning_rate': 0.09842512740797268, 'max_depth': 8, 'colsample_bytree': 0.8, 'subsample': 0.8, 'lambda': 4.818206887164953e-08, 'alpha': 0.0028997777188547818, 'hidden_layer_sizes': 3}. Best is trial 2 with value: 0.9020526070928083.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "MLP------------------\n",
      "===========fold 1================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "MLP------------------\n",
      "===========fold 2================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "MLP------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 11:55:07,013] Trial 4 finished with value: 0.899397668877507 and parameters: {'learning_rate': 0.05091998521371539, 'max_depth': 3, 'colsample_bytree': 1.0, 'subsample': 0.7000000000000001, 'lambda': 0.02626228296381512, 'alpha': 0.003974310254759571, 'hidden_layer_sizes': 2}. Best is trial 2 with value: 0.9020526070928083.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "MLP------------------\n",
      "===========fold 1================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "MLP------------------\n",
      "===========fold 2================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "MLP------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 12:04:19,448] Trial 5 finished with value: 0.9020526070928083 and parameters: {'learning_rate': 0.04851659008996471, 'max_depth': 8, 'colsample_bytree': 0.6, 'subsample': 0.1, 'lambda': 2.2435188661170915e-06, 'alpha': 0.12302118983218546, 'hidden_layer_sizes': 5}. Best is trial 2 with value: 0.9020526070928083.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "MLP------------------\n",
      "===========fold 1================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "MLP------------------\n",
      "===========fold 2================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "MLP------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 12:15:16,260] Trial 6 finished with value: 0.9011388708201604 and parameters: {'learning_rate': 0.062101346068810785, 'max_depth': 8, 'colsample_bytree': 0.2, 'subsample': 0.1, 'lambda': 0.12601868830455942, 'alpha': 2.5824396852552612e-05, 'hidden_layer_sizes': 3}. Best is trial 2 with value: 0.9020526070928083.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "MLP------------------\n",
      "===========fold 1================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "MLP------------------\n",
      "===========fold 2================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "MLP------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 12:23:30,459] Trial 7 finished with value: 0.899397668877507 and parameters: {'learning_rate': 0.08304722063512877, 'max_depth': 8, 'colsample_bytree': 0.30000000000000004, 'subsample': 0.4, 'lambda': 1.0901654766421031e-06, 'alpha': 3.73977657268166e-07, 'hidden_layer_sizes': 2}. Best is trial 2 with value: 0.9020526070928083.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "MLP------------------\n",
      "===========fold 1================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "MLP------------------\n",
      "===========fold 2================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "MLP------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 12:34:26,546] Trial 8 finished with value: 0.9011388708201604 and parameters: {'learning_rate': 0.06316039443312342, 'max_depth': 7, 'colsample_bytree': 0.4, 'subsample': 0.7000000000000001, 'lambda': 5.856084433639924e-08, 'alpha': 2.8607022055207455e-06, 'hidden_layer_sizes': 3}. Best is trial 2 with value: 0.9020526070928083.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "MLP------------------\n",
      "===========fold 1================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "MLP------------------\n",
      "===========fold 2================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "MLP------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 12:43:36,136] Trial 9 finished with value: 0.9020526070928083 and parameters: {'learning_rate': 0.08910810163144461, 'max_depth': 5, 'colsample_bytree': 0.4, 'subsample': 0.5, 'lambda': 0.23820714340564797, 'alpha': 5.482588542846957e-06, 'hidden_layer_sizes': 5}. Best is trial 2 with value: 0.9020526070928083.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of finished trials: 10\n",
      "Best trial:\n",
      "  Value: 0.9020526070928083\n",
      "  Params: \n",
      "    learning_rate: 0.09489887655374188\n",
      "    max_depth: 5\n",
      "    colsample_bytree: 0.8\n",
      "    subsample: 0.1\n",
      "    lambda: 0.000231391542151867\n",
      "    alpha: 0.6459845987300605\n",
      "    hidden_layer_sizes: 5\n"
     ]
    }
   ],
   "source": [
    "pruner = optuna.pruners.MedianPruner(n_warmup_steps=5)\n",
    "study = optuna.create_study(study_name='mlp',pruner=pruner, direction=\"maximize\")\n",
    "study.optimize(lambda trial: objective(trial, train=train, model_name='mlp'), n_trials=10)\n",
    "\n",
    "print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))\n",
    "all_trials['mlp'] = trial.params.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 12:43:36,148] A new study created in memory with name: rf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "Random Forest--------------\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " best_threshold 0.3899999999999999 best_score 0.7823217623466623\n",
      "===========fold 1================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "Random Forest--------------\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " best_threshold 0.3899999999999999 best_score 0.78181688484192\n",
      "===========fold 2================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "Random Forest--------------\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 13:21:12,531] Trial 0 finished with value: 0.8989454023697793 and parameters: {'learning_rate': 0.018831971923188563, 'max_depth': 6, 'colsample_bytree': 1.0, 'subsample': 0.9, 'lambda': 2.7910822587618976e-08, 'alpha': 1.9404937284683135e-07, 'hidden_layer_sizes': 5}. Best is trial 0 with value: 0.8989454023697793.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " best_threshold 0.4099999999999998 best_score 0.7813584856597724\n",
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "Random Forest--------------\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " best_threshold 0.3699999999999999 best_score 0.7575495505019685\n",
      "===========fold 1================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "Random Forest--------------\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " best_threshold 0.34999999999999987 best_score 0.7593920615630854\n",
      "===========fold 2================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "Random Forest--------------\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 13:23:17,470] Trial 1 finished with value: 0.8699741711520782 and parameters: {'learning_rate': 0.01737306235084337, 'max_depth': 1, 'colsample_bytree': 0.2, 'subsample': 0.1, 'lambda': 8.915607778059581e-08, 'alpha': 2.672849991271086e-05, 'hidden_layer_sizes': 5}. Best is trial 0 with value: 0.8989454023697793.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " best_threshold 0.33999999999999986 best_score 0.7600087522644281\n",
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "Random Forest--------------\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " best_threshold 0.3899999999999999 best_score 0.7831583912790232\n",
      "===========fold 1================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "Random Forest--------------\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " best_threshold 0.3899999999999999 best_score 0.7830333467152208\n",
      "===========fold 2================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "Random Forest--------------\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 13:53:37,559] Trial 2 finished with value: 0.8997400712471219 and parameters: {'learning_rate': 0.06689115993855875, 'max_depth': 6, 'colsample_bytree': 0.8, 'subsample': 0.2, 'lambda': 1.820820713660828e-05, 'alpha': 5.495465727143108e-06, 'hidden_layer_sizes': 4}. Best is trial 2 with value: 0.8997400712471219.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.89, \n",
      " best_threshold 0.3999999999999998 best_score 0.7823997168365828\n",
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "Random Forest--------------\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " best_threshold 0.3599999999999999 best_score 0.7714517322305086\n",
      "===========fold 1================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "Random Forest--------------\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " best_threshold 0.34999999999999987 best_score 0.770539209070608\n",
      "===========fold 2================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "Random Forest--------------\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 14:00:51,050] Trial 3 finished with value: 0.8919047862107998 and parameters: {'learning_rate': 0.09145222077357028, 'max_depth': 5, 'colsample_bytree': 0.2, 'subsample': 0.4, 'lambda': 0.810452055046723, 'alpha': 0.052003309861066944, 'hidden_layer_sizes': 5}. Best is trial 2 with value: 0.8997400712471219.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " best_threshold 0.33999999999999986 best_score 0.7721737948552756\n",
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "Random Forest--------------\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " best_threshold 0.3999999999999998 best_score 0.7862909774753857\n",
      "===========fold 1================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "Random Forest--------------\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " best_threshold 0.3899999999999999 best_score 0.7865954195192042\n",
      "===========fold 2================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "Random Forest--------------\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 14:29:40,931] Trial 4 finished with value: 0.9035132069272324 and parameters: {'learning_rate': 0.060492102527360606, 'max_depth': 9, 'colsample_bytree': 0.5, 'subsample': 0.2, 'lambda': 0.12619375123209245, 'alpha': 2.3009010377609224e-07, 'hidden_layer_sizes': 2}. Best is trial 4 with value: 0.9035132069272324.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " best_threshold 0.3999999999999998 best_score 0.7863975836251148\n",
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "Random Forest--------------\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " best_threshold 0.3899999999999999 best_score 0.7841798334260776\n",
      "===========fold 1================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "Random Forest--------------\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " best_threshold 0.3799999999999999 best_score 0.784288042000806\n",
      "===========fold 2================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "Random Forest--------------\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " best_threshold 0.3899999999999999 best_score 0.7839988833588984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 15:05:21,413] Trial 5 finished with value: 0.9014201024421329 and parameters: {'learning_rate': 0.020094515844559957, 'max_depth': 7, 'colsample_bytree': 0.8, 'subsample': 0.7000000000000001, 'lambda': 4.635165264658297e-07, 'alpha': 0.973407303914982, 'hidden_layer_sizes': 5}. Best is trial 4 with value: 0.9035132069272324.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "Random Forest--------------\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " best_threshold 0.3599999999999999 best_score 0.7717744331369092\n",
      "===========fold 1================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "Random Forest--------------\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " best_threshold 0.3899999999999999 best_score 0.7729951384487626\n",
      "===========fold 2================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "Random Forest--------------\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 15:18:36,970] Trial 6 finished with value: 0.8900416763078214 and parameters: {'learning_rate': 0.010875520739807602, 'max_depth': 3, 'colsample_bytree': 0.7000000000000001, 'subsample': 0.9, 'lambda': 0.00015993806012210444, 'alpha': 0.2866783754845834, 'hidden_layer_sizes': 3}. Best is trial 4 with value: 0.9035132069272324.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.89, \n",
      " best_threshold 0.3799999999999999 best_score 0.7718566999373144\n",
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "Random Forest--------------\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " best_threshold 0.3999999999999998 best_score 0.7857761921322288\n",
      "===========fold 1================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "Random Forest--------------\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " best_threshold 0.3799999999999999 best_score 0.7858533886597026\n",
      "===========fold 2================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "Random Forest--------------\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " best_threshold 0.3999999999999998 best_score 0.7859687558883741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 16:16:31,673] Trial 7 finished with value: 0.9033255320529374 and parameters: {'learning_rate': 0.023853905208365653, 'max_depth': 9, 'colsample_bytree': 1.0, 'subsample': 1.0, 'lambda': 0.03049450385660734, 'alpha': 3.6734536619911483e-07, 'hidden_layer_sizes': 5}. Best is trial 4 with value: 0.9035132069272324.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "Random Forest--------------\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " best_threshold 0.3699999999999999 best_score 0.7646882652541831\n",
      "===========fold 1================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "Random Forest--------------\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " best_threshold 0.3699999999999999 best_score 0.7644039423970941\n",
      "===========fold 2================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "Random Forest--------------\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " best_threshold 0.3699999999999999 best_score 0.7655294022945918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 16:22:21,254] Trial 8 finished with value: 0.8873885094856253 and parameters: {'learning_rate': 0.06175620628899644, 'max_depth': 4, 'colsample_bytree': 0.2, 'subsample': 0.5, 'lambda': 7.032313620107411e-07, 'alpha': 0.02130630658983126, 'hidden_layer_sizes': 2}. Best is trial 4 with value: 0.9035132069272324.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "Random Forest--------------\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " best_threshold 0.3799999999999999 best_score 0.764260761290132\n",
      "===========fold 1================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "Random Forest--------------\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " best_threshold 0.3799999999999999 best_score 0.763650920012642\n",
      "===========fold 2================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "Random Forest--------------\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-26 16:27:15,478] Trial 9 finished with value: 0.8869749680246688 and parameters: {'learning_rate': 0.09008829942733483, 'max_depth': 6, 'colsample_bytree': 0.1, 'subsample': 1.0, 'lambda': 6.2181601318806825e-06, 'alpha': 1.976675428290502e-06, 'hidden_layer_sizes': 4}. Best is trial 4 with value: 0.9035132069272324.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.89, \n",
      " best_threshold 0.3599999999999999 best_score 0.7641913373592009\n",
      "Number of finished trials: 10\n",
      "Best trial:\n",
      "  Value: 0.9035132069272324\n",
      "  Params: \n",
      "    learning_rate: 0.060492102527360606\n",
      "    max_depth: 9\n",
      "    colsample_bytree: 0.5\n",
      "    subsample: 0.2\n",
      "    lambda: 0.12619375123209245\n",
      "    alpha: 2.3009010377609224e-07\n",
      "    hidden_layer_sizes: 2\n"
     ]
    }
   ],
   "source": [
    "pruner = optuna.pruners.MedianPruner(n_warmup_steps=5)\n",
    "study = optuna.create_study(study_name='rf',pruner=pruner, direction=\"maximize\")\n",
    "study.optimize(lambda trial: objective(trial, train=train, model_name='rf'), n_trials=10)\n",
    "\n",
    "print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))\n",
    "all_trials['rf'] = trial.params.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_items = {}\n",
    "xgb_items = {}\n",
    "rf_items = {}\n",
    "mlp_items = {}\n",
    "for k,v in all_trials['lgb']:\n",
    "    lgb_items[k] = v\n",
    "for k,v in all_trials['xgb']:\n",
    "    xgb_items[k] = v\n",
    "for k,v in all_trials['rf']:\n",
    "    rf_items[k] = v\n",
    "for k,v in all_trials['mlp']:\n",
    "    mlp_items[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('learning_rate', 0.09500314347224413), ('max_depth', 7), ('colsample_bytree', 0.2), ('subsample', 0.9), ('lambda', 0.027332136849109373), ('alpha', 0.0006337507915634194), ('hidden_layer_sizes', 2)])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_trials['xgb']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('learning_rate', 0.060492102527360606), ('max_depth', 9), ('colsample_bytree', 0.5), ('subsample', 0.2), ('lambda', 0.12619375123209245), ('alpha', 2.3009010377609224e-07), ('hidden_layer_sizes', 2)])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_trials['rf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.09249779656872704,\n",
       " 'max_depth': 8,\n",
       " 'colsample_bytree': 1.0,\n",
       " 'subsample': 0.1,\n",
       " 'lambda': 0.9648085464373999,\n",
       " 'alpha': 0.9903394475964249,\n",
       " 'hidden_layer_sizes': 4}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgb_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.034630277480196384,\n",
       " 'max_depth': 9,\n",
       " 'colsample_bytree': 0.8,\n",
       " 'subsample': 0.30000000000000004,\n",
       " 'lambda': 1.3270228907353322e-06,\n",
       " 'alpha': 0.0020136244579038245,\n",
       " 'hidden_layer_sizes': 3}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.060492102527360606,\n",
       " 'max_depth': 9,\n",
       " 'colsample_bytree': 0.5,\n",
       " 'subsample': 0.2,\n",
       " 'lambda': 0.12619375123209245,\n",
       " 'alpha': 2.3009010377609224e-07,\n",
       " 'hidden_layer_sizes': 2}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_ESTIMATORS = 200\n",
    "SEED=42\n",
    "LGBM_Hyperparameters =  {\n",
    "    \"n_estimators\": N_ESTIMATORS,\n",
    "    'learning_rate':lgb_items['learning_rate'],\n",
    "     'max_depth':lgb_items['max_depth'],\n",
    "     'colsample_bytree':lgb_items['colsample_bytree'],\n",
    "     'subsample':lgb_items['subsample'],\n",
    "     'alpha':lgb_items['alpha'],\n",
    "     'lambda':lgb_items['lambda'],\n",
    "\n",
    "    \"random_state\":SEED,\n",
    "    'device':'gpu',\n",
    "#     \"class_weight\": \"balanced\"\n",
    "}\n",
    "\n",
    "XGBoost_Hyperparameters = {\n",
    "    'objective' : 'binary:logistic',\n",
    "     'eval_metric':['logloss', 'auc'],\n",
    "     'n_estimators':N_ESTIMATORS,\n",
    "     'learning_rate': xgb_items['learning_rate'],\n",
    "     'max_depth':int(xgb_items['max_depth']),\n",
    "     'colsample_bytree': xgb_items['colsample_bytree'],\n",
    "     'subsample':xgb_items['subsample'],\n",
    "     'lambda': xgb_items['lambda'],\n",
    "     'alpha':xgb_items['alpha'],\n",
    "     'seed':SEED,\n",
    "     # 'scale_pos_weight':3,\n",
    "     'enable_categorical':True,\n",
    "     'early_stopping_rounds': 50,\n",
    "     'tree_method':'gpu_hist'}\n",
    "RF_Hyperparameters = {\n",
    "    'n_estimators': N_ESTIMATORS,\n",
    "    'max_depth':rf_items['max_depth'],\n",
    "    'random_state':SEED,\n",
    "    'max_features': rf_items['colsample_bytree'], \n",
    "    'n_jobs': -1\n",
    "}\n",
    "Logreg_Hyperparameters = {'max_iter':N_ESTIMATORS,'random_state':SEED} \n",
    "MLP_Hyperparametesr = {'hidden_layer_sizes':(mlp_items['hidden_layer_sizes'],125), 'random_state':SEED, 'max_iter':min(N_ESTIMATORS,100)}\n",
    "kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=SEED)\n",
    "\n",
    "def cross_validate(train, USE_SMOTE=False, USE_CLASS_WEIGHT=False, USE_UNDER_SAMPLING=False):\n",
    "    oofs = np.zeros((train.shape[0], len(MODEL_NAMES)))\n",
    "    for i, (train_index, valid_index) in enumerate(kfold.split(train, train[TARGET])):\n",
    "        print(f\"===========fold {i}================\")\n",
    "        X_train, oh_encoder, robust_scaler = process_data(train.iloc[train_index])\n",
    "        X_valid, _, _  = process_data(train.iloc[valid_index], oh_encoder,robust_scaler)\n",
    "        print(X_train.isnull().sum())\n",
    "        y_train = train.iloc[train_index][TARGET].values\n",
    "        y_valid = train.iloc[valid_index][TARGET].values\n",
    "        logreg_hyperparameters = Logreg_Hyperparameters.copy()\n",
    "        lgb_hyperparameters = LGBM_Hyperparameters.copy()\n",
    "        xgboost_hyperparameters = XGBoost_Hyperparameters.copy()\n",
    "        rf_hyperparameters = RF_Hyperparameters.copy()\n",
    "        mlp_hyperparameters = MLP_Hyperparametesr.copy()\n",
    "        if USE_SMOTE:\n",
    "            print(\"SMOTEEEE\")\n",
    "            sm = SMOTE(random_state=42)\n",
    "            X_train, y_train = sm.fit_resample(X_train, y_train)\n",
    "        elif USE_CLASS_WEIGHT:\n",
    "            print(\"CLASS_WEIGHTTTT\")\n",
    "           \n",
    "            class_weights = compute_class_weight('balanced', classes=np.array([0, 1]), y=y_train)\n",
    "            class_weights =  {0: class_weights[0], 1: class_weights[1]}\n",
    "            lgb_hyperparameters['class_weight'] = class_weights\n",
    "            logreg_hyperparameters['class_weight'] = class_weights\n",
    "            xgboost_hyperparameters['scale_pos_weight'] = class_weights[1] /  class_weights[0]\n",
    "            rf_hyperparameters['class_weight'] = class_weights\n",
    "        elif USE_UNDER_SAMPLING:\n",
    "            print(\"UNDER SAMPLING\")\n",
    "            rus = RandomUnderSampler(random_state=42)\n",
    "            X_train, y_train = rus.fit_resample(X_train, y_train)\n",
    "        print(\"LOGREG--------------\")\n",
    "        logreg_model = LogisticRegression(**logreg_hyperparameters)\n",
    "        logreg_model.fit(X_train, y_train)\n",
    "        logreg_y_pred_proba = logreg_model.predict_proba(X_valid)[:,1]\n",
    "        best_threshold, best_score = find_best_threshold_f1_score(y_valid, logreg_y_pred_proba)\n",
    "        print(f\"\\n best_threshold {best_threshold} best_score {best_score}\")\n",
    "        y_pred = [1 if y_hat >=best_threshold else 0 for y_hat in logreg_y_pred_proba]\n",
    "\n",
    "        print(roc_auc_score(y_valid, logreg_y_pred_proba))\n",
    "\n",
    "        print(classification_report(y_valid, y_pred, digits=4))\n",
    "        oofs[valid_index,0] = logreg_y_pred_proba\n",
    "\n",
    "        print(\"Random Forest--------------\")\n",
    "        rf_model = RandomForestClassifier(**rf_hyperparameters)\n",
    "        rf_model.fit(X_train, y_train)\n",
    "        rf_y_pred_proba = rf_model.predict_proba(X_valid)[:,1]\n",
    "        best_threshold, best_score = find_best_threshold_f1_score(y_valid, rf_y_pred_proba)\n",
    "        print(f\"\\n best_threshold {best_threshold} best_score {best_score}\")\n",
    "        y_pred = [1 if y_hat >=best_threshold else 0 for y_hat in rf_y_pred_proba]\n",
    "\n",
    "        print(roc_auc_score(y_valid, rf_y_pred_proba))\n",
    "\n",
    "        print(classification_report(y_valid, y_pred, digits=4))\n",
    "        oofs[valid_index,1] = rf_y_pred_proba\n",
    "    #     models.append(model)\n",
    "        print(\"LGBModel--------------\")\n",
    "        lgb_model = LGBMClassifier(**lgb_hyperparameters)\n",
    "        callbacks = [lgb.early_stopping(200, verbose=50), lgb.log_evaluation(period=50)]\n",
    "        lgb_model.fit(X_train, y_train,\n",
    "                  eval_set=[(X_train, y_train), (X_valid, y_valid)],\n",
    "                  eval_metric=[\"logloss\", \"auc\"],\n",
    "                  callbacks=callbacks)\n",
    "\n",
    "        lgb_y_pred_proba = lgb_model.predict_proba(X_valid)[:,1]\n",
    "        best_threshold, best_score = find_best_threshold_f1_score(y_valid, lgb_y_pred_proba)\n",
    "        print(f\"\\n best_threshold {best_threshold} best_score {best_score}\")\n",
    "        y_pred = [1 if y_hat >=best_threshold else 0 for y_hat in lgb_y_pred_proba]\n",
    "        print(roc_auc_score(y_valid, lgb_y_pred_proba))\n",
    "        print(classification_report(y_valid, y_pred, digits=4))\n",
    "        oofs[valid_index,2] = lgb_y_pred_proba\n",
    "    #     models.append(model)\n",
    "        # display(pd.DataFrame({'score': lgb_model.feature_importances_, 'feature': lgb_model.feature_name_}).sort_values('score',ascending=False))\n",
    "\n",
    "        print(\"XGBoost--------------\")\n",
    "        xgb_model = XGBClassifier(**xgboost_hyperparameters)\n",
    "        xgb_model.fit(X_train, y_train,\n",
    "                  eval_set=[(X_train, y_train), (X_valid, y_valid)],\n",
    "                  verbose=50)\n",
    "        xgb_y_pred_proba = xgb_model.predict_proba(X_valid)[:,1]\n",
    "        best_threshold, best_score = find_best_threshold_f1_score(y_valid, xgb_y_pred_proba)\n",
    "        print(f\"\\n best_threshold {best_threshold} best_score {best_score}\")\n",
    "        y_pred = [1 if y_hat >=best_threshold else 0 for y_hat in xgb_y_pred_proba]\n",
    "        print(roc_auc_score(y_valid, xgb_y_pred_proba))\n",
    "        print(classification_report(y_valid, y_pred, digits=4))\n",
    "        oofs[valid_index, 3] = xgb_y_pred_proba\n",
    "        \n",
    "        print(\"MLP------------------\")\n",
    "        mlp_model = MLPClassifier(**mlp_hyperparameters)\n",
    "        mlp_model.fit(X_train, y_train)\n",
    "        mlp_y_pred_proba = mlp_model.predict_proba(X_valid)[:,1]\n",
    "        best_threshold, best_score = find_best_threshold_f1_score(y_valid, mlp_y_pred_proba)\n",
    "        print(f\"\\n best_threshold {best_threshold} best_score {best_score}\")\n",
    "        y_pred = [1 if y_hat >=best_threshold else 0 for y_hat in mlp_y_pred_proba]\n",
    "        print(roc_auc_score(y_valid, mlp_y_pred_proba))\n",
    "        print(classification_report(y_valid, y_pred, digits=4))\n",
    "        oofs[valid_index, 4] = mlp_y_pred_proba\n",
    "    return oofs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_scores = []\n",
    "f1_scores = []\n",
    "auc_scores = []\n",
    "specificity_scores = []\n",
    "sensitivity_scores = []\n",
    "def scoring(y_test,y_pred_proba, best_threshold):\n",
    "    y_pred = [1 if y_hat >= best_threshold else 0 for y_hat in y_pred_proba]\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    _f1_score = f1_score(y_test, y_pred, average='macro')\n",
    "    auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "    specificity = tn / (tn+fp)\n",
    "    sensitivity = tp / (tp+fn)\n",
    "    print(\"accuracy\", acc)\n",
    "    print(\"f1_score\", _f1_score)\n",
    "    print(\"auc\", auc_score)\n",
    "    print(\"sensitivity\", sensitivity, \"specificity\", specificity)\n",
    "    print(classification_report(y_test, y_pred, digits=4))\n",
    "    return acc, _f1_score, auc_score, specificity, sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(oofs,X_train, y_train, X_test, y_test, USE_SMOTE=False,USE_CLASS_WEIGHT=False, USE_UNDER_SAMPLING=False):\n",
    "    models = []\n",
    "    predictions = []\n",
    "    accuracy_scores = []\n",
    "    f1_scores = []\n",
    "    auc_scores = []\n",
    "    specificity_scores = []\n",
    "    sensitivity_scores = []\n",
    "    best_thresholds = []\n",
    "    for i in range(len(MODEL_NAMES)):\n",
    "        best_threshold, best_score = find_best_threshold_f1_score(train[TARGET].values, oofs[:,i])\n",
    "        best_thresholds.append(best_threshold)\n",
    "        print('\\n',best_threshold, best_score)\n",
    "    logreg_hyperparameters = Logreg_Hyperparameters.copy()\n",
    "    lgb_hyperparameters = LGBM_Hyperparameters.copy()\n",
    "    xgboost_hyperparameters = XGBoost_Hyperparameters.copy()\n",
    "    del xgboost_hyperparameters['early_stopping_rounds']\n",
    "    rf_hyperparameters = RF_Hyperparameters.copy()\n",
    "    mlp_hyperparameters = MLP_Hyperparametesr.copy()\n",
    "    if USE_SMOTE:\n",
    "        print(\"SMOTEEEE\")\n",
    "        sm = SMOTE(random_state=42)\n",
    "        X_train, y_train = sm.fit_resample(X_train, y_train)\n",
    "    elif USE_CLASS_WEIGHT:\n",
    "        print(\"CLASS_WEIGHTTTT\")\n",
    "        class_weights = compute_class_weight('balanced', classes=np.array([0, 1]), y=y_train)\n",
    "        class_weights =  {0: class_weights[0], 1: class_weights[1]}\n",
    "        lgb_hyperparameters['class_weight'] = class_weights\n",
    "        logreg_hyperparameters['class_weight'] = class_weights\n",
    "        xgboost_hyperparameters['scale_pos_weight'] = class_weights[1]/ class_weights[0]\n",
    "        rf_hyperparameters['class_weight'] = class_weights\n",
    "    elif USE_UNDER_SAMPLING:\n",
    "        print(\"UNDER SAMPLING\")\n",
    "        rus = RandomUnderSampler(random_state=42)\n",
    "        X_train, y_train = rus.fit_resample(X_train, y_train)\n",
    "\n",
    "    print(\"LOGREG--------------\")\n",
    "    logreg_model = LogisticRegression(**logreg_hyperparameters)\n",
    "    logreg_model.fit(X_train, y_train)\n",
    "    logreg_y_pred_proba = logreg_model.predict_proba(X_test)[:,1]\n",
    "    acc, _f1_score, auc_score, specificity, sensitivity = scoring(y_test,logreg_y_pred_proba,best_thresholds[0])\n",
    "    accuracy_scores.append(acc)\n",
    "    f1_scores.append(_f1_score)\n",
    "    auc_scores.append(auc_score)\n",
    "    specificity_scores.append(specificity)\n",
    "    sensitivity_scores.append(sensitivity)\n",
    "    predictions.append(logreg_y_pred_proba)\n",
    "    models.append(logreg_model)\n",
    "\n",
    "    print(\"Random Forest--------------\")\n",
    "    rf_model = RandomForestClassifier(**rf_hyperparameters)\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    rf_y_pred_proba = rf_model.predict_proba(X_test)[:,1]\n",
    "    acc, _f1_score, auc_score, specificity, sensitivity = scoring(y_test,rf_y_pred_proba,best_thresholds[1])\n",
    "    accuracy_scores.append(acc)\n",
    "    f1_scores.append(_f1_score)\n",
    "    auc_scores.append(auc_score)\n",
    "    specificity_scores.append(specificity)\n",
    "    sensitivity_scores.append(sensitivity)\n",
    "    predictions.append(rf_y_pred_proba)\n",
    "    models.append(rf_model)\n",
    "\n",
    "    print(\"LGBModel--------------\")\n",
    "    lgb_model = LGBMClassifier(**lgb_hyperparameters)\n",
    "    lgb_model.fit(X_train, y_train)\n",
    "    lgb_y_pred_proba = lgb_model.predict_proba(X_test)[:,1]\n",
    "    acc, _f1_score, auc_score, specificity, sensitivity = scoring(y_test,lgb_y_pred_proba,best_thresholds[2])\n",
    "    accuracy_scores.append(acc)\n",
    "    f1_scores.append(_f1_score)\n",
    "    auc_scores.append(auc_score)\n",
    "    specificity_scores.append(specificity)\n",
    "    sensitivity_scores.append(sensitivity)\n",
    "    predictions.append(lgb_y_pred_proba)\n",
    "    models.append(lgb_model)\n",
    "\n",
    "    print(\"XGBoost--------------\")\n",
    "    print(xgboost_hyperparameters)\n",
    "    xgb_model = XGBClassifier(**xgboost_hyperparameters)\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    xgb_y_pred_proba = xgb_model.predict_proba(X_test)[:,1]\n",
    "    acc, _f1_score, auc_score, specificity, sensitivity = scoring(y_test,xgb_y_pred_proba,best_thresholds[3])\n",
    "    accuracy_scores.append(acc)\n",
    "    f1_scores.append(_f1_score)\n",
    "    auc_scores.append(auc_score)\n",
    "    specificity_scores.append(specificity)\n",
    "    sensitivity_scores.append(sensitivity)\n",
    "    predictions.append(xgb_y_pred_proba)\n",
    "    models.append(xgb_model)\n",
    "    \n",
    "    print(\"MLP--------------\")\n",
    "    mlp_model = MLPClassifier(**mlp_hyperparameters)\n",
    "    mlp_model.fit(X_train, y_train)\n",
    "    mlp_y_pred_proba = mlp_model.predict_proba(X_test)[:,1]\n",
    "    acc, _f1_score, auc_score, specificity, sensitivity = scoring(y_test,mlp_y_pred_proba,best_thresholds[4])\n",
    "    accuracy_scores.append(acc)\n",
    "    f1_scores.append(_f1_score)\n",
    "    auc_scores.append(auc_score)\n",
    "    specificity_scores.append(specificity)\n",
    "    sensitivity_scores.append(sensitivity)\n",
    "    predictions.append(mlp_y_pred_proba)\n",
    "    models.append(mlp_model)\n",
    "\n",
    "    print(MODEL_NAMES)\n",
    "    print(accuracy_scores)\n",
    "    print(f1_scores)\n",
    "    print(auc_scores)\n",
    "    print(specificity_scores)\n",
    "    print(sensitivity_scores)\n",
    "    score_df = pd.DataFrame({'model_name': MODEL_NAMES,\n",
    "                         'accuracy_score':accuracy_scores, \n",
    "                         'f1_score': f1_scores, \n",
    "                         'auc_score': auc_scores, \n",
    "                         'specificity_score': specificity_scores, \n",
    "                         'sensitivity_score': sensitivity_scores})\n",
    "    return score_df,models, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n"
     ]
    }
   ],
   "source": [
    "X_train,oh_encoder,robust_scaler = process_data(train)\n",
    "X_test, _,_ = process_data(test,oh_encoder,robust_scaler)\n",
    "y_train = train[TARGET].values\n",
    "y_test = test[TARGET].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not handle Balanced\n",
      "===========fold 0================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "os_name_macos          0\n",
      "os_name_windows        0\n",
      "age_group_15-17        0\n",
      "age_group_18-24        0\n",
      "age_group_25-34        0\n",
      "                      ..\n",
      "work_count             0\n",
      "social_count           0\n",
      "news_count             0\n",
      "entertainment_count    0\n",
      "ecommerce_count        0\n",
      "Length: 123, dtype: int64\n",
      "LOGREG--------------\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " best_threshold 0.4199999999999998 best_score 0.7654678764547205\n",
      "0.8817845029801301\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8888    0.8776    0.8831    251891\n",
      "           1     0.6356    0.6605    0.6478     81443\n",
      "\n",
      "    accuracy                         0.8245    333334\n",
      "   macro avg     0.7622    0.7690    0.7655    333334\n",
      "weighted avg     0.8269    0.8245    0.8256    333334\n",
      "\n",
      "Random Forest--------------\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " best_threshold 0.4099999999999998 best_score 0.7864116939036683\n",
      "0.9033336789506895\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9040    0.8786    0.8911    251891\n",
      "           1     0.6545    0.7113    0.6817     81443\n",
      "\n",
      "    accuracy                         0.8377    333334\n",
      "   macro avg     0.7792    0.7950    0.7864    333334\n",
      "weighted avg     0.8430    0.8377    0.8399    333334\n",
      "\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.9648085464373999. Current value: lambda_l2=0.9648085464373999\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.9648085464373999. Current value: lambda_l2=0.9648085464373999\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162885, number of negative: 503781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7208\n",
      "[LightGBM] [Info] Number of data points in the train set: 666666, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.013538 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.9648085464373999. Current value: lambda_l2=0.9648085464373999\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244328 -> initscore=-1.129097\n",
      "[LightGBM] [Info] Start training from score -1.129097\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[50]\ttraining's binary_logloss: 0.321418\ttraining's auc: 0.905822\tvalid_1's binary_logloss: 0.322781\tvalid_1's auc: 0.904788\n",
      "[100]\ttraining's binary_logloss: 0.318009\ttraining's auc: 0.907399\tvalid_1's binary_logloss: 0.320818\tvalid_1's auc: 0.905518\n",
      "[150]\ttraining's binary_logloss: 0.316383\ttraining's auc: 0.908392\tvalid_1's binary_logloss: 0.320622\tvalid_1's auc: 0.90559\n",
      "[200]\ttraining's binary_logloss: 0.314951\ttraining's auc: 0.909276\tvalid_1's binary_logloss: 0.320481\tvalid_1's auc: 0.905662\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[199]\ttraining's binary_logloss: 0.314978\ttraining's auc: 0.909258\tvalid_1's binary_logloss: 0.320477\tvalid_1's auc: 0.905663\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.9648085464373999. Current value: lambda_l2=0.9648085464373999\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " best_threshold 0.3999999999999998 best_score 0.7881231547163672\n",
      "0.9056634653580582\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9097    0.8702    0.8895    251891\n",
      "           1     0.6460    0.7329    0.6867     81443\n",
      "\n",
      "    accuracy                         0.8366    333334\n",
      "   macro avg     0.7779    0.8015    0.7881    333334\n",
      "weighted avg     0.8453    0.8366    0.8400    333334\n",
      "\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-logloss:0.54346\tvalidation_0-auc:0.89516\tvalidation_1-logloss:0.54359\tvalidation_1-auc:0.89210\n",
      "[50]\tvalidation_0-logloss:0.33932\tvalidation_0-auc:0.90899\tvalidation_1-logloss:0.34349\tvalidation_1-auc:0.90484\n",
      "[100]\tvalidation_0-logloss:0.31541\tvalidation_0-auc:0.91135\tvalidation_1-logloss:0.32305\tvalidation_1-auc:0.90566\n",
      "[150]\tvalidation_0-logloss:0.30969\tvalidation_0-auc:0.91308\tvalidation_1-logloss:0.32019\tvalidation_1-auc:0.90597\n",
      "[199]\tvalidation_0-logloss:0.30698\tvalidation_0-auc:0.91448\tvalidation_1-logloss:0.31975\tvalidation_1-auc:0.90605\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " best_threshold 0.4099999999999998 best_score 0.7878542525124381\n",
      "0.9060532930968039\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9061    0.8766    0.8911    251891\n",
      "           1     0.6533    0.7191    0.6846     81443\n",
      "\n",
      "    accuracy                         0.8381    333334\n",
      "   macro avg     0.7797    0.7978    0.7879    333334\n",
      "weighted avg     0.8443    0.8381    0.8407    333334\n",
      "\n",
      "MLP------------------\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " best_threshold 0.3799999999999999 best_score 0.7818576487671994\n",
      "0.8998520002968362\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9102    0.8597    0.8843    251891\n",
      "           1     0.6297    0.7377    0.6795     81443\n",
      "\n",
      "    accuracy                         0.8299    333334\n",
      "   macro avg     0.7700    0.7987    0.7819    333334\n",
      "weighted avg     0.8417    0.8299    0.8342    333334\n",
      "\n",
      "===========fold 1================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "os_name_macos          0\n",
      "os_name_windows        0\n",
      "age_group_15-17        0\n",
      "age_group_18-24        0\n",
      "age_group_25-34        0\n",
      "                      ..\n",
      "work_count             0\n",
      "social_count           0\n",
      "news_count             0\n",
      "entertainment_count    0\n",
      "ecommerce_count        0\n",
      "Length: 123, dtype: int64\n",
      "LOGREG--------------\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " best_threshold 0.3899999999999999 best_score 0.7658763039604568\n",
      "0.8817584546366629\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8942    0.8664    0.8801    251891\n",
      "           1     0.6230    0.6831    0.6517     81442\n",
      "\n",
      "    accuracy                         0.8216    333333\n",
      "   macro avg     0.7586    0.7747    0.7659    333333\n",
      "weighted avg     0.8280    0.8216    0.8243    333333\n",
      "\n",
      "Random Forest--------------\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " best_threshold 0.3799999999999999 best_score 0.7865020800103608\n",
      "0.903710444460618\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9124    0.8628    0.8869    251891\n",
      "           1     0.6367    0.7438    0.6861     81442\n",
      "\n",
      "    accuracy                         0.8337    333333\n",
      "   macro avg     0.7746    0.8033    0.7865    333333\n",
      "weighted avg     0.8450    0.8337    0.8378    333333\n",
      "\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.9648085464373999. Current value: lambda_l2=0.9648085464373999\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.9648085464373999. Current value: lambda_l2=0.9648085464373999\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162886, number of negative: 503781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7222\n",
      "[LightGBM] [Info] Number of data points in the train set: 666667, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.013083 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.9648085464373999. Current value: lambda_l2=0.9648085464373999\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244329 -> initscore=-1.129091\n",
      "[LightGBM] [Info] Start training from score -1.129091\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[50]\ttraining's binary_logloss: 0.321582\ttraining's auc: 0.905665\tvalid_1's binary_logloss: 0.32223\tvalid_1's auc: 0.905286\n",
      "[100]\ttraining's binary_logloss: 0.31825\ttraining's auc: 0.90723\tvalid_1's binary_logloss: 0.320353\tvalid_1's auc: 0.905934\n",
      "[150]\ttraining's binary_logloss: 0.316606\ttraining's auc: 0.908209\tvalid_1's binary_logloss: 0.32009\tvalid_1's auc: 0.906046\n",
      "[200]\ttraining's binary_logloss: 0.315112\ttraining's auc: 0.909134\tvalid_1's binary_logloss: 0.319882\tvalid_1's auc: 0.906133\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[200]\ttraining's binary_logloss: 0.315112\ttraining's auc: 0.909134\tvalid_1's binary_logloss: 0.319882\tvalid_1's auc: 0.906133\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.9648085464373999. Current value: lambda_l2=0.9648085464373999\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " best_threshold 0.3999999999999998 best_score 0.7880491813367024\n",
      "0.9061334150653364\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9093    0.8709    0.8897    251891\n",
      "           1     0.6468    0.7313    0.6864     81442\n",
      "\n",
      "    accuracy                         0.8368    333333\n",
      "   macro avg     0.7780    0.8011    0.7880    333333\n",
      "weighted avg     0.8451    0.8368    0.8400    333333\n",
      "\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-logloss:0.54345\tvalidation_0-auc:0.89507\tvalidation_1-logloss:0.54355\tvalidation_1-auc:0.89281\n",
      "[50]\tvalidation_0-logloss:0.33950\tvalidation_0-auc:0.90882\tvalidation_1-logloss:0.34338\tvalidation_1-auc:0.90504\n",
      "[100]\tvalidation_0-logloss:0.31567\tvalidation_0-auc:0.91112\tvalidation_1-logloss:0.32280\tvalidation_1-auc:0.90591\n",
      "[150]\tvalidation_0-logloss:0.31006\tvalidation_0-auc:0.91283\tvalidation_1-logloss:0.31980\tvalidation_1-auc:0.90634\n",
      "[199]\tvalidation_0-logloss:0.30753\tvalidation_0-auc:0.91413\tvalidation_1-logloss:0.31934\tvalidation_1-auc:0.90644\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " best_threshold 0.3899999999999999 best_score 0.7887309351936749\n",
      "0.9064443503734745\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9122    0.8666    0.8888    251891\n",
      "           1     0.6426    0.7419    0.6887     81442\n",
      "\n",
      "    accuracy                         0.8361    333333\n",
      "   macro avg     0.7774    0.8042    0.7887    333333\n",
      "weighted avg     0.8463    0.8361    0.8399    333333\n",
      "\n",
      "MLP------------------\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " best_threshold 0.3999999999999998 best_score 0.7859349738485901\n",
      "0.9033802665987385\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9066    0.8728    0.8894    251891\n",
      "           1     0.6472    0.7219    0.6825     81442\n",
      "\n",
      "    accuracy                         0.8359    333333\n",
      "   macro avg     0.7769    0.7973    0.7859    333333\n",
      "weighted avg     0.8432    0.8359    0.8388    333333\n",
      "\n",
      "===========fold 2================\n",
      "fit train OneHotEncoder\n",
      "fit train RobustScaler\n",
      "loadd onehot encoder\n",
      "loadd robust scaler\n",
      "os_name_macos          0\n",
      "os_name_windows        0\n",
      "age_group_15-17        0\n",
      "age_group_18-24        0\n",
      "age_group_25-34        0\n",
      "                      ..\n",
      "work_count             0\n",
      "social_count           0\n",
      "news_count             0\n",
      "entertainment_count    0\n",
      "ecommerce_count        0\n",
      "Length: 123, dtype: int64\n",
      "LOGREG--------------\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " best_threshold 0.4199999999999998 best_score 0.766389215051437\n",
      "0.8821618871608989\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8891    0.8785    0.8837    251890\n",
      "           1     0.6375    0.6610    0.6490     81443\n",
      "\n",
      "    accuracy                         0.8253    333333\n",
      "   macro avg     0.7633    0.7697    0.7664    333333\n",
      "weighted avg     0.8276    0.8253    0.8264    333333\n",
      "\n",
      "Random Forest--------------\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " best_threshold 0.3999999999999998 best_score 0.7861763177850876\n",
      "0.9035165229776971\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9068    0.8727    0.8894    251890\n",
      "           1     0.6474    0.7226    0.6829     81443\n",
      "\n",
      "    accuracy                         0.8360    333333\n",
      "   macro avg     0.7771    0.7977    0.7862    333333\n",
      "weighted avg     0.8434    0.8360    0.8390    333333\n",
      "\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.9648085464373999. Current value: lambda_l2=0.9648085464373999\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.9648085464373999. Current value: lambda_l2=0.9648085464373999\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 162885, number of negative: 503782\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7207\n",
      "[LightGBM] [Info] Number of data points in the train set: 666667, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (15.26 MB) transferred to GPU in 0.013008 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.9648085464373999. Current value: lambda_l2=0.9648085464373999\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244327 -> initscore=-1.129099\n",
      "[LightGBM] [Info] Start training from score -1.129099\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[50]\ttraining's binary_logloss: 0.321577\ttraining's auc: 0.905712\tvalid_1's binary_logloss: 0.322636\tvalid_1's auc: 0.904993\n",
      "[100]\ttraining's binary_logloss: 0.318236\ttraining's auc: 0.907256\tvalid_1's binary_logloss: 0.320679\tvalid_1's auc: 0.905702\n",
      "[150]\ttraining's binary_logloss: 0.316641\ttraining's auc: 0.908245\tvalid_1's binary_logloss: 0.320468\tvalid_1's auc: 0.905792\n",
      "[200]\ttraining's binary_logloss: 0.31519\ttraining's auc: 0.909139\tvalid_1's binary_logloss: 0.320284\tvalid_1's auc: 0.905882\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[200]\ttraining's binary_logloss: 0.31519\ttraining's auc: 0.909139\tvalid_1's binary_logloss: 0.320284\tvalid_1's auc: 0.905882\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.9648085464373999. Current value: lambda_l2=0.9648085464373999\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " best_threshold 0.3899999999999999 best_score 0.7878589941117423\n",
      "0.905882256391938\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9119    0.8657    0.8882    251890\n",
      "           1     0.6410    0.7413    0.6875     81443\n",
      "\n",
      "    accuracy                         0.8353    333333\n",
      "   macro avg     0.7764    0.8035    0.7879    333333\n",
      "weighted avg     0.8457    0.8353    0.8392    333333\n",
      "\n",
      "XGBoost--------------\n",
      "[0]\tvalidation_0-logloss:0.54347\tvalidation_0-auc:0.89526\tvalidation_1-logloss:0.54358\tvalidation_1-auc:0.89235\n",
      "[50]\tvalidation_0-logloss:0.33940\tvalidation_0-auc:0.90892\tvalidation_1-logloss:0.34326\tvalidation_1-auc:0.90510\n",
      "[100]\tvalidation_0-logloss:0.31554\tvalidation_0-auc:0.91128\tvalidation_1-logloss:0.32279\tvalidation_1-auc:0.90592\n",
      "[150]\tvalidation_0-logloss:0.30987\tvalidation_0-auc:0.91295\tvalidation_1-logloss:0.31981\tvalidation_1-auc:0.90628\n",
      "[199]\tvalidation_0-logloss:0.30730\tvalidation_0-auc:0.91428\tvalidation_1-logloss:0.31941\tvalidation_1-auc:0.90635\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " best_threshold 0.4099999999999998 best_score 0.7878506576322418\n",
      "0.906354797142758\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9059    0.8769    0.8912    251890\n",
      "           1     0.6536    0.7184    0.6845     81443\n",
      "\n",
      "    accuracy                         0.8382    333333\n",
      "   macro avg     0.7798    0.7977    0.7879    333333\n",
      "weighted avg     0.8443    0.8382    0.8407    333333\n",
      "\n",
      "MLP------------------\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " best_threshold 0.3999999999999998 best_score 0.7850443498199667\n",
      "0.90292555438285\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9084    0.8679    0.8877    251890\n",
      "           1     0.6410    0.7294    0.6824     81443\n",
      "\n",
      "    accuracy                         0.8341    333333\n",
      "   macro avg     0.7747    0.7987    0.7850    333333\n",
      "weighted avg     0.8431    0.8341    0.8375    333333\n",
      "\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " 0.3899999999999999 0.765857580081317\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " 0.4099999999999998 0.7861300126200266\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " 0.3999999999999998 0.7879309499269584\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " 0.4099999999999998 0.7880993085912167\n",
      "0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, \n",
      " 0.3999999999999998 0.7841974981179998\n",
      "LOGREG--------------\n",
      "accuracy 0.82134\n",
      "f1_score 0.766070984602778\n",
      "auc 0.8820681496957821\n",
      "sensitivity 0.6852593712954257 specificity 0.865410328713081\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8946    0.8654    0.8798    151074\n",
      "           1     0.6225    0.6853    0.6524     48926\n",
      "\n",
      "    accuracy                         0.8213    200000\n",
      "   macro avg     0.7586    0.7753    0.7661    200000\n",
      "weighted avg     0.8281    0.8213    0.8241    200000\n",
      "\n",
      "Random Forest--------------\n",
      "accuracy 0.836955\n",
      "f1_score 0.7851712546408942\n",
      "auc 0.9032072042763251\n",
      "sensitivity 0.7071700118546376 specificity 0.8789864569681084\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9026    0.8790    0.8906    151074\n",
      "           1     0.6543    0.7072    0.6797     48926\n",
      "\n",
      "    accuracy                         0.8370    200000\n",
      "   macro avg     0.7784    0.7931    0.7852    200000\n",
      "weighted avg     0.8419    0.8370    0.8390    200000\n",
      "\n",
      "LGBModel--------------\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.9648085464373999. Current value: lambda_l2=0.9648085464373999\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.9648085464373999. Current value: lambda_l2=0.9648085464373999\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 244328, number of negative: 755672\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7203\n",
      "[LightGBM] [Info] Number of data points in the train set: 1000000, number of used features: 123\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 21 dense feature groups (22.89 MB) transferred to GPU in 0.017467 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244328 -> initscore=-1.129096\n",
      "[LightGBM] [Info] Start training from score -1.129096\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=0.9648085464373999. Current value: lambda_l2=0.9648085464373999\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "accuracy 0.83616\n",
      "f1_score 0.7874860583318217\n",
      "auc 0.9055505085191089\n",
      "sensitivity 0.7308588480562482 specificity 0.8702622555833565\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9090    0.8703    0.8892    151074\n",
      "           1     0.6459    0.7309    0.6858     48926\n",
      "\n",
      "    accuracy                         0.8362    200000\n",
      "   macro avg     0.7775    0.8006    0.7875    200000\n",
      "weighted avg     0.8446    0.8362    0.8394    200000\n",
      "\n",
      "XGBoost--------------\n",
      "{'objective': 'binary:logistic', 'eval_metric': ['logloss', 'auc'], 'n_estimators': 200, 'learning_rate': 0.034630277480196384, 'max_depth': 9, 'colsample_bytree': 0.8, 'subsample': 0.30000000000000004, 'lambda': 1.3270228907353322e-06, 'alpha': 0.0020136244579038245, 'seed': 42, 'enable_categorical': True, 'tree_method': 'gpu_hist'}\n",
      "accuracy 0.83788\n",
      "f1_score 0.7877033613230082\n",
      "auc 0.9059784331465455\n",
      "sensitivity 0.7188815762580223 specificity 0.8764181791704727\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9059    0.8764    0.8909    151074\n",
      "           1     0.6532    0.7189    0.6845     48926\n",
      "\n",
      "    accuracy                         0.8379    200000\n",
      "   macro avg     0.7796    0.7976    0.7877    200000\n",
      "weighted avg     0.8441    0.8379    0.8404    200000\n",
      "\n",
      "MLP--------------\n",
      "accuracy 0.834835\n",
      "f1_score 0.7846273265345889\n",
      "auc 0.9030631711027718\n",
      "sensitivity 0.7194743081388218 specificity 0.8721950832042575\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9057    0.8722    0.8886    151074\n",
      "           1     0.6458    0.7195    0.6806     48926\n",
      "\n",
      "    accuracy                         0.8348    200000\n",
      "   macro avg     0.7757    0.7958    0.7846    200000\n",
      "weighted avg     0.8421    0.8348    0.8377    200000\n",
      "\n",
      "['log_reg', 'randomforest', 'lightgbm', 'xgboost', 'mlp']\n",
      "[0.82134, 0.836955, 0.83616, 0.83788, 0.834835]\n",
      "[0.766070984602778, 0.7851712546408942, 0.7874860583318217, 0.7877033613230082, 0.7846273265345889]\n",
      "[0.8820681496957821, 0.9032072042763251, 0.9055505085191089, 0.9059784331465455, 0.9030631711027718]\n",
      "[0.865410328713081, 0.8789864569681084, 0.8702622555833565, 0.8764181791704727, 0.8721950832042575]\n",
      "[0.6852593712954257, 0.7071700118546376, 0.7308588480562482, 0.7188815762580223, 0.7194743081388218]\n"
     ]
    }
   ],
   "source": [
    "print(\"Not handle Balanced\")\n",
    "nothing_oofs = cross_validate(train)\n",
    "nothing_score_df, nothing_models, nothing_predictions = train_model(nothing_oofs,X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'xgb': dict_items([('learning_rate', 0.034630277480196384), ('max_depth', 9), ('colsample_bytree', 0.8), ('subsample', 0.30000000000000004), ('lambda', 1.3270228907353322e-06), ('alpha', 0.0020136244579038245), ('hidden_layer_sizes', 3)]),\n",
       " 'lgb': dict_items([('learning_rate', 0.09249779656872704), ('max_depth', 8), ('colsample_bytree', 1.0), ('subsample', 0.1), ('lambda', 0.9648085464373999), ('alpha', 0.9903394475964249), ('hidden_layer_sizes', 4)]),\n",
       " 'mlp': dict_items([('learning_rate', 0.09489887655374188), ('max_depth', 5), ('colsample_bytree', 0.8), ('subsample', 0.1), ('lambda', 0.000231391542151867), ('alpha', 0.6459845987300605), ('hidden_layer_sizes', 5)]),\n",
       " 'rf': dict_items([('learning_rate', 0.060492102527360606), ('max_depth', 9), ('colsample_bytree', 0.5), ('subsample', 0.2), ('lambda', 0.12619375123209245), ('alpha', 2.3009010377609224e-07), ('hidden_layer_sizes', 2)])}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>accuracy_score</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>auc_score</th>\n",
       "      <th>specificity_score</th>\n",
       "      <th>sensitivity_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>log_reg</td>\n",
       "      <td>0.821340</td>\n",
       "      <td>0.766071</td>\n",
       "      <td>0.882068</td>\n",
       "      <td>0.865410</td>\n",
       "      <td>0.685259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>randomforest</td>\n",
       "      <td>0.836955</td>\n",
       "      <td>0.785171</td>\n",
       "      <td>0.903207</td>\n",
       "      <td>0.878986</td>\n",
       "      <td>0.707170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lightgbm</td>\n",
       "      <td>0.836160</td>\n",
       "      <td>0.787486</td>\n",
       "      <td>0.905551</td>\n",
       "      <td>0.870262</td>\n",
       "      <td>0.730859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>xgboost</td>\n",
       "      <td>0.837880</td>\n",
       "      <td>0.787703</td>\n",
       "      <td>0.905978</td>\n",
       "      <td>0.876418</td>\n",
       "      <td>0.718882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mlp</td>\n",
       "      <td>0.834835</td>\n",
       "      <td>0.784627</td>\n",
       "      <td>0.903063</td>\n",
       "      <td>0.872195</td>\n",
       "      <td>0.719474</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     model_name  accuracy_score  f1_score  auc_score  specificity_score  \\\n",
       "0       log_reg        0.821340  0.766071   0.882068           0.865410   \n",
       "1  randomforest        0.836955  0.785171   0.903207           0.878986   \n",
       "2      lightgbm        0.836160  0.787486   0.905551           0.870262   \n",
       "3       xgboost        0.837880  0.787703   0.905978           0.876418   \n",
       "4           mlp        0.834835  0.784627   0.903063           0.872195   \n",
       "\n",
       "   sensitivity_score  \n",
       "0           0.685259  \n",
       "1           0.707170  \n",
       "2           0.730859  \n",
       "3           0.718882  \n",
       "4           0.719474  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nothing_score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(f\"../checkpoints/hyper_nothing_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump({\n",
    "            \"score_df\":nothing_score_df,\n",
    "            \"oofs\": nothing_oofs,\n",
    "            \"models\": nothing_models,\n",
    "            \"model_names\": ['lr', 'rf', 'lgb', 'xgb', 'mlp'],\n",
    "            \"predictions\":nothing_predictions},f,protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "churn_prediction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
